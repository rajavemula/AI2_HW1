{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![](fig/univ.png)\n",
                "\n",
                "# AI-2: Convolutional Neural Network\n",
                "## Homework 1: Artificial Neural Networks, Model Interpretation, and Regularization\n",
                "\n",
                "**AI2 Cohort 5**<br/>\n",
                "**Univ.AI**<br/>\n",
                "**Instructors**:Dr. Pavlos Protopapas<br />\n",
                "**Maximum Score**: 95 (+ 5 BONUS)\n",
                "\n",
                "<hr style=\"height:2.4pt\">"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "#RUN THIS CELL \n",
                "import requests\n",
                "from IPython.core.display import HTML"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "import random\n",
                "random.seed(112358)\n",
                "\n",
                "import os\n",
                "import sys\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "\n",
                "from sklearn.inspection import permutation_importance\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import roc_auc_score, accuracy_score\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.utils import resample\n",
                "\n",
                "# TensorFlow and tf.keras\n",
                "import tensorflow as tf\n",
                "\n",
                "%matplotlib inline"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### INSTRUCTIONS\n",
                "\n",
                "\n",
                "- This homework is a jupyter notebook. Download and work on it on your local machine.\n",
                "\n",
                "- This homework should be submitted in pairs.\n",
                "\n",
                "- Homework should be submitted by only one student in a team of two. If both teammates submit, scores will be penalized.\n",
                "\n",
                "- Please restart the kernel and run the entire notebook again before you submit.\n",
                "\n",
                "- Running cells out of order is a common pitfall in Jupyter Notebooks. To make sure your code works restart the kernel and run the whole notebook again before you submit. \n",
                "\n",
                "- If you decide to submit a colab notebook, ensure you have given **complete edit access** to the staff while submitting. Also add any data that requires reviewing on google drive and submit the link.\n",
                "\n",
                "\n",
                "- To submit the homework, either one of you upload the working notebook on edStem and click the submit button on the bottom right corner.\n",
                "\n",
                "- Submit the homework well before the given deadline. Submissions after the deadline will not be graded.\n",
                "\n",
                "- We have tried to include all the libraries you may need to do the assignment in the imports statement at the top of this notebook. We strongly suggest that you use those and not others as we may not be familiar with them.\n",
                "\n",
                "- Comment your code well. This would help the graders in case there is any issue with the notebook while running. It is important to remember that the graders will not troubleshoot your code. \n",
                "\n",
                "- Please use .head() when viewing data. Do not submit a notebook that is **excessively long**. \n",
                "\n",
                "- In questions that require code to answer, such as \"calculate the $R^2$\", do not just output the value from a cell. Write a `print()` function that includes a reference to the calculated value, **not hardcoded**. For example: \n",
                "```\n",
                "print(f'The R^2 is {R:.4f}')\n",
                "```\n",
                "- Your plots should include clear labels for the $x$ and $y$ axes as well as a descriptive title (\"MSE plot\" is not a descriptive title; \"95 % confidence interval of coefficients of polynomial degree 5\" is).\n",
                "\n",
                "- **Ensure you make appropraite plots for all the questions it is applicable to, regardless of it being explicitly asked for.**\n",
                "\n",
                "- **IMPORTANT** : Do a final \"RUN\" and make sure your **outputs** are all visible for **each cell** at the time of submission. We will not be able to grade any cells without outputs.\n",
                "\n",
                "<span style=\"color:red\"> \n",
                "\n",
                "**IMPORTANT** \n",
                "\n",
                "- Plagiarism of code/blocks of text is not acceptable for any Univ.AI submissions.\n",
                "- You are allowed to refer to a publicly available source of information for a SMALL chunk of code  provided you cite it clearly. \n",
                "- Copying the code blatantly without attribution is not permitted in any case. \n",
                "- Sharing code between homework teams is strictly not allowed. In case of any confusion, you are advised to ask the staff on the Ed forum rather than assume the course of action needed.\n",
                "- If caught plagiarizing, you risk expulsion from the program.\n",
                "</span>\n",
                "\n",
                "<hr style=\"height:2pt\">"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Names of the people who worked on this homework together\n",
                "#### Hari Krishna"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id=\"contents\"></a>\n",
                "\n",
                "## Notebook Contents\n",
                "\n",
                "- [**PART 1 [40 pts]: Model interpretation and predictive intervals in NN**](#part1)\n",
                "  - [Overview and Data Description](#part1intro)\n",
                "  - [Questions](#part1questions)\n",
                "  - [Solutions](#part1solutions)\n",
                "\n",
                "\n",
                "- [**PART 2.1 [30 pts]: Kannada MNIST Kaggle competition using ANNs**](#part2.1)\n",
                "  - [Problem Statement](#part2.1intro)\n",
                "  - [The Kannada MNIST Dataset](#part2.1about)\n",
                "  - [Downloading the Data Files](#part2.1data)\n",
                "  - [AI2-C2 Homework Kaggle Competition](#part2.1kaggle)\n",
                "  - [Questions](#part2.1questions)\n",
                "  - [Solutions](#part2.1solutions)\n",
                "\n",
                "- [**PART 2.2 [30 pts]: Kannada MNIST using CNNs**](#part2)\n",
                "  - [Questions](#part2.2questions)\n",
                "  - [Solutions](#part2.2solutions)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
                "<h1> PART 1 [40 pts]: Model interpretation and predictive intervals in NN </h1> \n",
                "\n",
                "<a id=\"part1intro\"></a>\n",
                "\n",
                "<b> Overview and Data Description </b>\n",
                "\n",
                "In this problem, you will be building and interpreting models to predict whether a flight was delayed for its arrival based on features that could be measured as the flight takes off.  \n",
                "We will also estimate the predictive intervals of the model using bootstrapping. We will utilize those predictive intervals to build a new kind of model: a model that refrains from making a prediction when it is not confident.  \n",
                "\n",
                "The included variables are:\n",
                "\n",
                "**ARRIVAL_DELAY**: the difference between scheduled arrival and actual arrival, in minutes (positive is late, negative is early).\n",
                "\n",
                "**DISTANCE**: the distance between arrival and departure airports, in miles.\n",
                "\n",
                "**SCHEDULED_TIME**: the flight's scheduled travel time.\n",
                "\n",
                "**MONTH**: the month the flight took off, 1 = January, 2 = February, etc.\n",
                "\n",
                "**SCHED_DEP_HOUR**: the scheduled departure time (the hour of the day).\n",
                "\n",
                "**SCHED_ARR_HOUR**: the scheduled arrival time (the hour of the day).\n",
                "\n",
                "**FLIGHT_COUNT**: the number of flights flying out of that airport before noon on a typical day.\n",
                "\n",
                "**DAY_OF_WEEK**: the day of the week, 1 = Monday, 2 = Tuesday, etc.\n",
                "\n",
                "**ORIGIN_AIRPORT**: the airport the flight took off from.\n",
                "\n",
                "**DESTINATION_AIRPORT**: the airport the flight was scheduled to land at.\n",
                "\n",
                "For the airport codes, see: https://www.bts.gov/topics/airlines-and-airports/world-airport-codes\n",
                "\n",
                "To sucessfully complete this part, you will proceed by fitting a NN model, evaluating its accuracy, interpreting the predictors' importance, and finally evaluating the predictive intervals.\n",
                "\n",
                "**NOTE:** The observations were sampled so that roughly half of the observations were delayed and half of the observations were not delayed.\n",
                "\n",
                "</div> "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
                "\n",
                "\n",
                "<h2>PART 1: Questions</h2> \n",
                "\n",
                "[Return to contents](#contents)\n",
                "\n",
                "**1.1.1 [2 points]**  Read in the dataset `flights.csv`. Create a variable `DELAY_OR_NOT` that denotes whether `ARRIVAL_DELAY` is greater than or equal to 15 minutes (the FAA and BTS define a flight as delayed only if it arrives 15 minutes late or more). This is going to be the response variable for the rest of this question. \n",
                "\n",
                "Hint : Remember to drop the `ARRIVAL_DELAY` column.\n",
                "\n",
                "**1.1.2 [2 points]** Preprocess the data: one-hot-encode the non-numeric categorical variables and split the data into training and test sets (use an 80/20 split with `random_state=111`). Remember to scale your data and deal with missing values should you find any. Print the resulting shapes of your $X$ and $y$ dataframes for both your train and your test sets.\n",
                "\n",
                "**HINT**: When do you think you should scale the data? before or after splitting it?\n",
                "\n",
                "**1.2 [2 points]** Fit an artificial neural network model using all predictors (name this model `NN_model`).  Use a dense feed-forward network with two hidden layers with 15 nodes in each hidden layer. For this network, use appropriate activation functions for each layer, select an appropriate loss function and optimizer, specify a validation split of 0.2, train for an appropriate number of epochs based on the results of your training and validation accuracy plot, and feel free to use the default batch size while training. Plot the training accuracy and validation accuracy as a function of epochs from your `NN_model` training history. Evaluate the `NN_model` model on both train and test, and print out the resulting train and test accuracies.\n",
                "\n",
                "**1.3 [10 points]** To begin our interpretation of the resulting `NN_model`, we will first use a \"proxy model\" that we know how to interpret and train it on our `NN_model` training predictions.\n",
                "\n",
                "- **1.3.1** For this we need to modify our training set. First, generate a set of `NN_model` class predictions for the training set. These training predictions will be used to form a revised training dataset for our proxy model: (a) use all of the same $X$ values used by `NN_model` for our $X$ train and (b) replace the actual response values $y$ with the predicted $\\hat{y}$ values generated by the fitted `NN_model`.\n",
                "\n",
                "- **1.3.2** Next, fit a logistic regression model using your revised training dataset from 1.3.1 (name this model `logreg`). Use ridge-like regularization. Print the `logreg` test accuracy to confirm that it is similar to what we saw for our `NN_model` test accuracy in 1.2. You may need to adjust `C` in order to achieve a similar accuracy.\n",
                "\n",
                "- **1.3.3** Now use sklearn's `permutation_importance` class (already included in this notebook's imports) to compute the feature importance using the `logreg` model.\n",
                "\n",
                "  - Read the official documentation for `permutation_importance` [here](https://scikit-learn.org/stable/modules/permutation_importance.html#:~:text=The%20permutation%20feature%20importance%20is,model%20depends%20on%20the%20feature.) as well as [here](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance) to learn how it works.\n",
                "\n",
                "  - You can use the default number of `n_repeats` and your estimator's default `scorer`. To speed up the time it takes to run your permutations, you can try setting `n_jobs=-1` to take full advantage of all of your available processor cores.\n",
                "\n",
                "  - Measure the **relative** variable importance (i.e. as a proportion of the variable importance of the most important variable identified by `permutation_importance`) and generate a barplot illustrating the relative variable importances for the top-10 most important predictors identified using `permutation_importance`.\n",
                "\n",
                "**1.4 [10 points]** Another way to interpret the  `NN_model` is by examining the response as a function of any of the predictors. Particularly, we will select from features often found most significant from the analysis above. **For all 1.4 plots below**, for ease of interpretation, **please be certain to** display all predictors on their original scales. \n",
                "\n",
                "   - **1.4.1** Set all predictors to their means/modes except for `SCHED_DEP_HOUR`. Predict the probability of delay and plot the predicted probabilities of delay vs. `SCHED_DEP_HOUR` on the data from the **training set**. Interpret what you see in 2-4 sentences.\n",
                "   - **1.4.2** Set all predictors to their means/modes except for `SCHED_DEP_HOUR` and `FLIGHT_COUNT`. Predict the probability of delay and plot the predicted probabilities of delay vs. `SCHED_DEP_HOUR` and `FLIGHT_COUNT` from the training set (see the question 1.4 \"HINT\" below).\n",
                "\n",
                "   - **1.4.3**   Set all predictors to their means/modes except for except for `SCHED_DEP_HOUR` and `SCHED_ARR_HOUR`. Predict the probability of delay and plot the predicted probabilities of delay vs. `SCHED_DEP_HOUR` and `SCHED_ARR_HOUR` from the training set.\n",
                "\n",
                "\n",
                "   - **1.4.4** Set all predictors to their means/modes except for except for `SCHED_DEP_HOUR` and `DISTANCE`. Predict the probability of delay and plot the predicted probabilities of delay vs. `SCHED_DEP_HOUR` and `DISTANCE` from the training set. \n",
                "  - **1.4.5** In 5-10 sentences, interpret what you have seen in 1.4.2, 1.4.3, and 1.4.4.\n",
                "  \n",
                "**HINT:** \n",
                "\n",
                ">(A) For 1.4.2, 1.4.3, and 1.4.4, when you include `SCHED_DEP_HOUR` on one axis and your second predictor on the other axis, you can color your datapoints based on their corresponding predicted probabilities by using  the `c` and `cmap` arguments in `plt.scatter`. You can also add a labeled colorbar to your plot to make clear what those colors mean. Please refer to the matplotlib documentation for examples. \n",
                "\n",
                ">(B) You may use the previously trained model from part 1.2 here. \n",
                "    \n",
                "**1.5 [2 points]**\n",
                "    \n",
                "In this part, we will attempt to do model inference. Neural Networks have too many parameters, and therefore inference on all the parameters is intractable and meaningless. \n",
                "\n",
                "Using the same network architecture as `NN_model` (layers, nodes, activations, etc.) and your scaled data from that model, create multiple training sets using bootstrapping and fit a separate neural network model to each bootstrapped set of data (a minimum of at least 50 bootstraps should be used). Predict the output on the test data for each model. Randomly select 8 test observations and on 8 subplots, plot the distribution of predicted probabilities (i.e. $n$ bootstrapped probabilites) with the 95% CI bounds clearly marked and reported in each subplot and the actual class of each observation included in each subplot's title for easy reference.\n",
                "\n",
                "Interpret what you see in 3-5 sentences.\n",
                "\n",
                "    \n",
                "**NOTE:** The code for this problem can take an extremely long time to execute. Please feel free to use the provided `progressbar` function below to visually track the progress of your bootstraps.\n",
                "    \n",
                "**1.6 [12 points]**\n",
                "    \n",
                "Using the probability distribution of the predictions obtained from the bootstrapped samples above, we can evaluate how \"significant\" our bagged (i.e. bootstrap-aggregated) prediction will be for each test observation.\n",
                "\n",
                "To accomplish this, you will first calculate the ratio of bootstrapped probabilities that cross the threshold value of $\\hat{p}=0.5$. Let's call this ratio the **Posterior Prediction Ratio (PPR)**. When a bagged prediction's $PPR=0$, all predictions are compatible (i.e. all bootstrapped probabilities for that test observation are on the same side of $\\hat{p}=0.5$). Likewise, when the $PPR=0.5$, half of the bootstrapped predictions for that test observation are $\\hat{y}=0$, and the other half are $\\hat{y}=1$. After calculating your $PPR$ values for all test observations, you should have $n=2000$ $PPR$ values (i.e. one for each test observation).\n",
                "\n",
                "Next, to get more accurate predictions, we can create an **abstain** model that will abstain from making a prediction for a particular observation if some defined threshold for significance (i.e. maximum $PPR$ value) is not met. (If you'd like to learn more about abstain models, you can read more [here](https://openreview.net/forum?id=rJxF73R9tX).)\n",
                "\n",
                "Let's explore how your resulting test accuracies might change by using your bootstrapped prediction results from question 1.5 for an **abstain bagging model** (i.e. a bootstrap aggregated model where some test observations are simply not predicted based on a given $PPR$ threshold). You can make your abstain model *stricter* by using smaller $PPR$ threshold values.\n",
                "\n",
                "- Print the test accuracy for your **bagging model** predictions from question 1.5 using predictions for all 2,000 of our test observations. \n",
                "\n",
                "- Plot the test accuracies for an **abstain bagging model** using your predictions from question 1.5 as a function of increasing $PPR$ thresholds.\n",
                "\n",
                "- Also, plot the proportion of test observations not abstained (i.e. the proportion of those predicted) for your **abstain bagging model** as a function of increasing $PPR$ thresholds.\n",
                "\n",
                "- Interpret what you see in 3-5 sentences.\n",
                "\n",
                "    \n",
                "**NOTE**: \n",
                ">You should observe that as $PPR$ decreases (more confident predictions), you must also compromise on the number of points that your abstain model predicts confidently. \n",
                ">Range of PPR Thresholds to consider will be 0 to 0.5.\n",
                "\n",
                "</div>"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id=\"part1solutions\"></a>\n",
                "\n",
                "## PART 1: Solutions\n",
                "\n",
                "[Return to contents](#contents)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class='exercise-r'>  \n",
                "\n",
                "**1.1**\n",
                "\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# your code here \n",
                "df = pd.read_csv('data/flights.csv')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 72,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>ARRIVAL_DELAY</th>\n",
                            "      <th>DISTANCE</th>\n",
                            "      <th>SCHEDULED_TIME</th>\n",
                            "      <th>MONTH</th>\n",
                            "      <th>SCHED_DEP_HOUR</th>\n",
                            "      <th>SCHED_ARR_HOUR</th>\n",
                            "      <th>FLIGHT_COUNT</th>\n",
                            "      <th>DAY_OF_WEEK</th>\n",
                            "      <th>ORIGIN_AIRPORT</th>\n",
                            "      <th>DESTINATION_AIRPORT</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>23.0</td>\n",
                            "      <td>2586</td>\n",
                            "      <td>342.0</td>\n",
                            "      <td>9</td>\n",
                            "      <td>7</td>\n",
                            "      <td>15</td>\n",
                            "      <td>240</td>\n",
                            "      <td>3</td>\n",
                            "      <td>SFO</td>\n",
                            "      <td>JFK</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>10.0</td>\n",
                            "      <td>1235</td>\n",
                            "      <td>185.0</td>\n",
                            "      <td>5</td>\n",
                            "      <td>6</td>\n",
                            "      <td>11</td>\n",
                            "      <td>366</td>\n",
                            "      <td>4</td>\n",
                            "      <td>LAX</td>\n",
                            "      <td>DFW</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>-7.0</td>\n",
                            "      <td>184</td>\n",
                            "      <td>76.0</td>\n",
                            "      <td>4</td>\n",
                            "      <td>17</td>\n",
                            "      <td>18</td>\n",
                            "      <td>172</td>\n",
                            "      <td>7</td>\n",
                            "      <td>BOS</td>\n",
                            "      <td>LGA</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>68.0</td>\n",
                            "      <td>862</td>\n",
                            "      <td>148.0</td>\n",
                            "      <td>7</td>\n",
                            "      <td>19</td>\n",
                            "      <td>21</td>\n",
                            "      <td>260</td>\n",
                            "      <td>7</td>\n",
                            "      <td>IAH</td>\n",
                            "      <td>DEN</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>24.0</td>\n",
                            "      <td>236</td>\n",
                            "      <td>71.0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>20</td>\n",
                            "      <td>21</td>\n",
                            "      <td>266</td>\n",
                            "      <td>2</td>\n",
                            "      <td>LAS</td>\n",
                            "      <td>LAX</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "   ARRIVAL_DELAY  DISTANCE  SCHEDULED_TIME  MONTH  SCHED_DEP_HOUR  \\\n",
                            "0           23.0      2586           342.0      9               7   \n",
                            "1           10.0      1235           185.0      5               6   \n",
                            "2           -7.0       184            76.0      4              17   \n",
                            "3           68.0       862           148.0      7              19   \n",
                            "4           24.0       236            71.0      3              20   \n",
                            "\n",
                            "   SCHED_ARR_HOUR  FLIGHT_COUNT  DAY_OF_WEEK ORIGIN_AIRPORT  \\\n",
                            "0              15           240            3            SFO   \n",
                            "1              11           366            4            LAX   \n",
                            "2              18           172            7            BOS   \n",
                            "3              21           260            7            IAH   \n",
                            "4              21           266            2            LAS   \n",
                            "\n",
                            "  DESTINATION_AIRPORT  \n",
                            "0                 JFK  \n",
                            "1                 DFW  \n",
                            "2                 LGA  \n",
                            "3                 DEN  \n",
                            "4                 LAX  "
                        ]
                    },
                    "execution_count": 72,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 73,
            "metadata": {},
            "outputs": [],
            "source": [
                "df['DELAY_OR_NOT'] = df['ARRIVAL_DELAY'] > 15"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 74,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.drop('ARRIVAL_DELAY', inplace=True, axis=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 75,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>DISTANCE</th>\n",
                            "      <th>SCHEDULED_TIME</th>\n",
                            "      <th>MONTH</th>\n",
                            "      <th>SCHED_DEP_HOUR</th>\n",
                            "      <th>SCHED_ARR_HOUR</th>\n",
                            "      <th>FLIGHT_COUNT</th>\n",
                            "      <th>DAY_OF_WEEK</th>\n",
                            "      <th>ORIGIN_AIRPORT</th>\n",
                            "      <th>DESTINATION_AIRPORT</th>\n",
                            "      <th>DELAY_OR_NOT</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>2586</td>\n",
                            "      <td>342.0</td>\n",
                            "      <td>9</td>\n",
                            "      <td>7</td>\n",
                            "      <td>15</td>\n",
                            "      <td>240</td>\n",
                            "      <td>3</td>\n",
                            "      <td>SFO</td>\n",
                            "      <td>JFK</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>1235</td>\n",
                            "      <td>185.0</td>\n",
                            "      <td>5</td>\n",
                            "      <td>6</td>\n",
                            "      <td>11</td>\n",
                            "      <td>366</td>\n",
                            "      <td>4</td>\n",
                            "      <td>LAX</td>\n",
                            "      <td>DFW</td>\n",
                            "      <td>False</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>184</td>\n",
                            "      <td>76.0</td>\n",
                            "      <td>4</td>\n",
                            "      <td>17</td>\n",
                            "      <td>18</td>\n",
                            "      <td>172</td>\n",
                            "      <td>7</td>\n",
                            "      <td>BOS</td>\n",
                            "      <td>LGA</td>\n",
                            "      <td>False</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>862</td>\n",
                            "      <td>148.0</td>\n",
                            "      <td>7</td>\n",
                            "      <td>19</td>\n",
                            "      <td>21</td>\n",
                            "      <td>260</td>\n",
                            "      <td>7</td>\n",
                            "      <td>IAH</td>\n",
                            "      <td>DEN</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>236</td>\n",
                            "      <td>71.0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>20</td>\n",
                            "      <td>21</td>\n",
                            "      <td>266</td>\n",
                            "      <td>2</td>\n",
                            "      <td>LAS</td>\n",
                            "      <td>LAX</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "   DISTANCE  SCHEDULED_TIME  MONTH  SCHED_DEP_HOUR  SCHED_ARR_HOUR  \\\n",
                            "0      2586           342.0      9               7              15   \n",
                            "1      1235           185.0      5               6              11   \n",
                            "2       184            76.0      4              17              18   \n",
                            "3       862           148.0      7              19              21   \n",
                            "4       236            71.0      3              20              21   \n",
                            "\n",
                            "   FLIGHT_COUNT  DAY_OF_WEEK ORIGIN_AIRPORT DESTINATION_AIRPORT  DELAY_OR_NOT  \n",
                            "0           240            3            SFO                 JFK          True  \n",
                            "1           366            4            LAX                 DFW         False  \n",
                            "2           172            7            BOS                 LGA         False  \n",
                            "3           260            7            IAH                 DEN          True  \n",
                            "4           266            2            LAS                 LAX          True  "
                        ]
                    },
                    "execution_count": 75,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class='exercise-r'>  \n",
                "\n",
                "**1.1.2**\n",
                "    \n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# your code here\n",
                "ohe_cols = ['MONTH', 'SCHED_DEP_HOUR', 'SCHED_ARR_HOUR', 'DAY_OF_WEEK', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT']\n",
                "# ohe_cols = ['MONTH', 'DAY_OF_WEEK', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 77,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.get_dummies(df, columns = ohe_cols, drop_first=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(df.drop('DELAY_OR_NOT', axis=1), df['DELAY_OR_NOT'], test_size=0.2, random_state=111)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "#scale the x_train and x_test\n",
                "X_train = StandardScaler().fit_transform(X_train)\n",
                "X_test = StandardScaler().fit_transform(X_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "# deal with null values in X_train\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class='exercise-r'>  \n",
                "    \n",
                "**1.2**\n",
                "    \n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2022-11-29 20:21:18.707191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
                        "Your kernel may have been built without NUMA support.\n",
                        "2022-11-29 20:21:19.170192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
                        "Your kernel may have been built without NUMA support.\n",
                        "2022-11-29 20:21:19.170459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
                        "Your kernel may have been built without NUMA support.\n",
                        "2022-11-29 20:21:19.205307: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
                        "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
                        "2022-11-29 20:21:19.225345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
                        "Your kernel may have been built without NUMA support.\n",
                        "2022-11-29 20:21:19.226104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
                        "Your kernel may have been built without NUMA support.\n",
                        "2022-11-29 20:21:19.226513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
                        "Your kernel may have been built without NUMA support.\n",
                        "2022-11-29 20:21:23.435101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
                        "Your kernel may have been built without NUMA support.\n",
                        "2022-11-29 20:21:23.438338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
                        "Your kernel may have been built without NUMA support.\n",
                        "2022-11-29 20:21:23.438534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
                        "2022-11-29 20:21:23.439800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
                        "Your kernel may have been built without NUMA support.\n",
                        "2022-11-29 20:21:23.443062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4604 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
                    ]
                }
            ],
            "source": [
                "# build your NN \n",
                "# your code here\n",
                "#dense feed-forward network with two hidden layers with 15 nodes in each hidden layer\n",
                "NN_model = tf.keras.models.Sequential()\n",
                "NN_model.add(tf.keras.layers.Dense(15, input_dim=X_train.shape[1], activation='relu'))\n",
                "NN_model.add(tf.keras.layers.Dense(15, activation='relu'))\n",
                "NN_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/10\n",
                        "200/200 [==============================] - 10s 36ms/step - loss: 0.3241 - accuracy: 0.8636 - val_loss: 0.3409 - val_accuracy: 0.8438\n",
                        "Epoch 2/10\n",
                        "200/200 [==============================] - 4s 22ms/step - loss: 0.3023 - accuracy: 0.8717 - val_loss: 0.3724 - val_accuracy: 0.8275\n",
                        "Epoch 3/10\n",
                        "200/200 [==============================] - 5s 24ms/step - loss: 0.2875 - accuracy: 0.8784 - val_loss: 0.3994 - val_accuracy: 0.8175\n",
                        "Epoch 4/10\n",
                        "200/200 [==============================] - 3s 15ms/step - loss: 0.2752 - accuracy: 0.8852 - val_loss: 0.4172 - val_accuracy: 0.8163\n",
                        "Epoch 5/10\n",
                        "200/200 [==============================] - 4s 19ms/step - loss: 0.2630 - accuracy: 0.8905 - val_loss: 0.4471 - val_accuracy: 0.8069\n",
                        "Epoch 6/10\n",
                        "200/200 [==============================] - 4s 22ms/step - loss: 0.2530 - accuracy: 0.8961 - val_loss: 0.4635 - val_accuracy: 0.8019\n",
                        "Epoch 7/10\n",
                        "200/200 [==============================] - 4s 18ms/step - loss: 0.2418 - accuracy: 0.8991 - val_loss: 0.4850 - val_accuracy: 0.8000\n",
                        "Epoch 8/10\n",
                        "200/200 [==============================] - 3s 14ms/step - loss: 0.2317 - accuracy: 0.9047 - val_loss: 0.5033 - val_accuracy: 0.7956\n",
                        "Epoch 9/10\n",
                        "200/200 [==============================] - 3s 14ms/step - loss: 0.2225 - accuracy: 0.9100 - val_loss: 0.5355 - val_accuracy: 0.7831\n",
                        "Epoch 10/10\n",
                        "200/200 [==============================] - 3s 14ms/step - loss: 0.2132 - accuracy: 0.9153 - val_loss: 0.5583 - val_accuracy: 0.7825\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "<keras.callbacks.History at 0x7fe7e01325f0>"
                        ]
                    },
                    "execution_count": 30,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# compile it and run it\n",
                "# your code here \n",
                "NN_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
                "NN_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1, validation_split=0.2)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
                        ]
                    },
                    "execution_count": 31,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "NN_model.history.history.keys()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[<matplotlib.lines.Line2D at 0x7fe7ac1c9de0>]"
                        ]
                    },
                    "execution_count": 35,
                    "metadata": {},
                    "output_type": "execute_result"
                },
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 432x288 with 1 Axes>"
                        ]
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# plot train and val acc as  a function of epochs\n",
                "# your code here\n",
                "plt.plot(NN_model.history.history['accuracy'])\n",
                "plt.plot(NN_model.history.history['val_accuracy'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "250/250 [==============================] - 3s 10ms/step\n",
                        "NN_model_train_auc: 0.9582647021628873\n"
                    ]
                }
            ],
            "source": [
                "# primer to print: \n",
                "# print(\"NN_model_train_auc:\", roc_auc_score(y_train, y_hat))\n",
                "# your code here \n",
                "y_hat = NN_model.predict(X_train)\n",
                "print(\"NN_model_train_auc:\", roc_auc_score(y_train, y_hat))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "63/63 [==============================] - 1s 14ms/step\n",
                        "NN_model_test_auc: 0.7487853836138303\n"
                    ]
                }
            ],
            "source": [
                "# your code here\n",
                "y_fit = NN_model.predict(X_test)\n",
                "print(\"NN_model_test_auc:\", roc_auc_score(y_test, y_fit))\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class='exercise-r'>\n",
                "\n",
                "**1.3**\n",
                "\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [],
            "source": [
                "DELAY_OR_NOT = y_hat > 0.5"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/munch/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
                        "  y = column_or_1d(y, warn=True)\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "0.7298849881853691"
                        ]
                    },
                    "execution_count": 41,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Fit the logistic regression model\n",
                "# your code here\n",
                "#use ridge  regression\n",
                "logreg = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=1000)\n",
                "logreg.fit(X_train, DELAY_OR_NOT)\n",
                "y_p = logreg.predict(X_test)\n",
                "roc_auc_score(y_test, y_p)\n",
                "\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class='exercise-r'>\n",
                "\n",
                "**1.4**    \n",
                "\n",
                "**1.4.1**\n",
                "\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# your code here\n",
                "#apply permutation importance to the model \n",
                "scores = permutation_importance(logreg, X_test, y_test, scoring='roc_auc')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(2000, 866)"
                        ]
                    },
                    "execution_count": 47,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "X_test.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[<matplotlib.lines.Line2D at 0x7fe78c16dd50>]"
                        ]
                    },
                    "execution_count": 44,
                    "metadata": {},
                    "output_type": "execute_result"
                },
                {
                    "data": {
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAwjklEQVR4nO3dd5xU9fX4/9fZ2UIHadIFBAtWFEU/GjUqiomKJiZBTTSJCWn+YmLySTAakxhLjPlp9KPGGHuFaCxEUQwCsdEWRGkiC1KWzu5SdpctM3O+f9w7d+7MzuzOdnfueT4e+9iZ2+beO3fe577rFVXFGGNM8OS09w4YY4xpHxYAjDEmoCwAGGNMQFkAMMaYgLIAYIwxAZXb3jvQGH379tXhw4e3924YY0yHsmTJkt2q2i95eocKAMOHD6ewsLC9d8MYYzoUEdmYaroVARljTEBZADDGmIDKKACIyEQRWSMiRSIyNcX8AhGZ7s5fKCLD3ekTRGSJiCx3/5/tW2eeu81l7l//FjsqY4wxDWqwDkBEQsADwASgGFgsIjNUdZVvsWuAMlUdJSKTgTuBbwC7gYtUdauIHA3MAgb71rtSVa1Q3xhj2kEmOYCTgSJVXa+qNcA0YFLSMpOAJ93XLwLniIio6oequtWdvhLoLCIFLbHjxhhjmieTADAY2Ox7X0ziXXzCMqoaBvYCfZKW+SqwVFWrfdMed4t/fisikurDRWSKiBSKSOGuXbsy2F1jjDGZaJNKYBE5CqdY6Ae+yVeq6jHAF9y/b6VaV1UfVtVxqjquX786zViNMcY0USYBYAsw1Pd+iDst5TIikgv0BErc90OAl4GrVHVdbAVV3eL+3w88h1PU1O42lVTyzqeW0zDGZL9MAsBiYLSIjBCRfGAyMCNpmRnA1e7ry4A5qqoi0gt4HZiqqu/HFhaRXBHp677OAy4EVjTrSFrIWX+Zy1WPLWrv3TDGmFbXYABwy/SvxWnBsxr4p6quFJFbRORid7FHgT4iUgRcD8Sail4LjAJuTmruWQDMEpGPgWU4OYh/tOBxNVnUno9jjAmIjIaCUNWZwMykaTf7XlcBX0ux3q3ArWk2e2Lmu2mMMaalWU9gY4wJKAsAxhgTUBYAjDEmoCwAGGNMQFkAMMaYgLIAkIaqtQc1xmQ3CwBpRKxDgDEmy1kASMPSf2NMtrMAkEbUioCMMVnOAkAaFgCMMdnOAkAaVgRkjMl2FgDSsEpgY0y2swCQhjUDNcZkOwsAaVgGwBiT7SwApGFFQMaYbGcBIA0rAjLGZDsLAGlELAAYY7KcBYA0rATIGJPtLACkEbUIYIzJchYA0rCewMaYbGcBIA3LABhjsp0FgDSsGagxJttZAEjDmoEaY7KdBYA0LANgjMl2FgDSsCIgY0y2swCQhrUCMsZkOwsAaVj6b4zJdhYA0rChIIwx2c4CQBpWBGSMyXYZBQARmSgia0SkSESmpphfICLT3fkLRWS4O32CiCwRkeXu/7N965zoTi8SkftERFrsqFqADQVhjMl2DQYAEQkBDwAXAGOAy0VkTNJi1wBlqjoKuAe4052+G7hIVY8Brgae9q3zN+D7wGj3b2IzjqPFWfpvjMl2meQATgaKVHW9qtYA04BJSctMAp50X78InCMioqofqupWd/pKoLObWxgI9FDVBer0uHoKuKS5B9OSrAjIGJPtMgkAg4HNvvfF7rSUy6hqGNgL9Ela5qvAUlWtdpcvbmCbAIjIFBEpFJHCXbt2ZbC7LcOKgIwx2a5NKoFF5CicYqEfNHZdVX1YVcep6rh+/fq1/M6lYem/MSbbZRIAtgBDfe+HuNNSLiMiuUBPoMR9PwR4GbhKVdf5lh/SwDbblTUDNcZku0wCwGJgtIiMEJF8YDIwI2mZGTiVvACXAXNUVUWkF/A6MFVV348trKrbgH0icorb+ucq4NXmHUrLsjoAY0y2azAAuGX61wKzgNXAP1V1pYjcIiIXu4s9CvQRkSLgeiDWVPRaYBRws4gsc//6u/N+DDwCFAHrgDda6qCaKmEEUEv/jTFZLjeThVR1JjAzadrNvtdVwNdSrHcrcGuabRYCRzdmZ1tbYvpvEcAYk92sJ7CPP8mPRtttN4wxpk1YAPDxFwHZ/b8xJttZAPBJyAFYJbAxJstZAPDxJ/qW/htjsp0FAJ/ERN8igDEmu1kASMN6Ahtjsp0FAJ+EZqAWAIwxWc4CgI+/7b9VAhtjsp0FAJ+odQQ2xgSIBQCfhH4AlgMwxmQ5CwA+/iTf0n9jTLazAOBjYwEZY4LEAoCfL823sYCMMdnOAoCP/67f7v+NMdnOAoCPvxWQNQM1xmQ7CwA+9kAYY0yQWADwsdFAjTFBYgHAxzIAxpggsQDgk1AJbBHAGJPlLAD4qFUCG2MCxAKAjxUBGWOCxAKAT2IRkIUAY0x2swDgY88DMMYEiQUAn8TB4CwCGGOymwUAn6ivK7A9EtIYk+0sAKRh6b8xJttZAPBJrAOwEGCMyW4WAHysI5gxJkgsAPhYRzBjTJBkFABEZKKIrBGRIhGZmmJ+gYhMd+cvFJHh7vQ+IjJXRMpF5P6kdea521zm/vVvkSNqBk3z2hhjslFuQwuISAh4AJgAFAOLRWSGqq7yLXYNUKaqo0RkMnAn8A2gCvgtcLT7l+xKVS1s5jG0mKhaEZAxJjgyyQGcDBSp6npVrQGmAZOSlpkEPOm+fhE4R0REVStU9T2cQPC5Z0VAxpggySQADAY2+94Xu9NSLqOqYWAv0CeDbT/uFv/8VkQk1QIiMkVECkWkcNeuXRlssjks0TfGBEd7VgJfqarHAF9w/76VaiFVfVhVx6nquH79+rXqDiXkAKwnmDEmy2USALYAQ33vh7jTUi4jIrlAT6Ckvo2q6hb3/37gOZyipnZllcDGmCDJJAAsBkaLyAgRyQcmAzOSlpkBXO2+vgyYo/X0pBKRXBHp677OAy4EVjR251uaVQIbY4KkwVZAqhoWkWuBWUAIeExVV4rILUChqs4AHgWeFpEioBQnSAAgIhuAHkC+iFwCnAdsBGa5iX8ImA38oyUPrCmsEtgYEyQNBgAAVZ0JzEyadrPvdRXwtTTrDk+z2RMz28W2Yw+EMcYEifUE9rEHwhhjgsQCgI89EMYYEyQWANKwOgBjTLazAOCT0AqoHffDGGPaggUAHysCMsYEiQUAH3smsDEmSCwA+KgVARljAsQCgI8/0bexgIwx2c4CgI/lAIwxQRLIAHDv7LUs2VhWZ7oNBWGMCZLABQBV5Z7Zn/LVv31Qd17Ccm23T8YY0x4CFwCqaqNp51mib4wJksAFgPLqMAD5uXUP3V8HYEVAxphsF7gAUOEGgC75Ie5+aw3Dp77uzYtaRzBjTIAELgDEcgBd8kLcN6cIiDf59I8GajkAY0y2C1wAiOUAOueHyHEfQx+O3frb8wCMMQESvABQEysCyiVHnAgQu9u3VkDGmCAJXACoCTutgHJDgpv+E4kVASXUAVgEMMZkt8AFgFi6rgriRoCI1q0DsPTfGJPtghcAfP9jdQCxSuCo9QQ2xgRI4AKAR9WrA4gXAdlYQMaY4AhcAPDf2NcJAL7lLAdgjMl2wQsAxBN7rxJY6+YALAtgjMl2gQgAc9fs5MpHFqCqCZXAyTmAqG+YIEv/jTHZLre9d6At/ODpJdSEo1SHo75KYPVVArv/bSwgY0yABCIHECvaEYm/TsgBaN1WQJb+G2OyXSACQNRX7OMn9bQCshyAMSbbBSIARHy39okdwZzX0VQ5gLbaOWOMaSeBCAAxqomtgLzB4CKxAGARwBgTHBkFABGZKCJrRKRIRKammF8gItPd+QtFZLg7vY+IzBWRchG5P2mdE0VkubvOfRIrj2lF0YRWQEoozWBwOWJFQMaY7NdgABCREPAAcAEwBrhcRMYkLXYNUKaqo4B7gDvd6VXAb4Ffptj034DvA6Pdv4lNOYDGUBLrAdLVAeTm5FglsDEm62WSAzgZKFLV9apaA0wDJiUtMwl40n39InCOiIiqVqjqeziBwCMiA4EeqrpAnVT3KeCSZhxHRlTjw72pQo579PFWQM7/nBzLARhjsl8mAWAwsNn3vtidlnIZVQ0De4E+DWyzuIFttjgnBxBP2L3nASR1BMvNybEqAGNM1vvcVwKLyBQRKRSRwl27djVrW04lsPsaJVbp4PUEjuUABHsegDEm62USALYAQ33vh7jTUi4jIrlAT6CkgW0OaWCbAKjqw6o6TlXH9evXL4PdrYfiRYBUHcFiaX4oR6wOwBiT9TIJAIuB0SIyQkTygcnAjKRlZgBXu68vA+ZoPbfQqroN2Ccip7itf64CXm303jdSVDWxGWhO0lhA7i6HrAjIGBMADY4FpKphEbkWmAWEgMdUdaWI3AIUquoM4FHgaREpAkpxggQAIrIB6AHki8glwHmqugr4MfAE0Bl4w/1rVf5WQKpKqE4AcOaFrBLYGBMAGQ0Gp6ozgZlJ0272va4CvpZm3eFpphcCR2e6oy3B3woISPFQeDcHIFYEZIzJfp/7SuCWlJAD8E2PeKOBOv9zcsRyAMaYrBesAOAbCoIUzwNQrw6g1TslG2NMuwtYANCEHECsI5g3GFw0HgAsB2CMyXbBCgD4+gGkeCi8VwlsdQDGmAAIVgDwVQI4o4GmawZqAcAYk/2CFQBIbAXkPRTeqwNw3lsRkDEmCIIVADTxgTB1HwnpywG0yx4aY0zbCVQAcJ4HEG/zH38ofPLzAMTGAjLGZL1ABYCEweDU9zyAVDkAS/+NMVkuUAEAkouAnNfRFHUAlv4bY7JdoAKAPwcA8TqAsPc8gPhQEFYJbIzJdsEKACipHgiT3A8gN2RFQMaY7BesAKD+1+o1A40m1QFYJbAxJggCFQCiyUNBeDkAd5obFHIEqwMwxmS9QASAIQd1BmJDQcQrfHPq5ACcoCBWB2CMCYBABID/Pf9wIKkjGBpvBuobCkIAAasDMMZkvUAEgFhCT/JQEO7/iK8jWCwHYAHAGJPtAhEAYkU9yUNBxCTkAMQZIyiTIqBwJEpNONrSu2uMMW0iEAFAiD360VcHQLyiN9YTODY+UKbPg7nkwfc57KZWf5SxMca0imAEgFgOAE3IAcSaekZ9HcFyxAkYmeQAVmzZ1yr7a4wxbSEYAcD9n5imx+sD/B3BnDoAqwQ2xmS/YASAhDoA9b12pvsHg3P6ATRuLCDrNGaM6YgCEQBieQB/ERDUfRawqpLjlAE1qh9AVa1VBBtjOp5ABICEVkDuNH/yHvF1BBPcHsKNuKkvrw63xG4aY0ybCkQAiPUDSGwGGs8NxPsBOA+KFxqXA6iwAGCM6YCCEQDc/+p7KrB/WAh/JbC4zUAbU6pfUWMBwBjT8QQjAKTpCBbPAcSmuc1AGzkWkNUBG2M6omAFAPyPhNQ6w0BHo3hFQI1J1G3gOGNMRxSQABCrA4jf9ivUqQOI+nIAjQsALbm3xhjTNoIRANz/0aSHwicPBRGrA3A6gmWeqlsOwJiOZ+mmMo7+3SxKK2rae1faTUYBQEQmisgaESkSkakp5heIyHR3/kIRGe6bd4M7fY2InO+bvkFElovIMhEpbJGjSb//7qt4y5+oLwL4+wE05YEw1hGs4/tsdwX7q2rbezdMG3po3jrKq8Ms+qy0vXel3TQYAEQkBDwAXACMAS4XkTFJi10DlKnqKOAe4E533THAZOAoYCLwoLu9mC+q6vGqOq7ZR1LfMbj/1TcYXFTjLYJiD4WPqBLKkYzHAoqx9L/j++Jf5nHFPxa2926YNmQ/28xyACcDRaq6XlVrgGnApKRlJgFPuq9fBM4R57Z7EjBNVatV9TOgyN1em0qoBPZyAPGy+1gOIBJVQk0YC8jqALLD8i1723sXjGlTmQSAwcBm3/tid1rKZVQ1DOwF+jSwrgJvicgSEZmS7sNFZIqIFIpI4a5duzLY3bpy/B3BYh+u6hXdhL0iIMjJcR8I04jtWx2AMR2XV0IcQO1ZCXy6qp6AU7T0ExE5I9VCqvqwqo5T1XH9+vVr0gfFK4GThoP2TQcnB5DjPhDGKoGNMdkukwCwBRjqez/EnZZyGRHJBXoCJfWtq6qx/zuBl2nNoqGEsYB8dQBuul3r9gSLqHoPhGlMmm7pvzEdj/1uMwsAi4HRIjJCRPJxKnVnJC0zA7jafX0ZMEedW+gZwGS3ldAIYDSwSES6ikh3ABHpCpwHrGj+4aQmvtFA8dUB1HkeQLRplcCWA+jYolaJE2gBLgEit6EFVDUsItcCs4AQ8JiqrhSRW4BCVZ0BPAo8LSJFQClOkMBd7p/AKiAM/ERVIyJyMPCy2zwzF3hOVd9sheMDfGV8mljzH/vhJ7cCamwzUEs/OjYL4CaoGgwAAKo6E5iZNO1m3+sq4Gtp1r0NuC1p2nrguMbubFP50v+Esn1/2b/zPvZEMGnUXaElIB2bBXATVIHoCZyTE3sofOIDYWIJf20kXgSU42syminrCNaxWQA3QRWIAJDYESwungNwK4GjsSKgxpUBRe2BYB2aBQATVBkVAXV0qTqCQTwH4K8DiD0Uvr5EQVV5+J313ntLQDq2iJUBBZR974HIAXjPBPYN/wDxst9wJD4WUCaVwMVlB7jjjU/qbMd0TPb9maAKRABIlwMI+4p+Yv+9SuBGjQVkKUhHZt9fsEmAuwIHIgDkSOqa3VjZvRcIvKEg6u8kkhtKvGDsDrJjsyIgE1SBCACJQ0HU0ww0qoTE6ThWXwBITjCsDqBjs/Q/mOxnG5QA4EaATaWVPDl/ozc9uRlorBWQ0wgo/dWR3OrHAkDHZkVAwRbcAqCgBAD3K/7Dv1clTK/bESw+FlB9d4XhpAhg6UfHFrEvMNCC/O0HIwCkCfFhrxmok6DHAkCogUrg5HmWA+jYrAgomPxDwzfF5tJKhk99ndc/3tZyO9XGAh0AvLGAkoqAcnKcOoB0w0GE69QBtNy+mrZng8EFW1O//tXb9gHw6rLkwZE7jmAEgDSlfF4/gKhTORx1WwHluuNBpCsasErg7GLfX7AF+fsPRgBIWwQUL8t3HhHptAKKjR2Urnlg8vRsqUTcW1lLTTh441pYBiDYmhsAOvLlE+gAEGv9A04wiHUEy20gAGRrEdBxt7zFNU8ubu/daHPWDyDYgvz1ByMApCkC8v/wayPqjAYaGwyOugl9THKZcTZlId9du7u9d6HNZUsOzjRNkL//YASAehr69uqSB8D8dSXOA2GScgBLN5XxflFiopitOYCgsu8vWKJRpbImHH/fzgFgefFedu6rapfPDkQAyKknAEw48mAA1u7cTyTqlP+HQs5piUSVrzz4AVc+sjBhneQcQDbcQWTDMTSVFQEFy52zPmHMzbO8INDc4dyb+9O56P73OPfu/zZvI00UiABQX1+/Hp3z6JIfoqS8BlXngTAhqb8O4MPNexLeZ0MzQn99SEe1YXcFtZHG/5rb+w7QtK0XC4sBqKiOAO3z/Z/9l3mMu/U/3vt9VeF6lm49gQgA9RUB5eYIfbrlU1pR4z0TOFUzUP8d8l2z1iRsIwvS/zq9mzuaHfuqOOsv87jt9dWNXtcCQDDFhntp6tffnFFE1++uYHd5TZPXbynBCAD1zMsNCb27FrC7vNprBeQ1A/XdFY+4YSZrd+xPuY1sSEA6eg6grNL5MX2wrvGV2NkQwE3jxTqANuX3u2b7fkrKq1t6l9pcQJ4Ilj4EhHJy6NU5j7LKGmc00Ho6gq3YupfRB3evs40sSP+bVHTyeZKupVcmsiGAZ6sPN5Xx4aY9fPf0ES22zVhyUBOJDQHT+G2c/9d3WmRf2rv+KRA5gPoqgfNyhO6dctlfFSaqeENBQPxZwTFd8lPHy2xIQMIdPAcQ05SvoqXqcKJRZfjU13ni/c9aZHsGLn3wA255bVXDCzZC7BqpjcTHAGsv7X3jFYgAUN/dYSgkdO+Ux/6qWiKqiOBrBpq4bFVtJOU2sqEIob0vxOZqzo+4pb6/qrBzfdw2s/H1EKbt1Ybjj4JtLzUWAFpfp/z0h5mXk0OPTrnsqwq7D4TxdwRL/HJirQaS1ffsgI4iXae39vbtxxfx1srtDS7XnBxMS90BBnEYjbYSboWEsjapCGjFlr1s2F3RpvtS287XTCACQPeCvLTzckNOEVBNOEo4qnTKC6UdCqKiOsya7XUrglvjBqKiOsziDaUtv+E0GnNRF5dVtknZZW0kyrw1u5jy9JIGl43dSa3dWc6Mj7Y26nOSi4DKKmqaNMRvtQWAVlMdjjL3k53M/WRns7cVqwOIBYDYtXzh/73HWX+Zl9G+tBR/44v2yIUHIgB0ykt/mLk5ThFQTLeCXEJpAkB5dThl5U9r9AP4+fRlfO2h+W3W0iDTVkA791Vx+p1z+fObn7TyHsGBNEVu4AzBu23vAe+9P4D9YcbKBrc955MdXP/PZd4osH4/enYJP3luaca9M99csZ3CDaVU11oAaElvLI8H4epwlO88sZjvPNH8sapizS9rm9gKqG4AaPrv/40V8WO87KH5Td5OUwUiANTXCig3lEP3TvHK3foCwP40nTVii725YjtVtRHW7tjP0k1l/O8LHzF86utN2uePivcAUFnjJIIV1WF+9eJHlFW0TtvhTPsBlLifP3dN8+/EGnKgJnUAqA5HuG7aMr7+9/m88uEWIlFNCGCd8kINbvu7TxTy0tItlFeH67T2Wrerwv2chs+JqvLDZ5Zw2UPzqXbrALKgTcDnwie+3Hbs3GZCk5797fex+7uC+B13Y7+vlizq8z+l8KOkDqZtIRABoD6hHKGHLwfQtSDXKwJKvit+LE3rjqgqyzbv4YfPLOHW11cx4Z53+MqDH/DCEqfHYVMqmWJBJRYApi/ezD8Li3lgblGjtlO4oTSjHEqmOYBMfzQ3vrycqf/6OO38op3lPPpe/a1l0gWA2PTNpQf42fRlPPbeZwkJRIGb4/vbvHUMn/p6veffaf2VNLaTe77Kq52Av3rbvrQ5sc2l8VxIaxQBbSypSNu34Y3l2/jVix81eptLNpZy48vLm135+eqyLSwv3tuodarDkQaLD/dV1aKqdM6PB/KqRuSufvPyCkbcMBNwcoan/WmO99CWbXvjubpYvdf63eUZbxsaF4w+7wIfAPLcOoCYrgUhrxloulY/yVTjg0st37KvzvyKNAmZX3l1mAfnFXlFGbHfZiwRiv1owlHl7v98yva96YsnVJUDNRH+++kuLntoPk/N31Dv50ajyuUPL2hwH/37E1svlUhUeXbhJqYt3kx5dZgPN5V58zbsrmDcrbM59+7/8sfXViUMyuVXG4ky4Z74+Cgf+AbkSy4aWrKxjGueLPTed8p1Eo473WKqypoIVbWRlJ+1r6qWX72YGKhiOYLYMV5w77tMeuB9b/6mkko+2b6PBetL2FRa6U2PXS/hqHNDALC7vJp/uTcCTXHmXfO44h8L60yvqo3wo2eX8s/C4kYXQV720HyeXbgpo+sy2Y59VRSXVXLHG6u5btoyLrr/vUatf/hNb/KjZ9LX6ZRW1HDs79/iwXnr6JQbT57SJbqbSyt5duHGhGnPL9oEOIl/WWUtW/Yc4Lppy1iwviTljcvzizY3qk4rVaCvCUdbpDWRv1izLWQUAERkooisEZEiEZmaYn6BiEx35y8UkeG+eTe409eIyPmZbrOljTvkoJTTQzk5deoAYjmAfVW1Kdf51cTDOffI/t77ipoIa3c4dxGpsnF7D9RSWlHD84s2pb1I7vnPp/z5zTW86bV4cZarqE5MtJZsLOO+t9dy0yvLeXFJMXe++QnTFm3i9zNWMn9dCQC/eXk5R978pnfn+F5Ribf+w++s47ppHwLOj/no383iz7PWZNwcrdwtBtu2t4qTb5/tHc9/Vu1gwXrnc0p9xVTffGQhlz74gXcc0ws3s9t3N12Spjv81j0HEnIlVzyykF37q1mxZS9TnkpMQGK9gGOS63xKK2r48n3vcvTvZtX5nDXb97Nrf+LdfawHeHl1mG+7Zc7FZc4Ps7ImzBl3zWXiX99l8sMLEn6w/oTh/jlrAacu5xcvfMTiDaX8Z9UOxt8+m82+oJFs/roSfvzsEqJRTVguuYLQP0Bhqut09qodfFy8J2XFYuwS3Hug7nqrt+3jln+vSggq2/dWMc8t8ht/+9ucfudc/v7f9XXWnfPJDnbuq6pzzcbsrXQ+761VO+rM2763ivfW7mbrHud8vrS0OOF8+utXfvvKCu/1958q5MaXVyRcc97nHahNuDYmP7wgbSBZuTV1TqakvJoL7n2Xop1OcZSqct49ifWAlTURDrvpDR6ct67O+m+v3pHQKOG+t9fWWyx86h1z0s5rDQ32BBaREPAAMAEoBhaLyAxV9ffOuAYoU9VRIjIZuBP4hoiMASYDRwGDgNkicpi7TkPbbFFPfvdkDtRGGHfr7ITpsY5gMV0Lcr07zB1uJeD9V4zl2uc+9Jbp1TmfrgXxdRoqythbWctNs5Yzd80uenXOo0fnPLbsOcCk4wdR4N6txi7g2I8nmpQD2FDilEvvOVDjzf/lC4nZ/yc+2MDC35zD84s2A3hBafbqHXUuunsnj2XdTmf+Q/+te+ECLPqslGOH9GR/VZiDuuRRHY5SuDF+N7+7vIY3V2zngmMG8v2nCr1z1bNzPKDG7oQ3lVby9b/Pr3P3tKu8mqG9u3jvq8MRHphTxFGDe9bZn5Num11nGsSfzRqzdNOehOP9wp/neq+jUeXTnfGy5SW+4wF4YG4R+91zXlEd5p1PdyXMT64H+l8399A5L5Qycdm5zwkuX/NV8D23aBNLNpSxaEMpV44fxm2XHsPLHxazcH0pry7byoHaCJsnVnrnDpzrY2NJJWOH9SIvlJOw3yUVNSzdVMb3nixkyU0T6NUlj++538f4Eb2Z/oNT6+wXONfl4F6dE6b99PkPWbuznG+deggj+nYFYPLD89lQUsnqWyam3I5zXmr57hPxXNiGP32Z2at28Oh7n/HEd0+iIDdUJ5ENR6LkhnKIRpVT7ngbgOsnHObN9+dQ/EHl6QUbOX10X84/aoD3fWzbe4DeXfMTtv/l+97jr5OPT5j23MJNKfd/7ifx77myJsy7a3fTvVMun+2uYPW2fdw/p4ifTzgs4Xcfs7HECdR3zVrDsN5duOi4QagqeyprvZzpxccNApwA0JCq2liONUJtJMqyzXuYdPzgBtdrCmko2yIipwK/V9Xz3fc3AKjqHb5lZrnLzBeRXGA70A+Y6l82tpy7Wr3bTGXcuHFaWFhY3yINmr1qB//+eCuvLnOi8sPfOpHxI/pw3C1vAbDyD+ezblc5F9//PgN6dGL7viqmTzmFypqI1wLh3snH88qHW5i7Zlfaz/EbM7AHq7bVLRoC+OYpwwB459PdXnHCFeOHeRdq9065XHTcIO99fm5Oi1RCnTCsF2t3lHuJnd/3vzCCkooaXloaf9h1324F7KtK/chI//6mc+IhB9VJbGNG9+/G+JG9iarz/ezc3z5jrOSHclqsY05+bg6Tjhvk1QPV5+QRvVn0WWKT36G9OyfUL/hdOX4Yz/rO98SjBng5xyEHdWbssIP4t++u8zunDWfrngP06pzPR8V7vMrVQ/t1pV/3Agb17ExpZQ2XHD+Yn01fBjit4/p1L2DssF7MXO5s+9Kxg3n5w7oPQP/zV4/l3x9vTXiYUP/uBd732L0glwlHHZxwPcUM6NGJqnCEPZWpc9v1+ek5oxMS1K+PG8K+A2FfLrp9nDqyD0s2liVcS9ecPoI3V2xny56mFfGs/MP5KYNPpkRkiaqOqzM9gwBwGTBRVb/nvv8WMF5Vr/Uts8Jdpth9vw4Yj5PYL1DVZ9zpjwJvuKvVu03ftqcAUwCGDRt24saNG5MXabTNpZXeXeHbvziTkX27epVGG/70ZVZs2cuF/xcv23z7F2dyaL9uvLliGw/OW8f9l59AbTTKba+vprw67P14jx3Sk5pwNKH1Qib6uHcuJRU1dCvIpSA3h3BUU2bR+3TN91ri5OZIszpw9e7qjILas3Me5x91MKP6d+P2ma3fvDOV2DmoCUfrBKVBPTsxZlAPZq9u3ZZHfbvle00EMw0GFx47kAljDua6actadd/q478mWkP3gtyUNwpNcVCXPHp2zmNDSWIx2HXnjObVZVu86S35ma3p2CE92ba3yitG7FaQS440bnjnUf27UbSz/orof/3ofzgxTTF2JtIFgM/9YHCq+jDwMDg5gJbY5tDeXbj90mM4fEA3Du3XDYBFvznHK/rJDSU2Gx3mFlFMPHogE48e6E1/7Nsn1fs5/kADzp3B1AuO8CoUjx7cg3snj/X2wU9VvaAUM+tnZ3D4gO5e8cba2y5gd3kN2/dWccyQnsxds5PvPL6Yob07M+6Q3kwYczBfOsbZ3+QioAuOHsCDV56Q0EQ2HIly5MAe/PDpJV72+7nvj+d/Du1b73E+vWAjv31lBeceeTCzV8fLd++dfDzXTVvGjV86kleWbWHl1n088Z2TOOtwp/7krLvmsqGkkkevHsc57oN5AEbfOJPaiPLMNeM5UBvh3CP7J+znxL++4wXZuy47ln+8u55P3eKuvt0KKLzpXG/ZeWt20qtLPscP7YWq8uqyrUx96eOEViW3X3oMV4x3cmK7y516hjMP68eF//ceK7fu4/4rxlJeFWbqS8vrHPufvnosnXJzEgLArycewZ1vfsJ154xm3pqdfOS2lPnsji953+mbP/sCRwzowZKNpXz1b/HioT9ecrRXxj28Txc2llbyyFXjEiq5R/brypxfnEVVbYRQjpAXyvG+3y8fO5B1O8tT3oT89RvH8++PtnLThWP47SsruHTsYH7xQupWRAtuOIfCjaUcP7QXQw5yrv9z/v95XhNZgIe+eQI/fGZpwnqzrz+TUf27cfdbazhiYA9+/Gx8/oQxB/OPq+JpUGyfY9f1zyccxo59VeSFcujdNZ/XPt7KpzvKGTu0Fy99uIW7LjuWbz++iAXr63aQ/OYpw3hmQepc6EPfPJGJRw9gy54DnPYnp4z9vV9/kYE9O3Pob2amXAfgjMP6ccdXjvHWSfb6T0/nqEF1iyrB+e1f/fgi1u+q4OlrTuaPr63yXaPOjcak4wdx7+SxgHPj8/W/z08o9nv3V1/kQG2E8+55h5nLt3HCsF7NGoI6lUwCwBZgqO/9EHdaqmWK3SKgnkBJA+s2tM1WFfvBx/Tv0cl7fVCXeFniJ3+cSF6oaY2lenZJ7IH8/JRTEt6/8uPTyE2z7VRfdOek9u0iTja9X/cCAA7uHj+Ge75xfMrtnjCsF0s37eG0UX3rfEZuKIcvjO7H9B+c6uWA+nQtSLkdv1NH9gHgqycM5pSRvbnVHZP/4uMG0bNzHqeP6utly5OPAeIBNmZAz05sLj3AqP7dGNCzU53lh/fp6iVwnfNDvPXzM7n51RU8NX8jc395ZsKysWADzvm6ZOxgRvXv5h3fpWMHJ1wLfbsVeOscM7gnK7fuoyA3hKY4DWOH9aJbUrZ8WO8u/OisQ/nhmSMREX4+4TAmPfA+FdXhhPPd3/2u+naLb/iFH57KuEMOYsfeKvJzc5hyxkhqItE69Q7nHzUASOzvMP+GswEY2NMp048lrvNvONurWDzr8H5cMtYpS37me+OpCUe54eXldMkPeUUwI/p25R9XjWNAz05ceOyghM/1333df8XYhJuhP33lGHp1yWNUf+dm5vrzDgfg+e+fwi9f+Igtew5winudJDvs4PgN0MG+36H/8794hPOdPHPNeK6btozXlyf21L71kmO8APDB1LMZ2LMTp985ly174nUDg9xrKT+U4wW1U0f2Yb7beOHi4wZ5Fba/u2gM3/6f4YgIIqmbPQ/q2bnuRNfQ3l047dC+rN9Vwch+3Xjs2yfxvScL+WT7fq/eb58vh5+fm8ONXz7Sqyt69OpxXt3Y+BG9eez9z/jBGSMT0qmWkEkAWAyMFpEROIn0ZOCKpGVmAFcD84HLgDmqqiIyA3hORO7GqQQeDSzCGaK/oW22m9hF2LtrfkaditLplmb00NjYQ+kS/3RiYxp1zQ/V6bwEeBV6Xz9xaJ15Ywb2oCocYdqUU/ln4WYuP3lYnWVijh7ck0uOH8Qry7bSt1t+2uViRvXvxppbJzoJpSpPL9jIxpJKRMRLTC8dO5glG8sYfFD8R/N/l5/A4x985lU2xjz+7ZP590dbObhH6uBz52XHMrJfV3aXV3Oum3P43UVH8fNzD0to0VXf8X1083l1AnSyGy44kgE9O/HFw/sxJ2kIgt9fNIYLjxtUZ50HrjgBSAzgr/7ktDqtv3q5FeX+AHDS8N4A/PL8w71pnfJCCeMcxe5ckyVP+8EZI/n7O+sZ0KMTeSGhNqL06pL4Xebn5vDprRcATmX9JQ+8zw0XHOEl4sl+fNYor+FBrKL/9kuP4TcvL+ei4walLKM+9dA+XHnKMP785hqOGtQj5XYbc1ebG8rh3snHc/YR/b3cy+zrz0hYZpD7O4httnfXPO9zXv3JaQmVxc9POYXZq3awaEMp1084jC8e0Y/R/btztK8RwpXjU+cuunWqP/m86cIj+cZJQ73f5cPfGscZd83lO6cN59bXV9cpJhrp/g7u+MoxCTnix79zEoUbylo88Qfivebq+wO+BHwKrANudKfdAlzsvu4EvAAU4STwI33r3uiutwa4oL5tNvR34oknalvZUlapu/ZXNXs7h/z6NT3k16/py0uLvWnb9x7QZZvKGlx32qKN+vT8Dd429h2oUVXVyuqwVlaHU66zv6pWo9Fos/e7qjasH2/e06R1w5Go1oQjCdOi0ai3/x1RTTiit7++Sr/35GI95Nev6aaSioT5T83foNMXbWpwO9MXb9LvPr4oYdrNryzXhetL0q5TUV3rXQNNsX3vAd1cWtHgcnsqGv5+Ssqr9f45azUSiV9jDV1vteGILvqs7vEd94dZTT4mVee3NXxqfP1H3l2vU//1sff+uueX6iG/fk137mve77g2HNE9FTW6etteXV68p1nfharqZ7vK9ZBfv6Y/eqawzrxIJNoiv99kQKGmSFMbrAT+PGmJVkBtbdytsznjsL7c/fXjm7wNf5l/U4ujTMuoDkdYuXUfJwxreoVcY0WjysjfxBspZIuKaqcXdia5tlSWbCyjf/eChGbEflW1EdZs389xQ3s1Yy/riv0em/Nd/GfVDsaP7J0wCkFr6rCVwB2dv0KyuSzxb38FuaE2TfwBr2d6tmlOs0agwVYxnfJCLZ74A8z86RcoLkvfmS8TE8Yc3PBCbcACgDHGNMKYQT0Yk6Y+o6OxW0pjjAkoCwAdwP939ihOdluIGGNMS7EioA7gF+cd3vBCJqvd/fXjGNAazQBNoFkAMKYD+MoJQ9p7F0wWsiIgY4wJKAsAxhgTUBYAjDEmoCwAGGNMQFkAMMaYgLIAYIwxAWUBwBhjAsoCgDHGBFSHGg5aRHYBTX0ocF9gd4NLBY+dl/Ts3KRm5yW1z/N5OURV+yVP7FABoDlEpDDVeNhBZ+clPTs3qdl5Sa0jnhcrAjLGmICyAGCMMQEVpADwcHvvwOeUnZf07NykZucltQ53XgJTB2CMMSZRkHIAxhhjfCwAGGNMQGV9ABCRiSKyRkSKRGRqe+9PWxORoSIyV0RWichKEbnOnd5bRP4jImvd/we500VE7nPP18cickL7HkHrEpGQiHwoIq+570eIyEL3+KeLSL47vcB9X+TOH96uO96KRKSXiLwoIp+IyGoROdWuF4eI/Nz9Ha0QkedFpFNHvmayOgCISAh4ALgAGANcLiJj2nev2lwY+IWqjgFOAX7inoOpwNuqOhp4230Pzrka7f5NAf7W9rvcpq4DVvve3wnco6qjgDLgGnf6NUCZO/0ed7lsdS/wpqoeARyHc34Cf72IyGDgp8A4VT0aCAGT6cjXjKpm7R9wKjDL9/4G4Ib23q92PievAhOANcBAd9pAYI37+u/A5b7lveWy7Q8YgpOYnQ28BghOT87c5OsHmAWc6r7OdZeT9j6GVjgnPYHPko/NrhcFGAxsBnq718BrwPkd+ZrJ6hwA8S8sptidFkhuFnQssBA4WFW3ubO2Awe7r4N0zv4K/AqIuu/7AHtUNey+9x+7d17c+Xvd5bPNCGAX8LhbNPaIiHTFrhdUdQvwF2ATsA3nGlhCB75msj0AGJeIdAP+BfxMVff556lzixKo9sAiciGwU1WXtPe+fM7kAicAf1PVsUAF8eIeIJjXC4Bb7zEJJ0gOAroCE9t1p5op2wPAFmCo7/0Qd1qgiEgeTuL/rKq+5E7eISID3fkDgZ3u9KCcs9OAi0VkAzANpxjoXqCXiOS6y/iP3Tsv7vyeQElb7nAbKQaKVXWh+/5FnIAQ9OsF4FzgM1Xdpaq1wEs411GHvWayPQAsBka7tfT5OBU2M9p5n9qUiAjwKLBaVe/2zZoBXO2+vhqnbiA2/Sq3dccpwF5f1j9rqOoNqjpEVYfjXBdzVPVKYC5wmbtY8nmJna/L3OWz7i5YVbcDm0XkcHfSOcAqAn69uDYBp4hIF/d3FTs3Hfeaae9KiDaouPkS8CmwDrixvfenHY7/dJzs+sfAMvfvSzhlkW8Da4HZQG93ecFpObUOWI7T4qHdj6OVz9FZwGvu65HAIqAIeAEocKd3ct8XufNHtvd+t+L5OB4odK+ZV4CD7Hrxzs0fgE+AFcDTQEFHvmZsKAhjjAmobC8CMsYYk4YFAGOMCSgLAMYYE1AWAIwxJqAsABhjTEBZADDGmICyAGCMMQH1/wADZScGtbZG8wAAAABJRU5ErkJggg==",
                        "text/plain": [
                            "<Figure size 432x288 with 1 Axes>"
                        ]
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": [
                "#plot the obtained permutation importance\n",
                "plt.plot(scores.importances_mean)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 85,
            "metadata": {},
            "outputs": [],
            "source": [
                "# get the mosr important features\n",
                "imp_colimns = np.argsort(scores.importances_mean)[-20:]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 86,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "Index(['MONTH_5', 'SCHED_ARR_HOUR_23', 'SCHED_DEP_HOUR_7',\n",
                            "       'ORIGIN_AIRPORT_ONT', 'SCHED_DEP_HOUR_16', 'DESTINATION_AIRPORT_LFT',\n",
                            "       'DAY_OF_WEEK_6', 'DAY_OF_WEEK_5', 'MONTH_10', 'DISTANCE',\n",
                            "       'SCHED_DEP_HOUR_6', 'SCHED_DEP_HOUR_21', 'SCHEDULED_TIME',\n",
                            "       'SCHED_DEP_HOUR_17', 'MONTH_8', 'SCHED_DEP_HOUR_5', 'MONTH_9',\n",
                            "       'SCHED_DEP_HOUR_20', 'SCHED_DEP_HOUR_18', 'SCHED_DEP_HOUR_19'],\n",
                            "      dtype='object')"
                        ]
                    },
                    "execution_count": 86,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df.columns[imp_colimns]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**INTERPRETATION:**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*Your answer here*"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class='exercise-r'>\n",
                "\n",
                "**1.4.2**\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 95,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "-4.758831255668108e-19"
                        ]
                    },
                    "execution_count": 95,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# your code here\n",
                "# Replace all values in all columns of the training set with the column mean\n",
                "X_train\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class='exercise-r'>\n",
                "\n",
                "**1.4.3**\n",
                "    \n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# your code here\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class='exercise-r'>\n",
                "\n",
                "**1.4.4**\n",
                "    \n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# your code here\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**INTERPRETATION:**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*your answer here*"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class='exercise-r'>\n",
                "\n",
                "**1.5**\n",
                "\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "def progressbar(n_step, n_total):\n",
                "    \"\"\"Prints self-updating progress bar to stdout to track for-loop progress\n",
                "    \n",
                "    There are entire 3rd-party libraries dedicated to custom progress-bars.\n",
                "    A simple function like this is often more than enough to get the job done.\n",
                "    \n",
                "    :param n_total: total number of expected for-loop iterations\n",
                "    :type n_total: int\n",
                "    :param n_step: current iteration number, starting at 0\n",
                "    :type n_step: int\n",
                "\n",
                "    .. example::\n",
                "    \n",
                "        for i in range(n_iterations):\n",
                "            progressbar(i, n_iterations)\n",
                "            \n",
                "    .. source:\n",
                "    \n",
                "        This function is a simplified version of code found here:\n",
                "        https://stackoverflow.com/questions/3160699/python-progress-bar/15860757#15860757\n",
                "    \"\"\"\n",
                "    n_step = n_step + 1\n",
                "    barlen = 50\n",
                "    progress = n_step / n_total\n",
                "    block = int(round(barlen * progress))\n",
                "    status = \"\"\n",
                "    if n_step == n_total:\n",
                "        status = \"Done...\\r\\n\\n\"\n",
                "    text = \"\\r [{0}] {1}/{2} {3}\".format(\n",
                "        \"=\" * block + \"-\" * (barlen - block),\n",
                "        n_step,\n",
                "        n_total,\n",
                "        status,\n",
                "    )\n",
                "    sys.stdout.write(text)\n",
                "    sys.stdout.flush()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CPU times: user 2 s, sys: 1e+03 ns, total: 3 s\n",
                        "Wall time: 5.01 s\n"
                    ]
                }
            ],
            "source": [
                "%%time\n",
                "# Bootstrap and train your networks and get predictions on fixed X test\n",
                "# your code here\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# generate your plot\n",
                "# your code here\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**INTERPRETATION:**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*your answer here*"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class='exercise-r'>\n",
                "\n",
                "**1.6**\n",
                "\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# your code here\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**INTERPRETATION:**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*your answer here*"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
                "\n",
                "<h1>PART 2.1 [30 pts]: Kannada MNIST Kaggle competition using ANNs </h1>\n",
                "\n",
                "[Return to contents](#contents)\n",
                "\n",
                "\n",
                "<a id=\"part2.1intro\"></a>\n",
                "<h2> Problem Statement </h2>\n",
                "\n",
                "ANNs can be prone to overfitting, where they learn specific patterns present in the training data, but the patterns do not generalize to new data.\n",
                "\n",
                "There are several methods used to improve ANN generalization. \n",
                "\n",
                "One approach is to use an architecture just barely wide or deep enough to fit the data. The idea here is that smaller networks are less expressive and thus less able to overfit the data.\n",
                "\n",
                "However, it is difficult to know a priori the correct size of the ANN, and it is computationally costly to hunt for the correct size. Given this, other methodologies are used to prevent overfitting and improve ANNs' generalizability. These methodologies, like other techniques that combat overfitting, fall under the umbrella term of \"regularization\".\n",
                "\n",
                "In this problem, you are asked to regularize a network of a given architecture.\n",
                "\n",
                "<a id=\"part2.1about\"></a>\n",
                "\n",
                "<h3> The Kannada MNIST Dataset </h3>\n",
                "\n",
                "<img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3630446%2F1e01bcc28b5ccb7ad38a4ffefb13cde0%2Fwondu.png?generation=1603204077179447&alt=media\" style=\"float:right\">\n",
                "\n",
                "For this problem, we will be working with a modified version of [Kannada MNIST dataset](https://arxiv.org/pdf/1908.01242.pdf) , which is a large database of handwritten digits in the indigenous language *Kannada*.\n",
                "\n",
                "This dataset consists of 60,000 28x28 grayscale images of the ten digits, along with a test set of 10,000 images. \n",
                "\n",
                "For this homework, we will simplify the problem by only use the digits labeled `0` and `1` owing to the similarity of the two symbols, and we want to use a total of 1200 samples for training (this includes the data you will use for validation).\n",
                "\n",
                "To understand the dataset better, we recommend this [article](https://towardsdatascience.com/a-new-handwritten-digits-dataset-in-ml-town-kannada-mnist-69df0f2d1456) by Vinay Prabhu, the curator of the dataset.\n",
                "\n",
                "<a id=\"part2data\"></a>\n",
                "\n",
                "<h3> Downloading the Data Files </h3>\n",
                "\n",
                "Please download all files from Kaggle. **The link will be given soon.**\n",
                "\n",
                "Here's a brief description of the available files:\n",
                "\n",
                "- `kmnist_train.csv` is our training dataset and the last column contains our response class. The 784 other columns correspond to the pixel values of the 28x28 dimension image.\n",
                "\n",
                "Class 0 means a sample is the handwritten digit `0` and class 1 means a sample is the handwritten digit `1` in the Kannada language.  \n",
                "\n",
                "- `kmnist_test.csv` has a structure similar to `kmnist_train.csv`, however the class label column is NOT included in with the test set. `kmnist_test.csv` has 2000 samples. \n",
                "\n",
                "Kaggle leaderboard scores are accuracy scores calculated by Kaggle when you upload your predictions on this test set.\n",
                "\n",
                "- `sample_submission.csv` is the format that kaggle will accept.\n",
                "\n",
                "<a id=\"part2.1kaggle\"></a>\n",
                "\n",
                "<h3> AI2-C2 Homework Kaggle Competition </h3>\n",
                "\n",
                "You need to create an account on Kaggle and join the competition. **The link is in the description.** \n",
                "\n",
                "**This is a limited participation competition. Please DO NOT share this link you will recieve.**\n",
                "\n",
                "For more information on the rules governing this Kaggle competition, **please [see question 2.1.3 below](#part2_3).**\n",
                "\n",
                "</div>"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
                "\n",
                "<h2>PART 2.1 Questions</h2>\n",
                "\n",
                "**2.1.1 [5 points]** **Get the data:**\n",
                "\n",
                "- Download data from the competition page.\n",
                "- We will utilize `kmnist_test.csv` in question 2.1.3.4 only. \n",
                "- Load the data and use the matplotlib function `imshow` to display a handwritten 0 and a handwritten 1.\n",
                "\n",
                "**2.1.2 [10 points]** **Overfit an ANN:** \n",
                "\n",
                "Build a fully-connected network (FCN) with the architecture given below using `tensorflow.keras` and assign it to a variable called `model_overfit`:\n",
                "\n",
                "- Number of hidden layers: 3\n",
                "- Nodes per hidden layer: 100, 100, 100\n",
                "- Activation function: ReLU \n",
                "- Loss function: binary_crossentropy\n",
                "- Output unit: Sigmoid \n",
                "- Optimizer: adam (use the defaults; no other tuning)\n",
                "- Epochs: no more than 2,000\n",
                "- Batch size: 128\n",
                "- Validation size: 0.3\n",
                "\n",
                "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3630446%2F6a491ff8d4ff590dc8ded9a25461cd4b%2FScreenshot%202020-10-20%20at%209.42.36%20PM.png?generation=1603210420701577&alt=media) \n",
                "    \n",
                "This ANN, when trained on the dataset, will overfit to the training set. Plot the training accuracy and validation accuracy (the x-axis should represent the number of epochs, and the y-axis should represent the accuracy). Explain how you can tell the model is overfitting. \n",
                "\n",
                "<a id=\"part2_3\"></a>\n",
                "\n",
                "**2.1.3 [15 points]** **Regularize overfit network:**\n",
                "\n",
                "Create an ANN that doesn't overfit and compete on Kaggle.\n",
                "\n",
                "**DON'TS**\n",
                "\n",
                "**Don't change the architecture**. In other words, keep the number of layers, number of nodes, activation function,  loss function and output unit the same. **No CNNs/RNNs/enhancements allowed for the competition.**\n",
                "\n",
                "    \n",
                "**NOTE**: We strongly discourage you to use a different training set than the one provided to you (Data augmentation is allowed). If the test set accuracy of your model in this notebook is significantly different than your kaggle submission score, you will receive zero credit for this segment of the homework.\n",
                "\n",
                "    \n",
                "**DOS**\n",
                "\n",
                "You can change the number of epochs (max 2000), batch size, optimizer, and of course, add elements that can help to regularize (e.g., dropout, L2 norm, etc.). You **should** experiment with data augmentation. \n",
                "\n",
                "\n",
                "- **2.1.3.1** Display your model summary and your training and validation accuracy and loss.\n",
                "\n",
                "\n",
                "- **2.1.3.2** Print the difference between the training and validation accuracies and the difference between the training and validation losses for the final trained epoch used by your model.\n",
                "\n",
                "\n",
                "- **2.1.3.3** Plot the training accuracy and validation accuracy as a function of epochs.\n",
                "\n",
                "\n",
                "- **2.1.3.4** Generate your test set class predictions using your regularized model. Save those predictions to a `.csv` formatted file. Submit that `.csv` file to this Kaggle Competition for leaderboard scoring.\n",
                "\n",
                "\n",
                "- **2.1.3.5** **Specify your Kaggle name that you have used on the leaderboard**. *We can't give you credit without this.*\n",
                "\n",
                "\n",
                "    \n",
                "**IMPORTANT NOTES ABOUT SCORING**:\n",
                "\n",
                "- The **public leaderboard** on kaggle only displays your performance on 50% of the test set.\n",
                "\n",
                "\n",
                "- After the competition is complete, the **private leaderboard** will show your performance on the FULL test set.\n",
                "    \n",
                "Only the **top 5** competitors (as ranked on the hidden private leaderboard) will be eligible for full credit on question 2.1.3 (out of **15 points**). Remaining competitors will be scored out of **10 points** only for 2.1.3.\n",
                "\n",
                "\n",
                "**ADDITIONAL RULES:**\n",
                "\n",
                "- Multiple Kaggle submissions are permitted, **just note** that you will need to choose, on Kaggle, which submission shall be used for final scoring.\n",
                "\n",
                "\n",
                "- The version of your final notebook submitted on edStem **must contain the same model** used to generate to your chosen Kaggle submission.\n",
                "\n",
                "\n",
                "- **Please do not manually label your submissions.** In other words, the labels should only be the outcome of your model.\n",
                "\n",
                "\n",
                "- **No external data are allowed, please only use the KMNIST training data downloaded via the link above for training your model.**\n",
                "\n",
                "\n",
                "- **Do not** create multiple accounts on Kaggle.\n",
                "\n",
                "</div>"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id=\"part2.1solutions\"></a>\n",
                "\n",
                "## PART 2.1 Solutions\n",
                "\n",
                "[Return to contents](#contents)\n",
                "\n",
                "\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class='exercise-r'>\n",
                "\n",
                "**2.1.1** \n",
                "    \n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 98,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# your code here \n",
                "#load the kmnist data\n",
                "df = pd.read_csv('kmnist_train.csv/kmnist_train.csv')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 118,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<matplotlib.image.AxesImage at 0x7fe658f372b0>"
                        ]
                    },
                    "execution_count": 118,
                    "metadata": {},
                    "output_type": "execute_result"
                },
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 432x288 with 1 Axes>"
                        ]
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": [
                "#imshow the images\n",
                "plt.imshow(df.iloc[6,:-1].values.reshape(28,28))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class='exercise-r'>\n",
                "\n",
                "**2.1.2**\n",
                "    \n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 120,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/500\n",
                        "7/7 [==============================] - 2s 72ms/step - loss: 9.1060 - accuracy: 0.5131 - val_loss: 2.1429 - val_accuracy: 0.5889\n",
                        "Epoch 2/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 1.9582 - accuracy: 0.6333 - val_loss: 1.6614 - val_accuracy: 0.6583\n",
                        "Epoch 3/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 1.0485 - accuracy: 0.7048 - val_loss: 0.9597 - val_accuracy: 0.7306\n",
                        "Epoch 4/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 0.6413 - accuracy: 0.7774 - val_loss: 0.7320 - val_accuracy: 0.7806\n",
                        "Epoch 5/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 0.5440 - accuracy: 0.8310 - val_loss: 0.6197 - val_accuracy: 0.7806\n",
                        "Epoch 6/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 0.3246 - accuracy: 0.8845 - val_loss: 0.5694 - val_accuracy: 0.8222\n",
                        "Epoch 7/500\n",
                        "7/7 [==============================] - 0s 25ms/step - loss: 0.2416 - accuracy: 0.9250 - val_loss: 0.6245 - val_accuracy: 0.8167\n",
                        "Epoch 8/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 0.2208 - accuracy: 0.9131 - val_loss: 0.4748 - val_accuracy: 0.8250\n",
                        "Epoch 9/500\n",
                        "7/7 [==============================] - 0s 36ms/step - loss: 0.2171 - accuracy: 0.9131 - val_loss: 0.4983 - val_accuracy: 0.8306\n",
                        "Epoch 10/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 0.1229 - accuracy: 0.9464 - val_loss: 0.4192 - val_accuracy: 0.8583\n",
                        "Epoch 11/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 0.1079 - accuracy: 0.9631 - val_loss: 0.4352 - val_accuracy: 0.8528\n",
                        "Epoch 12/500\n",
                        "7/7 [==============================] - 0s 37ms/step - loss: 0.0688 - accuracy: 0.9774 - val_loss: 0.4577 - val_accuracy: 0.8556\n",
                        "Epoch 13/500\n",
                        "7/7 [==============================] - 0s 43ms/step - loss: 0.0577 - accuracy: 0.9821 - val_loss: 0.4615 - val_accuracy: 0.8611\n",
                        "Epoch 14/500\n",
                        "7/7 [==============================] - 0s 36ms/step - loss: 0.0364 - accuracy: 0.9917 - val_loss: 0.4110 - val_accuracy: 0.8694\n",
                        "Epoch 15/500\n",
                        "7/7 [==============================] - 0s 48ms/step - loss: 0.0240 - accuracy: 0.9976 - val_loss: 0.4175 - val_accuracy: 0.8583\n",
                        "Epoch 16/500\n",
                        "7/7 [==============================] - 0s 45ms/step - loss: 0.0238 - accuracy: 0.9976 - val_loss: 0.4226 - val_accuracy: 0.8611\n",
                        "Epoch 17/500\n",
                        "7/7 [==============================] - 0s 44ms/step - loss: 0.0183 - accuracy: 0.9988 - val_loss: 0.4343 - val_accuracy: 0.8667\n",
                        "Epoch 18/500\n",
                        "7/7 [==============================] - 0s 45ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.4262 - val_accuracy: 0.8694\n",
                        "Epoch 19/500\n",
                        "7/7 [==============================] - 0s 41ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.4114 - val_accuracy: 0.8639\n",
                        "Epoch 20/500\n",
                        "7/7 [==============================] - 0s 36ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.4319 - val_accuracy: 0.8722\n",
                        "Epoch 21/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.4330 - val_accuracy: 0.8778\n",
                        "Epoch 22/500\n",
                        "7/7 [==============================] - 0s 37ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.4203 - val_accuracy: 0.8694\n",
                        "Epoch 23/500\n",
                        "7/7 [==============================] - 0s 38ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.4378 - val_accuracy: 0.8750\n",
                        "Epoch 24/500\n",
                        "7/7 [==============================] - 0s 49ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.4205 - val_accuracy: 0.8750\n",
                        "Epoch 25/500\n",
                        "7/7 [==============================] - 0s 37ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.4389 - val_accuracy: 0.8694\n",
                        "Epoch 26/500\n",
                        "7/7 [==============================] - 0s 39ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.4192 - val_accuracy: 0.8778\n",
                        "Epoch 27/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.4313 - val_accuracy: 0.8750\n",
                        "Epoch 28/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.4231 - val_accuracy: 0.8833\n",
                        "Epoch 29/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.4264 - val_accuracy: 0.8861\n",
                        "Epoch 30/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.4269 - val_accuracy: 0.8833\n",
                        "Epoch 31/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.4311 - val_accuracy: 0.8833\n",
                        "Epoch 32/500\n",
                        "7/7 [==============================] - 0s 26ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.4257 - val_accuracy: 0.8833\n",
                        "Epoch 33/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.4298 - val_accuracy: 0.8833\n",
                        "Epoch 34/500\n",
                        "7/7 [==============================] - 0s 31ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.4267 - val_accuracy: 0.8861\n",
                        "Epoch 35/500\n",
                        "7/7 [==============================] - 0s 21ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.4313 - val_accuracy: 0.8861\n",
                        "Epoch 36/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4278 - val_accuracy: 0.8861\n",
                        "Epoch 37/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4329 - val_accuracy: 0.8861\n",
                        "Epoch 38/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.4285 - val_accuracy: 0.8861\n",
                        "Epoch 39/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4302 - val_accuracy: 0.8861\n",
                        "Epoch 40/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4321 - val_accuracy: 0.8861\n",
                        "Epoch 41/500\n",
                        "7/7 [==============================] - 0s 40ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4335 - val_accuracy: 0.8861\n",
                        "Epoch 42/500\n",
                        "7/7 [==============================] - 0s 46ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4337 - val_accuracy: 0.8861\n",
                        "Epoch 43/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4370 - val_accuracy: 0.8861\n",
                        "Epoch 44/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4341 - val_accuracy: 0.8861\n",
                        "Epoch 45/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4358 - val_accuracy: 0.8861\n",
                        "Epoch 46/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4386 - val_accuracy: 0.8861\n",
                        "Epoch 47/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4374 - val_accuracy: 0.8861\n",
                        "Epoch 48/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4391 - val_accuracy: 0.8861\n",
                        "Epoch 49/500\n",
                        "7/7 [==============================] - 0s 31ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4345 - val_accuracy: 0.8861\n",
                        "Epoch 50/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4386 - val_accuracy: 0.8861\n",
                        "Epoch 51/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4347 - val_accuracy: 0.8861\n",
                        "Epoch 52/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4353 - val_accuracy: 0.8861\n",
                        "Epoch 53/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4483 - val_accuracy: 0.8806\n",
                        "Epoch 54/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4399 - val_accuracy: 0.8861\n",
                        "Epoch 55/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4496 - val_accuracy: 0.8806\n",
                        "Epoch 56/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4449 - val_accuracy: 0.8806\n",
                        "Epoch 57/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4474 - val_accuracy: 0.8806\n",
                        "Epoch 58/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4481 - val_accuracy: 0.8806\n",
                        "Epoch 59/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4492 - val_accuracy: 0.8806\n",
                        "Epoch 60/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 9.9783e-04 - accuracy: 1.0000 - val_loss: 0.4479 - val_accuracy: 0.8833\n",
                        "Epoch 61/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 9.7485e-04 - accuracy: 1.0000 - val_loss: 0.4501 - val_accuracy: 0.8833\n",
                        "Epoch 62/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 9.2721e-04 - accuracy: 1.0000 - val_loss: 0.4502 - val_accuracy: 0.8806\n",
                        "Epoch 63/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 9.0687e-04 - accuracy: 1.0000 - val_loss: 0.4559 - val_accuracy: 0.8806\n",
                        "Epoch 64/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 8.7514e-04 - accuracy: 1.0000 - val_loss: 0.4529 - val_accuracy: 0.8833\n",
                        "Epoch 65/500\n",
                        "7/7 [==============================] - 0s 31ms/step - loss: 8.5226e-04 - accuracy: 1.0000 - val_loss: 0.4577 - val_accuracy: 0.8806\n",
                        "Epoch 66/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 8.2500e-04 - accuracy: 1.0000 - val_loss: 0.4508 - val_accuracy: 0.8833\n",
                        "Epoch 67/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 8.2066e-04 - accuracy: 1.0000 - val_loss: 0.4536 - val_accuracy: 0.8833\n",
                        "Epoch 68/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 7.8125e-04 - accuracy: 1.0000 - val_loss: 0.4724 - val_accuracy: 0.8778\n",
                        "Epoch 69/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 7.7113e-04 - accuracy: 1.0000 - val_loss: 0.4555 - val_accuracy: 0.8861\n",
                        "Epoch 70/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 7.5067e-04 - accuracy: 1.0000 - val_loss: 0.4632 - val_accuracy: 0.8806\n",
                        "Epoch 71/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 7.0744e-04 - accuracy: 1.0000 - val_loss: 0.4783 - val_accuracy: 0.8778\n",
                        "Epoch 72/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 6.8034e-04 - accuracy: 1.0000 - val_loss: 0.4569 - val_accuracy: 0.8833\n",
                        "Epoch 73/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 6.9232e-04 - accuracy: 1.0000 - val_loss: 0.4606 - val_accuracy: 0.8861\n",
                        "Epoch 74/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 6.7281e-04 - accuracy: 1.0000 - val_loss: 0.4706 - val_accuracy: 0.8806\n",
                        "Epoch 75/500\n",
                        "7/7 [==============================] - 0s 31ms/step - loss: 6.1783e-04 - accuracy: 1.0000 - val_loss: 0.4768 - val_accuracy: 0.8778\n",
                        "Epoch 76/500\n",
                        "7/7 [==============================] - 0s 51ms/step - loss: 6.0424e-04 - accuracy: 1.0000 - val_loss: 0.4906 - val_accuracy: 0.8722\n",
                        "Epoch 77/500\n",
                        "7/7 [==============================] - 0s 40ms/step - loss: 5.7251e-04 - accuracy: 1.0000 - val_loss: 0.4867 - val_accuracy: 0.8778\n",
                        "Epoch 78/500\n",
                        "7/7 [==============================] - 0s 44ms/step - loss: 5.5037e-04 - accuracy: 1.0000 - val_loss: 0.4850 - val_accuracy: 0.8778\n",
                        "Epoch 79/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 5.3228e-04 - accuracy: 1.0000 - val_loss: 0.4947 - val_accuracy: 0.8750\n",
                        "Epoch 80/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 5.1031e-04 - accuracy: 1.0000 - val_loss: 0.5038 - val_accuracy: 0.8750\n",
                        "Epoch 81/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 4.9474e-04 - accuracy: 1.0000 - val_loss: 0.4889 - val_accuracy: 0.8778\n",
                        "Epoch 82/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 4.8732e-04 - accuracy: 1.0000 - val_loss: 0.5028 - val_accuracy: 0.8750\n",
                        "Epoch 83/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 4.6849e-04 - accuracy: 1.0000 - val_loss: 0.4944 - val_accuracy: 0.8750\n",
                        "Epoch 84/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 4.4809e-04 - accuracy: 1.0000 - val_loss: 0.5118 - val_accuracy: 0.8750\n",
                        "Epoch 85/500\n",
                        "7/7 [==============================] - 0s 31ms/step - loss: 4.3235e-04 - accuracy: 1.0000 - val_loss: 0.4955 - val_accuracy: 0.8750\n",
                        "Epoch 86/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 4.2337e-04 - accuracy: 1.0000 - val_loss: 0.5188 - val_accuracy: 0.8750\n",
                        "Epoch 87/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 4.0662e-04 - accuracy: 1.0000 - val_loss: 0.5142 - val_accuracy: 0.8750\n",
                        "Epoch 88/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 3.9732e-04 - accuracy: 1.0000 - val_loss: 0.5019 - val_accuracy: 0.8778\n",
                        "Epoch 89/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 3.9006e-04 - accuracy: 1.0000 - val_loss: 0.5152 - val_accuracy: 0.8750\n",
                        "Epoch 90/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 3.7505e-04 - accuracy: 1.0000 - val_loss: 0.5176 - val_accuracy: 0.8778\n",
                        "Epoch 91/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 3.6588e-04 - accuracy: 1.0000 - val_loss: 0.5226 - val_accuracy: 0.8806\n",
                        "Epoch 92/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 3.5780e-04 - accuracy: 1.0000 - val_loss: 0.5235 - val_accuracy: 0.8778\n",
                        "Epoch 93/500\n",
                        "7/7 [==============================] - 0s 44ms/step - loss: 3.4756e-04 - accuracy: 1.0000 - val_loss: 0.5235 - val_accuracy: 0.8778\n",
                        "Epoch 94/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 3.3843e-04 - accuracy: 1.0000 - val_loss: 0.5204 - val_accuracy: 0.8806\n",
                        "Epoch 95/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 3.3352e-04 - accuracy: 1.0000 - val_loss: 0.5273 - val_accuracy: 0.8806\n",
                        "Epoch 96/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 3.2510e-04 - accuracy: 1.0000 - val_loss: 0.5292 - val_accuracy: 0.8806\n",
                        "Epoch 97/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 3.1836e-04 - accuracy: 1.0000 - val_loss: 0.5201 - val_accuracy: 0.8806\n",
                        "Epoch 98/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 3.1235e-04 - accuracy: 1.0000 - val_loss: 0.5377 - val_accuracy: 0.8833\n",
                        "Epoch 99/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 3.0411e-04 - accuracy: 1.0000 - val_loss: 0.5190 - val_accuracy: 0.8806\n",
                        "Epoch 100/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 3.0485e-04 - accuracy: 1.0000 - val_loss: 0.5284 - val_accuracy: 0.8806\n",
                        "Epoch 101/500\n",
                        "7/7 [==============================] - 0s 26ms/step - loss: 2.9261e-04 - accuracy: 1.0000 - val_loss: 0.5312 - val_accuracy: 0.8861\n",
                        "Epoch 102/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 2.8584e-04 - accuracy: 1.0000 - val_loss: 0.5469 - val_accuracy: 0.8861\n",
                        "Epoch 103/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 2.7639e-04 - accuracy: 1.0000 - val_loss: 0.5422 - val_accuracy: 0.8833\n",
                        "Epoch 104/500\n",
                        "7/7 [==============================] - 0s 40ms/step - loss: 2.7066e-04 - accuracy: 1.0000 - val_loss: 0.5527 - val_accuracy: 0.8861\n",
                        "Epoch 105/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 2.6417e-04 - accuracy: 1.0000 - val_loss: 0.5421 - val_accuracy: 0.8833\n",
                        "Epoch 106/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 2.6033e-04 - accuracy: 1.0000 - val_loss: 0.5438 - val_accuracy: 0.8833\n",
                        "Epoch 107/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 2.5462e-04 - accuracy: 1.0000 - val_loss: 0.5505 - val_accuracy: 0.8861\n",
                        "Epoch 108/500\n",
                        "7/7 [==============================] - 0s 31ms/step - loss: 2.5034e-04 - accuracy: 1.0000 - val_loss: 0.5439 - val_accuracy: 0.8833\n",
                        "Epoch 109/500\n",
                        "7/7 [==============================] - 0s 31ms/step - loss: 2.4415e-04 - accuracy: 1.0000 - val_loss: 0.5575 - val_accuracy: 0.8861\n",
                        "Epoch 110/500\n",
                        "7/7 [==============================] - 0s 48ms/step - loss: 2.3964e-04 - accuracy: 1.0000 - val_loss: 0.5557 - val_accuracy: 0.8889\n",
                        "Epoch 111/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 2.3313e-04 - accuracy: 1.0000 - val_loss: 0.5491 - val_accuracy: 0.8833\n",
                        "Epoch 112/500\n",
                        "7/7 [==============================] - 0s 37ms/step - loss: 2.3388e-04 - accuracy: 1.0000 - val_loss: 0.5376 - val_accuracy: 0.8806\n",
                        "Epoch 113/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 2.2739e-04 - accuracy: 1.0000 - val_loss: 0.5732 - val_accuracy: 0.8861\n",
                        "Epoch 114/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 2.2296e-04 - accuracy: 1.0000 - val_loss: 0.5307 - val_accuracy: 0.8806\n",
                        "Epoch 115/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 2.2224e-04 - accuracy: 1.0000 - val_loss: 0.5711 - val_accuracy: 0.8861\n",
                        "Epoch 116/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 2.1247e-04 - accuracy: 1.0000 - val_loss: 0.5600 - val_accuracy: 0.8833\n",
                        "Epoch 117/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 2.0712e-04 - accuracy: 1.0000 - val_loss: 0.5703 - val_accuracy: 0.8861\n",
                        "Epoch 118/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 2.0273e-04 - accuracy: 1.0000 - val_loss: 0.5665 - val_accuracy: 0.8833\n",
                        "Epoch 119/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 1.9889e-04 - accuracy: 1.0000 - val_loss: 0.5486 - val_accuracy: 0.8833\n",
                        "Epoch 120/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 1.9724e-04 - accuracy: 1.0000 - val_loss: 0.5720 - val_accuracy: 0.8833\n",
                        "Epoch 121/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 1.9392e-04 - accuracy: 1.0000 - val_loss: 0.5351 - val_accuracy: 0.8833\n",
                        "Epoch 122/500\n",
                        "7/7 [==============================] - 0s 41ms/step - loss: 1.9919e-04 - accuracy: 1.0000 - val_loss: 0.5483 - val_accuracy: 0.8806\n",
                        "Epoch 123/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 1.8700e-04 - accuracy: 1.0000 - val_loss: 0.5610 - val_accuracy: 0.8833\n",
                        "Epoch 124/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 1.8690e-04 - accuracy: 1.0000 - val_loss: 0.5444 - val_accuracy: 0.8806\n",
                        "Epoch 125/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 1.7990e-04 - accuracy: 1.0000 - val_loss: 0.5972 - val_accuracy: 0.8806\n",
                        "Epoch 126/500\n",
                        "7/7 [==============================] - 0s 26ms/step - loss: 1.7726e-04 - accuracy: 1.0000 - val_loss: 0.5436 - val_accuracy: 0.8806\n",
                        "Epoch 127/500\n",
                        "7/7 [==============================] - 0s 26ms/step - loss: 1.7612e-04 - accuracy: 1.0000 - val_loss: 0.5712 - val_accuracy: 0.8833\n",
                        "Epoch 128/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 1.7213e-04 - accuracy: 1.0000 - val_loss: 0.5571 - val_accuracy: 0.8833\n",
                        "Epoch 129/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 1.7182e-04 - accuracy: 1.0000 - val_loss: 0.5477 - val_accuracy: 0.8806\n",
                        "Epoch 130/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 1.6410e-04 - accuracy: 1.0000 - val_loss: 0.5887 - val_accuracy: 0.8778\n",
                        "Epoch 131/500\n",
                        "7/7 [==============================] - 0s 23ms/step - loss: 1.6075e-04 - accuracy: 1.0000 - val_loss: 0.5652 - val_accuracy: 0.8833\n",
                        "Epoch 132/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 1.5721e-04 - accuracy: 1.0000 - val_loss: 0.5825 - val_accuracy: 0.8806\n",
                        "Epoch 133/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 1.5474e-04 - accuracy: 1.0000 - val_loss: 0.5707 - val_accuracy: 0.8833\n",
                        "Epoch 134/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 1.5340e-04 - accuracy: 1.0000 - val_loss: 0.5652 - val_accuracy: 0.8833\n",
                        "Epoch 135/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 1.5218e-04 - accuracy: 1.0000 - val_loss: 0.5623 - val_accuracy: 0.8833\n",
                        "Epoch 136/500\n",
                        "7/7 [==============================] - 0s 31ms/step - loss: 1.4793e-04 - accuracy: 1.0000 - val_loss: 0.5742 - val_accuracy: 0.8833\n",
                        "Epoch 137/500\n",
                        "7/7 [==============================] - 0s 37ms/step - loss: 1.4564e-04 - accuracy: 1.0000 - val_loss: 0.5633 - val_accuracy: 0.8833\n",
                        "Epoch 138/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 1.4256e-04 - accuracy: 1.0000 - val_loss: 0.5855 - val_accuracy: 0.8806\n",
                        "Epoch 139/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 1.4009e-04 - accuracy: 1.0000 - val_loss: 0.5738 - val_accuracy: 0.8833\n",
                        "Epoch 140/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 1.3785e-04 - accuracy: 1.0000 - val_loss: 0.5882 - val_accuracy: 0.8806\n",
                        "Epoch 141/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 1.3590e-04 - accuracy: 1.0000 - val_loss: 0.5769 - val_accuracy: 0.8833\n",
                        "Epoch 142/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 1.3383e-04 - accuracy: 1.0000 - val_loss: 0.5929 - val_accuracy: 0.8806\n",
                        "Epoch 143/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 1.3155e-04 - accuracy: 1.0000 - val_loss: 0.5748 - val_accuracy: 0.8833\n",
                        "Epoch 144/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 1.2974e-04 - accuracy: 1.0000 - val_loss: 0.5852 - val_accuracy: 0.8806\n",
                        "Epoch 145/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 1.2735e-04 - accuracy: 1.0000 - val_loss: 0.5859 - val_accuracy: 0.8806\n",
                        "Epoch 146/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 1.2543e-04 - accuracy: 1.0000 - val_loss: 0.5835 - val_accuracy: 0.8806\n",
                        "Epoch 147/500\n",
                        "7/7 [==============================] - 0s 26ms/step - loss: 1.2384e-04 - accuracy: 1.0000 - val_loss: 0.5823 - val_accuracy: 0.8806\n",
                        "Epoch 148/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 1.2232e-04 - accuracy: 1.0000 - val_loss: 0.5893 - val_accuracy: 0.8806\n",
                        "Epoch 149/500\n",
                        "7/7 [==============================] - 0s 46ms/step - loss: 1.2043e-04 - accuracy: 1.0000 - val_loss: 0.5811 - val_accuracy: 0.8806\n",
                        "Epoch 150/500\n",
                        "7/7 [==============================] - 0s 52ms/step - loss: 1.1924e-04 - accuracy: 1.0000 - val_loss: 0.5842 - val_accuracy: 0.8806\n",
                        "Epoch 151/500\n",
                        "7/7 [==============================] - 0s 55ms/step - loss: 1.1750e-04 - accuracy: 1.0000 - val_loss: 0.5884 - val_accuracy: 0.8806\n",
                        "Epoch 152/500\n",
                        "7/7 [==============================] - 0s 55ms/step - loss: 1.1578e-04 - accuracy: 1.0000 - val_loss: 0.5865 - val_accuracy: 0.8806\n",
                        "Epoch 153/500\n",
                        "7/7 [==============================] - 0s 46ms/step - loss: 1.1462e-04 - accuracy: 1.0000 - val_loss: 0.5880 - val_accuracy: 0.8806\n",
                        "Epoch 154/500\n",
                        "7/7 [==============================] - 0s 39ms/step - loss: 1.1253e-04 - accuracy: 1.0000 - val_loss: 0.5891 - val_accuracy: 0.8806\n",
                        "Epoch 155/500\n",
                        "7/7 [==============================] - 0s 31ms/step - loss: 1.1148e-04 - accuracy: 1.0000 - val_loss: 0.5920 - val_accuracy: 0.8806\n",
                        "Epoch 156/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 1.0983e-04 - accuracy: 1.0000 - val_loss: 0.5890 - val_accuracy: 0.8806\n",
                        "Epoch 157/500\n",
                        "7/7 [==============================] - 0s 26ms/step - loss: 1.0866e-04 - accuracy: 1.0000 - val_loss: 0.5906 - val_accuracy: 0.8806\n",
                        "Epoch 158/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 1.0713e-04 - accuracy: 1.0000 - val_loss: 0.5880 - val_accuracy: 0.8806\n",
                        "Epoch 159/500\n",
                        "7/7 [==============================] - 0s 37ms/step - loss: 1.0574e-04 - accuracy: 1.0000 - val_loss: 0.5921 - val_accuracy: 0.8806\n",
                        "Epoch 160/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 1.0444e-04 - accuracy: 1.0000 - val_loss: 0.5921 - val_accuracy: 0.8806\n",
                        "Epoch 161/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 1.0327e-04 - accuracy: 1.0000 - val_loss: 0.5840 - val_accuracy: 0.8806\n",
                        "Epoch 162/500\n",
                        "7/7 [==============================] - 0s 26ms/step - loss: 1.0226e-04 - accuracy: 1.0000 - val_loss: 0.5948 - val_accuracy: 0.8806\n",
                        "Epoch 163/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 1.0085e-04 - accuracy: 1.0000 - val_loss: 0.5855 - val_accuracy: 0.8806\n",
                        "Epoch 164/500\n",
                        "7/7 [==============================] - 0s 42ms/step - loss: 9.9489e-05 - accuracy: 1.0000 - val_loss: 0.5936 - val_accuracy: 0.8806\n",
                        "Epoch 165/500\n",
                        "7/7 [==============================] - 0s 26ms/step - loss: 9.8344e-05 - accuracy: 1.0000 - val_loss: 0.5887 - val_accuracy: 0.8806\n",
                        "Epoch 166/500\n",
                        "7/7 [==============================] - 0s 36ms/step - loss: 9.7661e-05 - accuracy: 1.0000 - val_loss: 0.5834 - val_accuracy: 0.8861\n",
                        "Epoch 167/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 9.6330e-05 - accuracy: 1.0000 - val_loss: 0.5888 - val_accuracy: 0.8806\n",
                        "Epoch 168/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 9.6030e-05 - accuracy: 1.0000 - val_loss: 0.5631 - val_accuracy: 0.8861\n",
                        "Epoch 169/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 9.4909e-05 - accuracy: 1.0000 - val_loss: 0.5896 - val_accuracy: 0.8806\n",
                        "Epoch 170/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 9.2768e-05 - accuracy: 1.0000 - val_loss: 0.5957 - val_accuracy: 0.8806\n",
                        "Epoch 171/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 9.1536e-05 - accuracy: 1.0000 - val_loss: 0.5913 - val_accuracy: 0.8806\n",
                        "Epoch 172/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 9.0325e-05 - accuracy: 1.0000 - val_loss: 0.5975 - val_accuracy: 0.8806\n",
                        "Epoch 173/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 8.9244e-05 - accuracy: 1.0000 - val_loss: 0.5942 - val_accuracy: 0.8806\n",
                        "Epoch 174/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 8.8845e-05 - accuracy: 1.0000 - val_loss: 0.5987 - val_accuracy: 0.8806\n",
                        "Epoch 175/500\n",
                        "7/7 [==============================] - 0s 25ms/step - loss: 8.8238e-05 - accuracy: 1.0000 - val_loss: 0.5948 - val_accuracy: 0.8806\n",
                        "Epoch 176/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 8.6423e-05 - accuracy: 1.0000 - val_loss: 0.5893 - val_accuracy: 0.8833\n",
                        "Epoch 177/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 8.5459e-05 - accuracy: 1.0000 - val_loss: 0.5876 - val_accuracy: 0.8861\n",
                        "Epoch 178/500\n",
                        "7/7 [==============================] - 0s 49ms/step - loss: 8.4508e-05 - accuracy: 1.0000 - val_loss: 0.6003 - val_accuracy: 0.8806\n",
                        "Epoch 179/500\n",
                        "7/7 [==============================] - 0s 42ms/step - loss: 8.3363e-05 - accuracy: 1.0000 - val_loss: 0.5957 - val_accuracy: 0.8806\n",
                        "Epoch 180/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 8.2653e-05 - accuracy: 1.0000 - val_loss: 0.5969 - val_accuracy: 0.8806\n",
                        "Epoch 181/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 8.1494e-05 - accuracy: 1.0000 - val_loss: 0.5981 - val_accuracy: 0.8806\n",
                        "Epoch 182/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 8.0485e-05 - accuracy: 1.0000 - val_loss: 0.5968 - val_accuracy: 0.8806\n",
                        "Epoch 183/500\n",
                        "7/7 [==============================] - 0s 36ms/step - loss: 7.9647e-05 - accuracy: 1.0000 - val_loss: 0.5989 - val_accuracy: 0.8806\n",
                        "Epoch 184/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 7.8767e-05 - accuracy: 1.0000 - val_loss: 0.5986 - val_accuracy: 0.8806\n",
                        "Epoch 185/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 7.7705e-05 - accuracy: 1.0000 - val_loss: 0.6023 - val_accuracy: 0.8806\n",
                        "Epoch 186/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 7.6685e-05 - accuracy: 1.0000 - val_loss: 0.6013 - val_accuracy: 0.8806\n",
                        "Epoch 187/500\n",
                        "7/7 [==============================] - 0s 26ms/step - loss: 7.6250e-05 - accuracy: 1.0000 - val_loss: 0.5958 - val_accuracy: 0.8833\n",
                        "Epoch 188/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 7.5592e-05 - accuracy: 1.0000 - val_loss: 0.5854 - val_accuracy: 0.8861\n",
                        "Epoch 189/500\n",
                        "7/7 [==============================] - 0s 31ms/step - loss: 7.4757e-05 - accuracy: 1.0000 - val_loss: 0.5980 - val_accuracy: 0.8806\n",
                        "Epoch 190/500\n",
                        "7/7 [==============================] - 0s 26ms/step - loss: 7.3681e-05 - accuracy: 1.0000 - val_loss: 0.5981 - val_accuracy: 0.8806\n",
                        "Epoch 191/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 7.3073e-05 - accuracy: 1.0000 - val_loss: 0.5852 - val_accuracy: 0.8861\n",
                        "Epoch 192/500\n",
                        "7/7 [==============================] - 0s 37ms/step - loss: 7.2468e-05 - accuracy: 1.0000 - val_loss: 0.5981 - val_accuracy: 0.8833\n",
                        "Epoch 193/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 7.1287e-05 - accuracy: 1.0000 - val_loss: 0.5959 - val_accuracy: 0.8833\n",
                        "Epoch 194/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 7.0737e-05 - accuracy: 1.0000 - val_loss: 0.5879 - val_accuracy: 0.8861\n",
                        "Epoch 195/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 6.9618e-05 - accuracy: 1.0000 - val_loss: 0.6037 - val_accuracy: 0.8806\n",
                        "Epoch 196/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 6.8545e-05 - accuracy: 1.0000 - val_loss: 0.6045 - val_accuracy: 0.8806\n",
                        "Epoch 197/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 6.7718e-05 - accuracy: 1.0000 - val_loss: 0.6037 - val_accuracy: 0.8806\n",
                        "Epoch 198/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 6.6878e-05 - accuracy: 1.0000 - val_loss: 0.6072 - val_accuracy: 0.8806\n",
                        "Epoch 199/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 6.5996e-05 - accuracy: 1.0000 - val_loss: 0.6041 - val_accuracy: 0.8806\n",
                        "Epoch 200/500\n",
                        "7/7 [==============================] - 0s 42ms/step - loss: 6.5196e-05 - accuracy: 1.0000 - val_loss: 0.6044 - val_accuracy: 0.8806\n",
                        "Epoch 201/500\n",
                        "7/7 [==============================] - 0s 38ms/step - loss: 6.4439e-05 - accuracy: 1.0000 - val_loss: 0.6027 - val_accuracy: 0.8806\n",
                        "Epoch 202/500\n",
                        "7/7 [==============================] - 0s 39ms/step - loss: 6.3906e-05 - accuracy: 1.0000 - val_loss: 0.6002 - val_accuracy: 0.8833\n",
                        "Epoch 203/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 6.2911e-05 - accuracy: 1.0000 - val_loss: 0.6074 - val_accuracy: 0.8806\n",
                        "Epoch 204/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 6.2198e-05 - accuracy: 1.0000 - val_loss: 0.6043 - val_accuracy: 0.8806\n",
                        "Epoch 205/500\n",
                        "7/7 [==============================] - 0s 46ms/step - loss: 6.1444e-05 - accuracy: 1.0000 - val_loss: 0.6079 - val_accuracy: 0.8806\n",
                        "Epoch 206/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 6.0795e-05 - accuracy: 1.0000 - val_loss: 0.6069 - val_accuracy: 0.8806\n",
                        "Epoch 207/500\n",
                        "7/7 [==============================] - 0s 37ms/step - loss: 6.0112e-05 - accuracy: 1.0000 - val_loss: 0.6082 - val_accuracy: 0.8806\n",
                        "Epoch 208/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 5.9449e-05 - accuracy: 1.0000 - val_loss: 0.6073 - val_accuracy: 0.8806\n",
                        "Epoch 209/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 5.8763e-05 - accuracy: 1.0000 - val_loss: 0.6097 - val_accuracy: 0.8806\n",
                        "Epoch 210/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 5.8145e-05 - accuracy: 1.0000 - val_loss: 0.5987 - val_accuracy: 0.8833\n",
                        "Epoch 211/500\n",
                        "7/7 [==============================] - 0s 41ms/step - loss: 5.7734e-05 - accuracy: 1.0000 - val_loss: 0.6030 - val_accuracy: 0.8833\n",
                        "Epoch 212/500\n",
                        "7/7 [==============================] - 0s 37ms/step - loss: 5.6924e-05 - accuracy: 1.0000 - val_loss: 0.6132 - val_accuracy: 0.8806\n",
                        "Epoch 213/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 5.6430e-05 - accuracy: 1.0000 - val_loss: 0.6090 - val_accuracy: 0.8806\n",
                        "Epoch 214/500\n",
                        "7/7 [==============================] - 0s 36ms/step - loss: 5.5774e-05 - accuracy: 1.0000 - val_loss: 0.6122 - val_accuracy: 0.8806\n",
                        "Epoch 215/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 5.5276e-05 - accuracy: 1.0000 - val_loss: 0.6068 - val_accuracy: 0.8833\n",
                        "Epoch 216/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 5.4726e-05 - accuracy: 1.0000 - val_loss: 0.6067 - val_accuracy: 0.8833\n",
                        "Epoch 217/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 5.4170e-05 - accuracy: 1.0000 - val_loss: 0.6089 - val_accuracy: 0.8833\n",
                        "Epoch 218/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 5.3484e-05 - accuracy: 1.0000 - val_loss: 0.6123 - val_accuracy: 0.8833\n",
                        "Epoch 219/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 5.3093e-05 - accuracy: 1.0000 - val_loss: 0.6007 - val_accuracy: 0.8861\n",
                        "Epoch 220/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 5.3172e-05 - accuracy: 1.0000 - val_loss: 0.5854 - val_accuracy: 0.8917\n",
                        "Epoch 221/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 5.2716e-05 - accuracy: 1.0000 - val_loss: 0.6040 - val_accuracy: 0.8861\n",
                        "Epoch 222/500\n",
                        "7/7 [==============================] - 0s 36ms/step - loss: 5.1564e-05 - accuracy: 1.0000 - val_loss: 0.6128 - val_accuracy: 0.8833\n",
                        "Epoch 223/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 5.1210e-05 - accuracy: 1.0000 - val_loss: 0.5930 - val_accuracy: 0.8917\n",
                        "Epoch 224/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 5.0817e-05 - accuracy: 1.0000 - val_loss: 0.6036 - val_accuracy: 0.8861\n",
                        "Epoch 225/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 4.9951e-05 - accuracy: 1.0000 - val_loss: 0.6210 - val_accuracy: 0.8833\n",
                        "Epoch 226/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 4.9637e-05 - accuracy: 1.0000 - val_loss: 0.6044 - val_accuracy: 0.8861\n",
                        "Epoch 227/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 4.9220e-05 - accuracy: 1.0000 - val_loss: 0.6055 - val_accuracy: 0.8861\n",
                        "Epoch 228/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 4.8750e-05 - accuracy: 1.0000 - val_loss: 0.6136 - val_accuracy: 0.8833\n",
                        "Epoch 229/500\n",
                        "7/7 [==============================] - 0s 43ms/step - loss: 4.8399e-05 - accuracy: 1.0000 - val_loss: 0.6176 - val_accuracy: 0.8833\n",
                        "Epoch 230/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 4.7664e-05 - accuracy: 1.0000 - val_loss: 0.6125 - val_accuracy: 0.8861\n",
                        "Epoch 231/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 4.7230e-05 - accuracy: 1.0000 - val_loss: 0.6191 - val_accuracy: 0.8833\n",
                        "Epoch 232/500\n",
                        "7/7 [==============================] - 0s 31ms/step - loss: 4.6500e-05 - accuracy: 1.0000 - val_loss: 0.6131 - val_accuracy: 0.8861\n",
                        "Epoch 233/500\n",
                        "7/7 [==============================] - 0s 39ms/step - loss: 4.6430e-05 - accuracy: 1.0000 - val_loss: 0.6057 - val_accuracy: 0.8861\n",
                        "Epoch 234/500\n",
                        "7/7 [==============================] - 0s 39ms/step - loss: 4.5837e-05 - accuracy: 1.0000 - val_loss: 0.6179 - val_accuracy: 0.8833\n",
                        "Epoch 235/500\n",
                        "7/7 [==============================] - 0s 40ms/step - loss: 4.5318e-05 - accuracy: 1.0000 - val_loss: 0.6188 - val_accuracy: 0.8833\n",
                        "Epoch 236/500\n",
                        "7/7 [==============================] - 0s 43ms/step - loss: 4.4784e-05 - accuracy: 1.0000 - val_loss: 0.6173 - val_accuracy: 0.8833\n",
                        "Epoch 237/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 4.4357e-05 - accuracy: 1.0000 - val_loss: 0.6199 - val_accuracy: 0.8833\n",
                        "Epoch 238/500\n",
                        "7/7 [==============================] - 0s 42ms/step - loss: 4.3893e-05 - accuracy: 1.0000 - val_loss: 0.6188 - val_accuracy: 0.8833\n",
                        "Epoch 239/500\n",
                        "7/7 [==============================] - 0s 43ms/step - loss: 4.3518e-05 - accuracy: 1.0000 - val_loss: 0.6201 - val_accuracy: 0.8833\n",
                        "Epoch 240/500\n",
                        "7/7 [==============================] - 0s 50ms/step - loss: 4.3193e-05 - accuracy: 1.0000 - val_loss: 0.6207 - val_accuracy: 0.8833\n",
                        "Epoch 241/500\n",
                        "7/7 [==============================] - 0s 40ms/step - loss: 4.3027e-05 - accuracy: 1.0000 - val_loss: 0.6198 - val_accuracy: 0.8833\n",
                        "Epoch 242/500\n",
                        "7/7 [==============================] - 0s 46ms/step - loss: 4.2488e-05 - accuracy: 1.0000 - val_loss: 0.6192 - val_accuracy: 0.8833\n",
                        "Epoch 243/500\n",
                        "7/7 [==============================] - 0s 42ms/step - loss: 4.1955e-05 - accuracy: 1.0000 - val_loss: 0.6153 - val_accuracy: 0.8861\n",
                        "Epoch 244/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 4.1530e-05 - accuracy: 1.0000 - val_loss: 0.6209 - val_accuracy: 0.8833\n",
                        "Epoch 245/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 4.1334e-05 - accuracy: 1.0000 - val_loss: 0.6109 - val_accuracy: 0.8861\n",
                        "Epoch 246/500\n",
                        "7/7 [==============================] - 0s 41ms/step - loss: 4.0989e-05 - accuracy: 1.0000 - val_loss: 0.6071 - val_accuracy: 0.8889\n",
                        "Epoch 247/500\n",
                        "7/7 [==============================] - 0s 46ms/step - loss: 4.0573e-05 - accuracy: 1.0000 - val_loss: 0.6216 - val_accuracy: 0.8861\n",
                        "Epoch 248/500\n",
                        "7/7 [==============================] - 0s 56ms/step - loss: 3.9977e-05 - accuracy: 1.0000 - val_loss: 0.6156 - val_accuracy: 0.8861\n",
                        "Epoch 249/500\n",
                        "7/7 [==============================] - 0s 60ms/step - loss: 3.9816e-05 - accuracy: 1.0000 - val_loss: 0.6074 - val_accuracy: 0.8889\n",
                        "Epoch 250/500\n",
                        "7/7 [==============================] - 0s 53ms/step - loss: 3.9389e-05 - accuracy: 1.0000 - val_loss: 0.6160 - val_accuracy: 0.8861\n",
                        "Epoch 251/500\n",
                        "7/7 [==============================] - 0s 38ms/step - loss: 3.8840e-05 - accuracy: 1.0000 - val_loss: 0.6265 - val_accuracy: 0.8833\n",
                        "Epoch 252/500\n",
                        "7/7 [==============================] - 0s 36ms/step - loss: 3.8582e-05 - accuracy: 1.0000 - val_loss: 0.6126 - val_accuracy: 0.8861\n",
                        "Epoch 253/500\n",
                        "7/7 [==============================] - 0s 36ms/step - loss: 3.8097e-05 - accuracy: 1.0000 - val_loss: 0.6269 - val_accuracy: 0.8833\n",
                        "Epoch 254/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 3.7751e-05 - accuracy: 1.0000 - val_loss: 0.6107 - val_accuracy: 0.8889\n",
                        "Epoch 255/500\n",
                        "7/7 [==============================] - 0s 21ms/step - loss: 3.7792e-05 - accuracy: 1.0000 - val_loss: 0.6033 - val_accuracy: 0.8889\n",
                        "Epoch 256/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 3.7326e-05 - accuracy: 1.0000 - val_loss: 0.6204 - val_accuracy: 0.8861\n",
                        "Epoch 257/500\n",
                        "7/7 [==============================] - 0s 36ms/step - loss: 3.6474e-05 - accuracy: 1.0000 - val_loss: 0.6296 - val_accuracy: 0.8833\n",
                        "Epoch 258/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 3.6061e-05 - accuracy: 1.0000 - val_loss: 0.6229 - val_accuracy: 0.8861\n",
                        "Epoch 259/500\n",
                        "7/7 [==============================] - 0s 49ms/step - loss: 3.5742e-05 - accuracy: 1.0000 - val_loss: 0.6325 - val_accuracy: 0.8833\n",
                        "Epoch 260/500\n",
                        "7/7 [==============================] - 0s 37ms/step - loss: 3.5288e-05 - accuracy: 1.0000 - val_loss: 0.6270 - val_accuracy: 0.8861\n",
                        "Epoch 261/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 3.4956e-05 - accuracy: 1.0000 - val_loss: 0.6283 - val_accuracy: 0.8861\n",
                        "Epoch 262/500\n",
                        "7/7 [==============================] - 0s 41ms/step - loss: 3.4583e-05 - accuracy: 1.0000 - val_loss: 0.6313 - val_accuracy: 0.8833\n",
                        "Epoch 263/500\n",
                        "7/7 [==============================] - 0s 56ms/step - loss: 3.4237e-05 - accuracy: 1.0000 - val_loss: 0.6314 - val_accuracy: 0.8833\n",
                        "Epoch 264/500\n",
                        "7/7 [==============================] - 0s 55ms/step - loss: 3.3931e-05 - accuracy: 1.0000 - val_loss: 0.6312 - val_accuracy: 0.8833\n",
                        "Epoch 265/500\n",
                        "7/7 [==============================] - 0s 54ms/step - loss: 3.3558e-05 - accuracy: 1.0000 - val_loss: 0.6324 - val_accuracy: 0.8833\n",
                        "Epoch 266/500\n",
                        "7/7 [==============================] - 0s 59ms/step - loss: 3.3258e-05 - accuracy: 1.0000 - val_loss: 0.6258 - val_accuracy: 0.8861\n",
                        "Epoch 267/500\n",
                        "7/7 [==============================] - 0s 48ms/step - loss: 3.3124e-05 - accuracy: 1.0000 - val_loss: 0.6182 - val_accuracy: 0.8861\n",
                        "Epoch 268/500\n",
                        "7/7 [==============================] - 0s 69ms/step - loss: 3.2786e-05 - accuracy: 1.0000 - val_loss: 0.6283 - val_accuracy: 0.8861\n",
                        "Epoch 269/500\n",
                        "7/7 [==============================] - 0s 39ms/step - loss: 3.2372e-05 - accuracy: 1.0000 - val_loss: 0.6314 - val_accuracy: 0.8861\n",
                        "Epoch 270/500\n",
                        "7/7 [==============================] - 0s 42ms/step - loss: 3.2126e-05 - accuracy: 1.0000 - val_loss: 0.6287 - val_accuracy: 0.8861\n",
                        "Epoch 271/500\n",
                        "7/7 [==============================] - 0s 56ms/step - loss: 3.1862e-05 - accuracy: 1.0000 - val_loss: 0.6347 - val_accuracy: 0.8833\n",
                        "Epoch 272/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 3.1521e-05 - accuracy: 1.0000 - val_loss: 0.6357 - val_accuracy: 0.8833\n",
                        "Epoch 273/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 3.1241e-05 - accuracy: 1.0000 - val_loss: 0.6322 - val_accuracy: 0.8861\n",
                        "Epoch 274/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 3.0991e-05 - accuracy: 1.0000 - val_loss: 0.6359 - val_accuracy: 0.8833\n",
                        "Epoch 275/500\n",
                        "7/7 [==============================] - 0s 31ms/step - loss: 3.0694e-05 - accuracy: 1.0000 - val_loss: 0.6322 - val_accuracy: 0.8861\n",
                        "Epoch 276/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 3.0625e-05 - accuracy: 1.0000 - val_loss: 0.6138 - val_accuracy: 0.8889\n",
                        "Epoch 277/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 3.0524e-05 - accuracy: 1.0000 - val_loss: 0.6213 - val_accuracy: 0.8861\n",
                        "Epoch 278/500\n",
                        "7/7 [==============================] - 0s 40ms/step - loss: 3.0029e-05 - accuracy: 1.0000 - val_loss: 0.6394 - val_accuracy: 0.8806\n",
                        "Epoch 279/500\n",
                        "7/7 [==============================] - 0s 43ms/step - loss: 2.9763e-05 - accuracy: 1.0000 - val_loss: 0.6325 - val_accuracy: 0.8861\n",
                        "Epoch 280/500\n",
                        "7/7 [==============================] - 0s 40ms/step - loss: 2.9553e-05 - accuracy: 1.0000 - val_loss: 0.6255 - val_accuracy: 0.8861\n",
                        "Epoch 281/500\n",
                        "7/7 [==============================] - 0s 42ms/step - loss: 2.9201e-05 - accuracy: 1.0000 - val_loss: 0.6336 - val_accuracy: 0.8861\n",
                        "Epoch 282/500\n",
                        "7/7 [==============================] - 0s 36ms/step - loss: 2.9071e-05 - accuracy: 1.0000 - val_loss: 0.6402 - val_accuracy: 0.8833\n",
                        "Epoch 283/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 2.8697e-05 - accuracy: 1.0000 - val_loss: 0.6267 - val_accuracy: 0.8861\n",
                        "Epoch 284/500\n",
                        "7/7 [==============================] - 0s 38ms/step - loss: 2.8575e-05 - accuracy: 1.0000 - val_loss: 0.6311 - val_accuracy: 0.8861\n",
                        "Epoch 285/500\n",
                        "7/7 [==============================] - 0s 38ms/step - loss: 2.8136e-05 - accuracy: 1.0000 - val_loss: 0.6424 - val_accuracy: 0.8806\n",
                        "Epoch 286/500\n",
                        "7/7 [==============================] - 0s 37ms/step - loss: 2.7895e-05 - accuracy: 1.0000 - val_loss: 0.6347 - val_accuracy: 0.8861\n",
                        "Epoch 287/500\n",
                        "7/7 [==============================] - 0s 53ms/step - loss: 2.7656e-05 - accuracy: 1.0000 - val_loss: 0.6371 - val_accuracy: 0.8861\n",
                        "Epoch 288/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 2.7370e-05 - accuracy: 1.0000 - val_loss: 0.6423 - val_accuracy: 0.8806\n",
                        "Epoch 289/500\n",
                        "7/7 [==============================] - 0s 40ms/step - loss: 2.7061e-05 - accuracy: 1.0000 - val_loss: 0.6403 - val_accuracy: 0.8833\n",
                        "Epoch 290/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 2.6821e-05 - accuracy: 1.0000 - val_loss: 0.6409 - val_accuracy: 0.8833\n",
                        "Epoch 291/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 2.6650e-05 - accuracy: 1.0000 - val_loss: 0.6364 - val_accuracy: 0.8833\n",
                        "Epoch 292/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 2.6559e-05 - accuracy: 1.0000 - val_loss: 0.6226 - val_accuracy: 0.8861\n",
                        "Epoch 293/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 2.6407e-05 - accuracy: 1.0000 - val_loss: 0.6292 - val_accuracy: 0.8833\n",
                        "Epoch 294/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 2.6035e-05 - accuracy: 1.0000 - val_loss: 0.6473 - val_accuracy: 0.8806\n",
                        "Epoch 295/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 2.5837e-05 - accuracy: 1.0000 - val_loss: 0.6380 - val_accuracy: 0.8833\n",
                        "Epoch 296/500\n",
                        "7/7 [==============================] - 0s 47ms/step - loss: 2.5549e-05 - accuracy: 1.0000 - val_loss: 0.6414 - val_accuracy: 0.8861\n",
                        "Epoch 297/500\n",
                        "7/7 [==============================] - 0s 31ms/step - loss: 2.5314e-05 - accuracy: 1.0000 - val_loss: 0.6411 - val_accuracy: 0.8861\n",
                        "Epoch 298/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 2.5166e-05 - accuracy: 1.0000 - val_loss: 0.6293 - val_accuracy: 0.8833\n",
                        "Epoch 299/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 2.5012e-05 - accuracy: 1.0000 - val_loss: 0.6351 - val_accuracy: 0.8833\n",
                        "Epoch 300/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 2.4718e-05 - accuracy: 1.0000 - val_loss: 0.6467 - val_accuracy: 0.8778\n",
                        "Epoch 301/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 2.4528e-05 - accuracy: 1.0000 - val_loss: 0.6413 - val_accuracy: 0.8833\n",
                        "Epoch 302/500\n",
                        "7/7 [==============================] - 0s 41ms/step - loss: 2.4332e-05 - accuracy: 1.0000 - val_loss: 0.6406 - val_accuracy: 0.8833\n",
                        "Epoch 303/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 2.4132e-05 - accuracy: 1.0000 - val_loss: 0.6458 - val_accuracy: 0.8806\n",
                        "Epoch 304/500\n",
                        "7/7 [==============================] - 0s 38ms/step - loss: 2.3934e-05 - accuracy: 1.0000 - val_loss: 0.6445 - val_accuracy: 0.8806\n",
                        "Epoch 305/500\n",
                        "7/7 [==============================] - 0s 44ms/step - loss: 2.3748e-05 - accuracy: 1.0000 - val_loss: 0.6444 - val_accuracy: 0.8833\n",
                        "Epoch 306/500\n",
                        "7/7 [==============================] - 0s 39ms/step - loss: 2.3609e-05 - accuracy: 1.0000 - val_loss: 0.6449 - val_accuracy: 0.8806\n",
                        "Epoch 307/500\n",
                        "7/7 [==============================] - 0s 31ms/step - loss: 2.3397e-05 - accuracy: 1.0000 - val_loss: 0.6455 - val_accuracy: 0.8806\n",
                        "Epoch 308/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 2.3200e-05 - accuracy: 1.0000 - val_loss: 0.6444 - val_accuracy: 0.8833\n",
                        "Epoch 309/500\n",
                        "7/7 [==============================] - 0s 57ms/step - loss: 2.3044e-05 - accuracy: 1.0000 - val_loss: 0.6465 - val_accuracy: 0.8806\n",
                        "Epoch 310/500\n",
                        "7/7 [==============================] - 0s 55ms/step - loss: 2.2856e-05 - accuracy: 1.0000 - val_loss: 0.6437 - val_accuracy: 0.8833\n",
                        "Epoch 311/500\n",
                        "7/7 [==============================] - 0s 46ms/step - loss: 2.2722e-05 - accuracy: 1.0000 - val_loss: 0.6450 - val_accuracy: 0.8833\n",
                        "Epoch 312/500\n",
                        "7/7 [==============================] - 0s 45ms/step - loss: 2.2520e-05 - accuracy: 1.0000 - val_loss: 0.6479 - val_accuracy: 0.8833\n",
                        "Epoch 313/500\n",
                        "7/7 [==============================] - 0s 36ms/step - loss: 2.2337e-05 - accuracy: 1.0000 - val_loss: 0.6462 - val_accuracy: 0.8833\n",
                        "Epoch 314/500\n",
                        "7/7 [==============================] - 0s 42ms/step - loss: 2.2177e-05 - accuracy: 1.0000 - val_loss: 0.6469 - val_accuracy: 0.8861\n",
                        "Epoch 315/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 2.2023e-05 - accuracy: 1.0000 - val_loss: 0.6475 - val_accuracy: 0.8861\n",
                        "Epoch 316/500\n",
                        "7/7 [==============================] - 0s 31ms/step - loss: 2.1849e-05 - accuracy: 1.0000 - val_loss: 0.6484 - val_accuracy: 0.8861\n",
                        "Epoch 317/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 2.1685e-05 - accuracy: 1.0000 - val_loss: 0.6482 - val_accuracy: 0.8861\n",
                        "Epoch 318/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 2.1531e-05 - accuracy: 1.0000 - val_loss: 0.6496 - val_accuracy: 0.8833\n",
                        "Epoch 319/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 2.1396e-05 - accuracy: 1.0000 - val_loss: 0.6461 - val_accuracy: 0.8833\n",
                        "Epoch 320/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 2.1196e-05 - accuracy: 1.0000 - val_loss: 0.6497 - val_accuracy: 0.8861\n",
                        "Epoch 321/500\n",
                        "7/7 [==============================] - 0s 36ms/step - loss: 2.1070e-05 - accuracy: 1.0000 - val_loss: 0.6351 - val_accuracy: 0.8833\n",
                        "Epoch 322/500\n",
                        "7/7 [==============================] - 0s 41ms/step - loss: 2.1040e-05 - accuracy: 1.0000 - val_loss: 0.6346 - val_accuracy: 0.8833\n",
                        "Epoch 323/500\n",
                        "7/7 [==============================] - 0s 38ms/step - loss: 2.0800e-05 - accuracy: 1.0000 - val_loss: 0.6431 - val_accuracy: 0.8833\n",
                        "Epoch 324/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 2.0570e-05 - accuracy: 1.0000 - val_loss: 0.6533 - val_accuracy: 0.8833\n",
                        "Epoch 325/500\n",
                        "7/7 [==============================] - 0s 37ms/step - loss: 2.0423e-05 - accuracy: 1.0000 - val_loss: 0.6485 - val_accuracy: 0.8833\n",
                        "Epoch 326/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 2.0292e-05 - accuracy: 1.0000 - val_loss: 0.6491 - val_accuracy: 0.8833\n",
                        "Epoch 327/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 2.0165e-05 - accuracy: 1.0000 - val_loss: 0.6527 - val_accuracy: 0.8861\n",
                        "Epoch 328/500\n",
                        "7/7 [==============================] - 0s 41ms/step - loss: 1.9968e-05 - accuracy: 1.0000 - val_loss: 0.6519 - val_accuracy: 0.8861\n",
                        "Epoch 329/500\n",
                        "7/7 [==============================] - 0s 46ms/step - loss: 1.9823e-05 - accuracy: 1.0000 - val_loss: 0.6486 - val_accuracy: 0.8833\n",
                        "Epoch 330/500\n",
                        "7/7 [==============================] - 1s 82ms/step - loss: 1.9695e-05 - accuracy: 1.0000 - val_loss: 0.6514 - val_accuracy: 0.8833\n",
                        "Epoch 331/500\n",
                        "7/7 [==============================] - 0s 45ms/step - loss: 1.9541e-05 - accuracy: 1.0000 - val_loss: 0.6542 - val_accuracy: 0.8861\n",
                        "Epoch 332/500\n",
                        "7/7 [==============================] - 0s 46ms/step - loss: 1.9554e-05 - accuracy: 1.0000 - val_loss: 0.6542 - val_accuracy: 0.8889\n",
                        "Epoch 333/500\n",
                        "7/7 [==============================] - 0s 39ms/step - loss: 1.9300e-05 - accuracy: 1.0000 - val_loss: 0.6537 - val_accuracy: 0.8861\n",
                        "Epoch 334/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 1.9144e-05 - accuracy: 1.0000 - val_loss: 0.6539 - val_accuracy: 0.8861\n",
                        "Epoch 335/500\n",
                        "7/7 [==============================] - 0s 44ms/step - loss: 1.9011e-05 - accuracy: 1.0000 - val_loss: 0.6543 - val_accuracy: 0.8861\n",
                        "Epoch 336/500\n",
                        "7/7 [==============================] - 0s 40ms/step - loss: 1.8829e-05 - accuracy: 1.0000 - val_loss: 0.6568 - val_accuracy: 0.8917\n",
                        "Epoch 337/500\n",
                        "7/7 [==============================] - 0s 45ms/step - loss: 1.8658e-05 - accuracy: 1.0000 - val_loss: 0.6548 - val_accuracy: 0.8861\n",
                        "Epoch 338/500\n",
                        "7/7 [==============================] - 0s 43ms/step - loss: 1.8545e-05 - accuracy: 1.0000 - val_loss: 0.6563 - val_accuracy: 0.8889\n",
                        "Epoch 339/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 1.8392e-05 - accuracy: 1.0000 - val_loss: 0.6572 - val_accuracy: 0.8917\n",
                        "Epoch 340/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 1.8242e-05 - accuracy: 1.0000 - val_loss: 0.6581 - val_accuracy: 0.8917\n",
                        "Epoch 341/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 1.8123e-05 - accuracy: 1.0000 - val_loss: 0.6572 - val_accuracy: 0.8861\n",
                        "Epoch 342/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 1.7971e-05 - accuracy: 1.0000 - val_loss: 0.6596 - val_accuracy: 0.8917\n",
                        "Epoch 343/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 1.7858e-05 - accuracy: 1.0000 - val_loss: 0.6514 - val_accuracy: 0.8833\n",
                        "Epoch 344/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 1.7742e-05 - accuracy: 1.0000 - val_loss: 0.6525 - val_accuracy: 0.8833\n",
                        "Epoch 345/500\n",
                        "7/7 [==============================] - 0s 51ms/step - loss: 1.7577e-05 - accuracy: 1.0000 - val_loss: 0.6581 - val_accuracy: 0.8861\n",
                        "Epoch 346/500\n",
                        "7/7 [==============================] - 0s 28ms/step - loss: 1.7440e-05 - accuracy: 1.0000 - val_loss: 0.6612 - val_accuracy: 0.8889\n",
                        "Epoch 347/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 1.7343e-05 - accuracy: 1.0000 - val_loss: 0.6579 - val_accuracy: 0.8833\n",
                        "Epoch 348/500\n",
                        "7/7 [==============================] - 0s 55ms/step - loss: 1.7178e-05 - accuracy: 1.0000 - val_loss: 0.6598 - val_accuracy: 0.8861\n",
                        "Epoch 349/500\n",
                        "7/7 [==============================] - 0s 54ms/step - loss: 1.7083e-05 - accuracy: 1.0000 - val_loss: 0.6594 - val_accuracy: 0.8861\n",
                        "Epoch 350/500\n",
                        "7/7 [==============================] - 0s 59ms/step - loss: 1.6943e-05 - accuracy: 1.0000 - val_loss: 0.6556 - val_accuracy: 0.8833\n",
                        "Epoch 351/500\n",
                        "7/7 [==============================] - 0s 37ms/step - loss: 1.6808e-05 - accuracy: 1.0000 - val_loss: 0.6589 - val_accuracy: 0.8833\n",
                        "Epoch 352/500\n",
                        "7/7 [==============================] - 0s 46ms/step - loss: 1.6692e-05 - accuracy: 1.0000 - val_loss: 0.6628 - val_accuracy: 0.8861\n",
                        "Epoch 353/500\n",
                        "7/7 [==============================] - 0s 38ms/step - loss: 1.6545e-05 - accuracy: 1.0000 - val_loss: 0.6635 - val_accuracy: 0.8889\n",
                        "Epoch 354/500\n",
                        "7/7 [==============================] - 0s 42ms/step - loss: 1.6399e-05 - accuracy: 1.0000 - val_loss: 0.6633 - val_accuracy: 0.8861\n",
                        "Epoch 355/500\n",
                        "7/7 [==============================] - 0s 42ms/step - loss: 1.6267e-05 - accuracy: 1.0000 - val_loss: 0.6639 - val_accuracy: 0.8889\n",
                        "Epoch 356/500\n",
                        "7/7 [==============================] - 0s 40ms/step - loss: 1.6135e-05 - accuracy: 1.0000 - val_loss: 0.6631 - val_accuracy: 0.8861\n",
                        "Epoch 357/500\n",
                        "7/7 [==============================] - 0s 37ms/step - loss: 1.6020e-05 - accuracy: 1.0000 - val_loss: 0.6659 - val_accuracy: 0.8889\n",
                        "Epoch 358/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 1.5890e-05 - accuracy: 1.0000 - val_loss: 0.6631 - val_accuracy: 0.8861\n",
                        "Epoch 359/500\n",
                        "7/7 [==============================] - 0s 56ms/step - loss: 1.5771e-05 - accuracy: 1.0000 - val_loss: 0.6636 - val_accuracy: 0.8861\n",
                        "Epoch 360/500\n",
                        "7/7 [==============================] - 0s 38ms/step - loss: 1.5725e-05 - accuracy: 1.0000 - val_loss: 0.6664 - val_accuracy: 0.8889\n",
                        "Epoch 361/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 1.5561e-05 - accuracy: 1.0000 - val_loss: 0.6670 - val_accuracy: 0.8889\n",
                        "Epoch 362/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 1.5441e-05 - accuracy: 1.0000 - val_loss: 0.6665 - val_accuracy: 0.8889\n",
                        "Epoch 363/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 1.5321e-05 - accuracy: 1.0000 - val_loss: 0.6667 - val_accuracy: 0.8861\n",
                        "Epoch 364/500\n",
                        "7/7 [==============================] - 0s 37ms/step - loss: 1.5217e-05 - accuracy: 1.0000 - val_loss: 0.6665 - val_accuracy: 0.8861\n",
                        "Epoch 365/500\n",
                        "7/7 [==============================] - 0s 55ms/step - loss: 1.5114e-05 - accuracy: 1.0000 - val_loss: 0.6669 - val_accuracy: 0.8861\n",
                        "Epoch 366/500\n",
                        "7/7 [==============================] - 0s 39ms/step - loss: 1.5039e-05 - accuracy: 1.0000 - val_loss: 0.6694 - val_accuracy: 0.8889\n",
                        "Epoch 367/500\n",
                        "7/7 [==============================] - 0s 39ms/step - loss: 1.4907e-05 - accuracy: 1.0000 - val_loss: 0.6682 - val_accuracy: 0.8861\n",
                        "Epoch 368/500\n",
                        "7/7 [==============================] - 0s 41ms/step - loss: 1.4793e-05 - accuracy: 1.0000 - val_loss: 0.6675 - val_accuracy: 0.8861\n",
                        "Epoch 369/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 1.4664e-05 - accuracy: 1.0000 - val_loss: 0.6711 - val_accuracy: 0.8889\n",
                        "Epoch 370/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 1.4581e-05 - accuracy: 1.0000 - val_loss: 0.6666 - val_accuracy: 0.8861\n",
                        "Epoch 371/500\n",
                        "7/7 [==============================] - 0s 37ms/step - loss: 1.4471e-05 - accuracy: 1.0000 - val_loss: 0.6624 - val_accuracy: 0.8833\n",
                        "Epoch 372/500\n",
                        "7/7 [==============================] - 0s 53ms/step - loss: 1.4379e-05 - accuracy: 1.0000 - val_loss: 0.6660 - val_accuracy: 0.8833\n",
                        "Epoch 373/500\n",
                        "7/7 [==============================] - 0s 44ms/step - loss: 1.4276e-05 - accuracy: 1.0000 - val_loss: 0.6757 - val_accuracy: 0.8861\n",
                        "Epoch 374/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 1.4228e-05 - accuracy: 1.0000 - val_loss: 0.6579 - val_accuracy: 0.8861\n",
                        "Epoch 375/500\n",
                        "7/7 [==============================] - 0s 41ms/step - loss: 1.4125e-05 - accuracy: 1.0000 - val_loss: 0.6561 - val_accuracy: 0.8861\n",
                        "Epoch 376/500\n",
                        "7/7 [==============================] - 0s 42ms/step - loss: 1.4019e-05 - accuracy: 1.0000 - val_loss: 0.6630 - val_accuracy: 0.8833\n",
                        "Epoch 377/500\n",
                        "7/7 [==============================] - 0s 43ms/step - loss: 1.3844e-05 - accuracy: 1.0000 - val_loss: 0.6812 - val_accuracy: 0.8833\n",
                        "Epoch 378/500\n",
                        "7/7 [==============================] - 0s 39ms/step - loss: 1.3893e-05 - accuracy: 1.0000 - val_loss: 0.6521 - val_accuracy: 0.8889\n",
                        "Epoch 379/500\n",
                        "7/7 [==============================] - 0s 43ms/step - loss: 1.3857e-05 - accuracy: 1.0000 - val_loss: 0.6486 - val_accuracy: 0.8889\n",
                        "Epoch 380/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 1.3734e-05 - accuracy: 1.0000 - val_loss: 0.6567 - val_accuracy: 0.8861\n",
                        "Epoch 381/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 1.3540e-05 - accuracy: 1.0000 - val_loss: 0.6686 - val_accuracy: 0.8833\n",
                        "Epoch 382/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 1.3361e-05 - accuracy: 1.0000 - val_loss: 0.6751 - val_accuracy: 0.8861\n",
                        "Epoch 383/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 1.3234e-05 - accuracy: 1.0000 - val_loss: 0.6758 - val_accuracy: 0.8861\n",
                        "Epoch 384/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 1.3105e-05 - accuracy: 1.0000 - val_loss: 0.6774 - val_accuracy: 0.8889\n",
                        "Epoch 385/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 1.3018e-05 - accuracy: 1.0000 - val_loss: 0.6746 - val_accuracy: 0.8861\n",
                        "Epoch 386/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 1.2912e-05 - accuracy: 1.0000 - val_loss: 0.6771 - val_accuracy: 0.8889\n",
                        "Epoch 387/500\n",
                        "7/7 [==============================] - 0s 41ms/step - loss: 1.2825e-05 - accuracy: 1.0000 - val_loss: 0.6764 - val_accuracy: 0.8861\n",
                        "Epoch 388/500\n",
                        "7/7 [==============================] - 0s 51ms/step - loss: 1.2720e-05 - accuracy: 1.0000 - val_loss: 0.6775 - val_accuracy: 0.8889\n",
                        "Epoch 389/500\n",
                        "7/7 [==============================] - 0s 44ms/step - loss: 1.2646e-05 - accuracy: 1.0000 - val_loss: 0.6776 - val_accuracy: 0.8889\n",
                        "Epoch 390/500\n",
                        "7/7 [==============================] - 0s 40ms/step - loss: 1.2575e-05 - accuracy: 1.0000 - val_loss: 0.6780 - val_accuracy: 0.8889\n",
                        "Epoch 391/500\n",
                        "7/7 [==============================] - 0s 42ms/step - loss: 1.2442e-05 - accuracy: 1.0000 - val_loss: 0.6774 - val_accuracy: 0.8889\n",
                        "Epoch 392/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 1.2367e-05 - accuracy: 1.0000 - val_loss: 0.6789 - val_accuracy: 0.8889\n",
                        "Epoch 393/500\n",
                        "7/7 [==============================] - 0s 46ms/step - loss: 1.2276e-05 - accuracy: 1.0000 - val_loss: 0.6791 - val_accuracy: 0.8889\n",
                        "Epoch 394/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 1.2193e-05 - accuracy: 1.0000 - val_loss: 0.6788 - val_accuracy: 0.8889\n",
                        "Epoch 395/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 1.2114e-05 - accuracy: 1.0000 - val_loss: 0.6798 - val_accuracy: 0.8889\n",
                        "Epoch 396/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 1.2041e-05 - accuracy: 1.0000 - val_loss: 0.6782 - val_accuracy: 0.8889\n",
                        "Epoch 397/500\n",
                        "7/7 [==============================] - 0s 37ms/step - loss: 1.1989e-05 - accuracy: 1.0000 - val_loss: 0.6758 - val_accuracy: 0.8861\n",
                        "Epoch 398/500\n",
                        "7/7 [==============================] - 0s 44ms/step - loss: 1.1875e-05 - accuracy: 1.0000 - val_loss: 0.6766 - val_accuracy: 0.8861\n",
                        "Epoch 399/500\n",
                        "7/7 [==============================] - 0s 59ms/step - loss: 1.1792e-05 - accuracy: 1.0000 - val_loss: 0.6809 - val_accuracy: 0.8889\n",
                        "Epoch 400/500\n",
                        "7/7 [==============================] - 0s 52ms/step - loss: 1.1708e-05 - accuracy: 1.0000 - val_loss: 0.6725 - val_accuracy: 0.8833\n",
                        "Epoch 401/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 1.1664e-05 - accuracy: 1.0000 - val_loss: 0.6704 - val_accuracy: 0.8833\n",
                        "Epoch 402/500\n",
                        "7/7 [==============================] - 0s 40ms/step - loss: 1.1556e-05 - accuracy: 1.0000 - val_loss: 0.6769 - val_accuracy: 0.8861\n",
                        "Epoch 403/500\n",
                        "7/7 [==============================] - 0s 44ms/step - loss: 1.1463e-05 - accuracy: 1.0000 - val_loss: 0.6833 - val_accuracy: 0.8889\n",
                        "Epoch 404/500\n",
                        "7/7 [==============================] - 0s 50ms/step - loss: 1.1402e-05 - accuracy: 1.0000 - val_loss: 0.6770 - val_accuracy: 0.8861\n",
                        "Epoch 405/500\n",
                        "7/7 [==============================] - 0s 53ms/step - loss: 1.1323e-05 - accuracy: 1.0000 - val_loss: 0.6759 - val_accuracy: 0.8861\n",
                        "Epoch 406/500\n",
                        "7/7 [==============================] - 0s 46ms/step - loss: 1.1226e-05 - accuracy: 1.0000 - val_loss: 0.6798 - val_accuracy: 0.8861\n",
                        "Epoch 407/500\n",
                        "7/7 [==============================] - 0s 52ms/step - loss: 1.1126e-05 - accuracy: 1.0000 - val_loss: 0.6798 - val_accuracy: 0.8861\n",
                        "Epoch 408/500\n",
                        "7/7 [==============================] - 0s 42ms/step - loss: 1.1067e-05 - accuracy: 1.0000 - val_loss: 0.6757 - val_accuracy: 0.8833\n",
                        "Epoch 409/500\n",
                        "7/7 [==============================] - 0s 47ms/step - loss: 1.1014e-05 - accuracy: 1.0000 - val_loss: 0.6778 - val_accuracy: 0.8861\n",
                        "Epoch 410/500\n",
                        "7/7 [==============================] - 0s 53ms/step - loss: 1.0893e-05 - accuracy: 1.0000 - val_loss: 0.6838 - val_accuracy: 0.8889\n",
                        "Epoch 411/500\n",
                        "7/7 [==============================] - 0s 65ms/step - loss: 1.0825e-05 - accuracy: 1.0000 - val_loss: 0.6794 - val_accuracy: 0.8861\n",
                        "Epoch 412/500\n",
                        "7/7 [==============================] - 0s 49ms/step - loss: 1.0760e-05 - accuracy: 1.0000 - val_loss: 0.6792 - val_accuracy: 0.8861\n",
                        "Epoch 413/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 1.0676e-05 - accuracy: 1.0000 - val_loss: 0.6837 - val_accuracy: 0.8889\n",
                        "Epoch 414/500\n",
                        "7/7 [==============================] - 0s 39ms/step - loss: 1.0630e-05 - accuracy: 1.0000 - val_loss: 0.6797 - val_accuracy: 0.8861\n",
                        "Epoch 415/500\n",
                        "7/7 [==============================] - 0s 46ms/step - loss: 1.0553e-05 - accuracy: 1.0000 - val_loss: 0.6742 - val_accuracy: 0.8833\n",
                        "Epoch 416/500\n",
                        "7/7 [==============================] - 0s 44ms/step - loss: 1.0486e-05 - accuracy: 1.0000 - val_loss: 0.6775 - val_accuracy: 0.8833\n",
                        "Epoch 417/500\n",
                        "7/7 [==============================] - 0s 45ms/step - loss: 1.0390e-05 - accuracy: 1.0000 - val_loss: 0.6863 - val_accuracy: 0.8889\n",
                        "Epoch 418/500\n",
                        "7/7 [==============================] - 0s 52ms/step - loss: 1.0314e-05 - accuracy: 1.0000 - val_loss: 0.6839 - val_accuracy: 0.8889\n",
                        "Epoch 419/500\n",
                        "7/7 [==============================] - 0s 54ms/step - loss: 1.0244e-05 - accuracy: 1.0000 - val_loss: 0.6801 - val_accuracy: 0.8861\n",
                        "Epoch 420/500\n",
                        "7/7 [==============================] - 0s 53ms/step - loss: 1.0190e-05 - accuracy: 1.0000 - val_loss: 0.6829 - val_accuracy: 0.8861\n",
                        "Epoch 421/500\n",
                        "7/7 [==============================] - 0s 50ms/step - loss: 1.0104e-05 - accuracy: 1.0000 - val_loss: 0.6860 - val_accuracy: 0.8889\n",
                        "Epoch 422/500\n",
                        "7/7 [==============================] - 0s 43ms/step - loss: 1.0034e-05 - accuracy: 1.0000 - val_loss: 0.6863 - val_accuracy: 0.8889\n",
                        "Epoch 423/500\n",
                        "7/7 [==============================] - 0s 45ms/step - loss: 9.9652e-06 - accuracy: 1.0000 - val_loss: 0.6874 - val_accuracy: 0.8889\n",
                        "Epoch 424/500\n",
                        "7/7 [==============================] - 0s 52ms/step - loss: 9.9046e-06 - accuracy: 1.0000 - val_loss: 0.6872 - val_accuracy: 0.8889\n",
                        "Epoch 425/500\n",
                        "7/7 [==============================] - 0s 31ms/step - loss: 9.8220e-06 - accuracy: 1.0000 - val_loss: 0.6804 - val_accuracy: 0.8861\n",
                        "Epoch 426/500\n",
                        "7/7 [==============================] - 0s 39ms/step - loss: 9.7523e-06 - accuracy: 1.0000 - val_loss: 0.6821 - val_accuracy: 0.8861\n",
                        "Epoch 427/500\n",
                        "7/7 [==============================] - 0s 45ms/step - loss: 9.6613e-06 - accuracy: 1.0000 - val_loss: 0.6867 - val_accuracy: 0.8889\n",
                        "Epoch 428/500\n",
                        "7/7 [==============================] - 0s 38ms/step - loss: 9.5650e-06 - accuracy: 1.0000 - val_loss: 0.6906 - val_accuracy: 0.8889\n",
                        "Epoch 429/500\n",
                        "7/7 [==============================] - 0s 36ms/step - loss: 9.4701e-06 - accuracy: 1.0000 - val_loss: 0.6865 - val_accuracy: 0.8889\n",
                        "Epoch 430/500\n",
                        "7/7 [==============================] - 0s 36ms/step - loss: 9.3810e-06 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.8889\n",
                        "Epoch 431/500\n",
                        "7/7 [==============================] - 0s 45ms/step - loss: 9.2885e-06 - accuracy: 1.0000 - val_loss: 0.6903 - val_accuracy: 0.8889\n",
                        "Epoch 432/500\n",
                        "7/7 [==============================] - 0s 42ms/step - loss: 9.2353e-06 - accuracy: 1.0000 - val_loss: 0.6890 - val_accuracy: 0.8889\n",
                        "Epoch 433/500\n",
                        "7/7 [==============================] - 0s 42ms/step - loss: 9.1450e-06 - accuracy: 1.0000 - val_loss: 0.6907 - val_accuracy: 0.8889\n",
                        "Epoch 434/500\n",
                        "7/7 [==============================] - 0s 43ms/step - loss: 9.0784e-06 - accuracy: 1.0000 - val_loss: 0.6887 - val_accuracy: 0.8889\n",
                        "Epoch 435/500\n",
                        "7/7 [==============================] - 0s 51ms/step - loss: 9.0136e-06 - accuracy: 1.0000 - val_loss: 0.6873 - val_accuracy: 0.8861\n",
                        "Epoch 436/500\n",
                        "7/7 [==============================] - 0s 63ms/step - loss: 8.9340e-06 - accuracy: 1.0000 - val_loss: 0.6905 - val_accuracy: 0.8889\n",
                        "Epoch 437/500\n",
                        "7/7 [==============================] - 0s 48ms/step - loss: 8.8505e-06 - accuracy: 1.0000 - val_loss: 0.6919 - val_accuracy: 0.8889\n",
                        "Epoch 438/500\n",
                        "7/7 [==============================] - 0s 42ms/step - loss: 8.7922e-06 - accuracy: 1.0000 - val_loss: 0.6923 - val_accuracy: 0.8889\n",
                        "Epoch 439/500\n",
                        "7/7 [==============================] - 0s 37ms/step - loss: 8.7328e-06 - accuracy: 1.0000 - val_loss: 0.6914 - val_accuracy: 0.8889\n",
                        "Epoch 440/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 8.6601e-06 - accuracy: 1.0000 - val_loss: 0.6933 - val_accuracy: 0.8861\n",
                        "Epoch 441/500\n",
                        "7/7 [==============================] - 0s 47ms/step - loss: 8.5873e-06 - accuracy: 1.0000 - val_loss: 0.6929 - val_accuracy: 0.8861\n",
                        "Epoch 442/500\n",
                        "7/7 [==============================] - 0s 42ms/step - loss: 8.5220e-06 - accuracy: 1.0000 - val_loss: 0.6926 - val_accuracy: 0.8861\n",
                        "Epoch 443/500\n",
                        "7/7 [==============================] - 0s 39ms/step - loss: 8.4628e-06 - accuracy: 1.0000 - val_loss: 0.6929 - val_accuracy: 0.8861\n",
                        "Epoch 444/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 8.4021e-06 - accuracy: 1.0000 - val_loss: 0.6870 - val_accuracy: 0.8861\n",
                        "Epoch 445/500\n",
                        "7/7 [==============================] - 0s 27ms/step - loss: 8.3693e-06 - accuracy: 1.0000 - val_loss: 0.6853 - val_accuracy: 0.8861\n",
                        "Epoch 446/500\n",
                        "7/7 [==============================] - 0s 38ms/step - loss: 8.2761e-06 - accuracy: 1.0000 - val_loss: 0.6906 - val_accuracy: 0.8861\n",
                        "Epoch 447/500\n",
                        "7/7 [==============================] - 0s 67ms/step - loss: 8.2603e-06 - accuracy: 1.0000 - val_loss: 0.7004 - val_accuracy: 0.8833\n",
                        "Epoch 448/500\n",
                        "7/7 [==============================] - 0s 42ms/step - loss: 8.1155e-06 - accuracy: 1.0000 - val_loss: 0.6937 - val_accuracy: 0.8833\n",
                        "Epoch 449/500\n",
                        "7/7 [==============================] - 0s 45ms/step - loss: 8.0148e-06 - accuracy: 1.0000 - val_loss: 0.6921 - val_accuracy: 0.8833\n",
                        "Epoch 450/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 7.8959e-06 - accuracy: 1.0000 - val_loss: 0.6969 - val_accuracy: 0.8861\n",
                        "Epoch 451/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 7.8151e-06 - accuracy: 1.0000 - val_loss: 0.6979 - val_accuracy: 0.8861\n",
                        "Epoch 452/500\n",
                        "7/7 [==============================] - 0s 42ms/step - loss: 7.7284e-06 - accuracy: 1.0000 - val_loss: 0.6984 - val_accuracy: 0.8861\n",
                        "Epoch 453/500\n",
                        "7/7 [==============================] - 0s 50ms/step - loss: 7.6556e-06 - accuracy: 1.0000 - val_loss: 0.6991 - val_accuracy: 0.8861\n",
                        "Epoch 454/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 7.5804e-06 - accuracy: 1.0000 - val_loss: 0.6999 - val_accuracy: 0.8861\n",
                        "Epoch 455/500\n",
                        "7/7 [==============================] - 0s 38ms/step - loss: 7.5197e-06 - accuracy: 1.0000 - val_loss: 0.6950 - val_accuracy: 0.8833\n",
                        "Epoch 456/500\n",
                        "7/7 [==============================] - 0s 41ms/step - loss: 7.4587e-06 - accuracy: 1.0000 - val_loss: 0.6902 - val_accuracy: 0.8833\n",
                        "Epoch 457/500\n",
                        "7/7 [==============================] - 0s 39ms/step - loss: 7.3933e-06 - accuracy: 1.0000 - val_loss: 0.6945 - val_accuracy: 0.8833\n",
                        "Epoch 458/500\n",
                        "7/7 [==============================] - 0s 54ms/step - loss: 7.3303e-06 - accuracy: 1.0000 - val_loss: 0.7006 - val_accuracy: 0.8861\n",
                        "Epoch 459/500\n",
                        "7/7 [==============================] - 0s 41ms/step - loss: 7.2765e-06 - accuracy: 1.0000 - val_loss: 0.7028 - val_accuracy: 0.8861\n",
                        "Epoch 460/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 7.2062e-06 - accuracy: 1.0000 - val_loss: 0.7024 - val_accuracy: 0.8861\n",
                        "Epoch 461/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 7.1407e-06 - accuracy: 1.0000 - val_loss: 0.7029 - val_accuracy: 0.8861\n",
                        "Epoch 462/500\n",
                        "7/7 [==============================] - 0s 36ms/step - loss: 7.0909e-06 - accuracy: 1.0000 - val_loss: 0.7036 - val_accuracy: 0.8861\n",
                        "Epoch 463/500\n",
                        "7/7 [==============================] - 0s 43ms/step - loss: 7.0358e-06 - accuracy: 1.0000 - val_loss: 0.7025 - val_accuracy: 0.8861\n",
                        "Epoch 464/500\n",
                        "7/7 [==============================] - 0s 37ms/step - loss: 6.9681e-06 - accuracy: 1.0000 - val_loss: 0.7038 - val_accuracy: 0.8861\n",
                        "Epoch 465/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 6.9343e-06 - accuracy: 1.0000 - val_loss: 0.6882 - val_accuracy: 0.8861\n",
                        "Epoch 466/500\n",
                        "7/7 [==============================] - 0s 40ms/step - loss: 6.9028e-06 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.8861\n",
                        "Epoch 467/500\n",
                        "7/7 [==============================] - 0s 39ms/step - loss: 6.8517e-06 - accuracy: 1.0000 - val_loss: 0.6925 - val_accuracy: 0.8861\n",
                        "Epoch 468/500\n",
                        "7/7 [==============================] - 0s 38ms/step - loss: 6.7818e-06 - accuracy: 1.0000 - val_loss: 0.7016 - val_accuracy: 0.8833\n",
                        "Epoch 469/500\n",
                        "7/7 [==============================] - 0s 49ms/step - loss: 6.7243e-06 - accuracy: 1.0000 - val_loss: 0.7048 - val_accuracy: 0.8861\n",
                        "Epoch 470/500\n",
                        "7/7 [==============================] - 0s 40ms/step - loss: 6.6709e-06 - accuracy: 1.0000 - val_loss: 0.7047 - val_accuracy: 0.8861\n",
                        "Epoch 471/500\n",
                        "7/7 [==============================] - 0s 39ms/step - loss: 6.6129e-06 - accuracy: 1.0000 - val_loss: 0.7065 - val_accuracy: 0.8861\n",
                        "Epoch 472/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 6.5698e-06 - accuracy: 1.0000 - val_loss: 0.7050 - val_accuracy: 0.8861\n",
                        "Epoch 473/500\n",
                        "7/7 [==============================] - 0s 39ms/step - loss: 6.5188e-06 - accuracy: 1.0000 - val_loss: 0.7048 - val_accuracy: 0.8861\n",
                        "Epoch 474/500\n",
                        "7/7 [==============================] - 0s 35ms/step - loss: 6.4763e-06 - accuracy: 1.0000 - val_loss: 0.7043 - val_accuracy: 0.8833\n",
                        "Epoch 475/500\n",
                        "7/7 [==============================] - 0s 49ms/step - loss: 6.4228e-06 - accuracy: 1.0000 - val_loss: 0.7066 - val_accuracy: 0.8861\n",
                        "Epoch 476/500\n",
                        "7/7 [==============================] - 0s 31ms/step - loss: 6.3832e-06 - accuracy: 1.0000 - val_loss: 0.7065 - val_accuracy: 0.8861\n",
                        "Epoch 477/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 6.3368e-06 - accuracy: 1.0000 - val_loss: 0.7069 - val_accuracy: 0.8861\n",
                        "Epoch 478/500\n",
                        "7/7 [==============================] - 0s 31ms/step - loss: 6.2957e-06 - accuracy: 1.0000 - val_loss: 0.7021 - val_accuracy: 0.8861\n",
                        "Epoch 479/500\n",
                        "7/7 [==============================] - 0s 31ms/step - loss: 6.2488e-06 - accuracy: 1.0000 - val_loss: 0.7018 - val_accuracy: 0.8861\n",
                        "Epoch 480/500\n",
                        "7/7 [==============================] - 0s 45ms/step - loss: 6.2027e-06 - accuracy: 1.0000 - val_loss: 0.7062 - val_accuracy: 0.8833\n",
                        "Epoch 481/500\n",
                        "7/7 [==============================] - 0s 37ms/step - loss: 6.1589e-06 - accuracy: 1.0000 - val_loss: 0.7030 - val_accuracy: 0.8861\n",
                        "Epoch 482/500\n",
                        "7/7 [==============================] - 0s 50ms/step - loss: 6.1272e-06 - accuracy: 1.0000 - val_loss: 0.6985 - val_accuracy: 0.8861\n",
                        "Epoch 483/500\n",
                        "7/7 [==============================] - 0s 39ms/step - loss: 6.0668e-06 - accuracy: 1.0000 - val_loss: 0.7039 - val_accuracy: 0.8861\n",
                        "Epoch 484/500\n",
                        "7/7 [==============================] - 0s 29ms/step - loss: 6.0120e-06 - accuracy: 1.0000 - val_loss: 0.7102 - val_accuracy: 0.8861\n",
                        "Epoch 485/500\n",
                        "7/7 [==============================] - 0s 45ms/step - loss: 5.9711e-06 - accuracy: 1.0000 - val_loss: 0.7081 - val_accuracy: 0.8861\n",
                        "Epoch 486/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 5.9161e-06 - accuracy: 1.0000 - val_loss: 0.7080 - val_accuracy: 0.8861\n",
                        "Epoch 487/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 5.8672e-06 - accuracy: 1.0000 - val_loss: 0.7106 - val_accuracy: 0.8861\n",
                        "Epoch 488/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 5.8141e-06 - accuracy: 1.0000 - val_loss: 0.7107 - val_accuracy: 0.8861\n",
                        "Epoch 489/500\n",
                        "7/7 [==============================] - 0s 30ms/step - loss: 5.7706e-06 - accuracy: 1.0000 - val_loss: 0.7102 - val_accuracy: 0.8861\n",
                        "Epoch 490/500\n",
                        "7/7 [==============================] - 0s 36ms/step - loss: 5.7305e-06 - accuracy: 1.0000 - val_loss: 0.7109 - val_accuracy: 0.8889\n",
                        "Epoch 491/500\n",
                        "7/7 [==============================] - 0s 52ms/step - loss: 5.6815e-06 - accuracy: 1.0000 - val_loss: 0.7101 - val_accuracy: 0.8861\n",
                        "Epoch 492/500\n",
                        "7/7 [==============================] - 0s 32ms/step - loss: 5.6365e-06 - accuracy: 1.0000 - val_loss: 0.7136 - val_accuracy: 0.8861\n",
                        "Epoch 493/500\n",
                        "7/7 [==============================] - 0s 43ms/step - loss: 5.5975e-06 - accuracy: 1.0000 - val_loss: 0.7101 - val_accuracy: 0.8861\n",
                        "Epoch 494/500\n",
                        "7/7 [==============================] - 0s 34ms/step - loss: 5.5540e-06 - accuracy: 1.0000 - val_loss: 0.7123 - val_accuracy: 0.8861\n",
                        "Epoch 495/500\n",
                        "7/7 [==============================] - 0s 50ms/step - loss: 5.5173e-06 - accuracy: 1.0000 - val_loss: 0.7021 - val_accuracy: 0.8861\n",
                        "Epoch 496/500\n",
                        "7/7 [==============================] - 0s 62ms/step - loss: 5.4893e-06 - accuracy: 1.0000 - val_loss: 0.6998 - val_accuracy: 0.8861\n",
                        "Epoch 497/500\n",
                        "7/7 [==============================] - 0s 42ms/step - loss: 5.4490e-06 - accuracy: 1.0000 - val_loss: 0.7043 - val_accuracy: 0.8861\n",
                        "Epoch 498/500\n",
                        "7/7 [==============================] - 0s 40ms/step - loss: 5.4014e-06 - accuracy: 1.0000 - val_loss: 0.7123 - val_accuracy: 0.8861\n",
                        "Epoch 499/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 5.3604e-06 - accuracy: 1.0000 - val_loss: 0.7160 - val_accuracy: 0.8889\n",
                        "Epoch 500/500\n",
                        "7/7 [==============================] - 0s 33ms/step - loss: 5.3312e-06 - accuracy: 1.0000 - val_loss: 0.7128 - val_accuracy: 0.8861\n"
                    ]
                }
            ],
            "source": [
                "# your code here\n",
                "'''\n",
                "Build a fully-connected network (FCN) with the architecture given below using tensorflow.keras and assign it to a variable called model_overfit:\n",
                "\n",
                "Number of hidden layers: 3\n",
                "Nodes per hidden layer: 100, 100, 100\n",
                "Activation function: ReLU\n",
                "Loss function: binary_crossentropy\n",
                "Output unit: Sigmoid\n",
                "Optimizer: adam (use the defaults; no other tuning)\n",
                "Epochs: no more than 2,000\n",
                "Batch size: 128\n",
                "Validation size: 0.3\n",
                "'''\n",
                "NN_model = tf.keras.models.Sequential()\n",
                "NN_model.add(tf.keras.layers.Dense(100, input_dim=df.shape[1]-1, activation='relu'))\n",
                "NN_model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
                "NN_model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
                "NN_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
                "NN_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
                "history = NN_model.fit(df.iloc[:,:-1], df.iloc[:,-1], epochs=500, batch_size=128, verbose=1, validation_split=0.3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 122,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<matplotlib.legend.Legend at 0x7fe65833ff70>"
                        ]
                    },
                    "execution_count": 122,
                    "metadata": {},
                    "output_type": "execute_result"
                },
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 432x288 with 1 Axes>"
                        ]
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Plot the training accuracy and validation accuracy\n",
                "plt.plot(history.history['accuracy'])\n",
                "plt.plot(history.history['val_accuracy'])\n",
                "plt.legend(['train', 'val'], loc='upper left')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 169,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_test = pd.read_csv('kmnist_test.csv/kmnist_test.csv')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 173,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "63/63 [==============================] - 0s 4ms/step\n"
                    ]
                }
            ],
            "source": [
                "# fit on test data\n",
                "y_cap = NN_model.predict(df_test.iloc[:,:])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 180,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<matplotlib.image.AxesImage at 0x7fe648718370>"
                        ]
                    },
                    "execution_count": 180,
                    "metadata": {},
                    "output_type": "execute_result"
                },
                {
                    "data": {
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAVbElEQVR4nO3de3hV1ZkG8PcjISAiSATCVQIJF5GGgDGgUkcfLwG0glUZ8TLYscbrjLYM9TZjnXamRVtRO1acKFS0VEsrVDqlKFJHa1uBgFxFIOFiEq4Kys0ICd/8kQOTata3w97nRtf7ex6ehPNmnb1yyMc5Od9ee4mqgoj+9rVI9QSIKDlY7ESeYLETeYLFTuQJFjuRJzKTebAsaaWtcXIyD3mMZGbYX5BpPxRa+7kz61dw0By7fmUb+9gB+hYcMHOBOLNV+04zx7ba9JmZB31vQaJ8720G2nn7Fvbctuzp5MyyauzHNMihbvbPcdbWgH+z1q2cmfWzBgCHuruPfXjPbtQfONDkD4REab2JyEgATwLIAPCcqk62vr6dZOswuSj08aLI6Gj/0KNTthnXr93gzF7butwcW9Kt0D52gP+pWWrmLcX9H1neH75hjs2/4T0zD/regkT53gvtqWFUu5VmfuvLtzqz3Af/EmZKx2z+j3PMPPdf7fvP6J/vzOrXVZhjN/3Afezqpx5HbXVVk8Ue+mW8iGQA+CmAUQAGAhgvIgH/FxNRqkT5nb0YQIWqblTVQwBeBjAmPtMioniLUuzdAVQ1+nt17La/IiKlIlIuIuWHYf8uQkSJk/B341W1TFWLVLWoJdxvShBRYkUp9hoAPRv9vUfsNiJKQ1GKfQmAviLSW0SyAFwLYG58pkVE8Ra19TYawBNoaL1NV9X/tL4+ka23oBZR3i9vM/P8b70b+tibXi6w7/v2KjM/XJBr5kcy7f+TMxfarTnLrrn9zbzTFevM/MBVw8z85FcWHfecmktaZpm5njXAHb5rt+0SLbNrF2dWt2176PtdpAuxV3c32XqLdFKNqs4DMC/KfRBRcvB0WSJPsNiJPMFiJ/IEi53IEyx2Ik+w2Ik8kdT17BCBtHKfMjt/k92TLVg83pmVdLMPnY/wffQgff/lYzPfMs2e3OrhPzPzqysvNvOKM891Zu031pljO12x2MyDROmjZ/buZeZ1m7aY+Sfjhpr5p2Pda8rbzrOXqGb/zF6iOrFijZk/ln+mmR/K7+rMWkTos1v4zE7kCRY7kSdY7ESeYLETeYLFTuQJFjuRJyItcT1eiVziGrTMtPe10ZY0bnjKvZRz9LDl5tinutvtqaArsG54/iwz73tT+CWuQUuDB7xzo5n3GrfKzB8x2qn39raXx6azoMft/NtLzXzk999yZm8VnBRmSgDsJa58ZifyBIudyBMsdiJPsNiJPMFiJ/IEi53IEyx2Ik8ktc9eNLi1Ln6tpzMv6WH3k/f8to8z63CZe5dVANg02V7S2Pu+8Lt6ZnRybw0MAPNWLAh9381h9emD+sHF711j5kGPaxQtBhmXegZwZPUHCTt2VJm93D/HAKB795v5lufcy557XGUvn7X+TYtLqlC+opZ9diKfsdiJPMFiJ/IEi53IEyx2Ik+w2Ik8wWIn8kRSLyW9fmWbgJ6wvS77nBW5oY/dbei20GODfPCwu/8PAIMfucPMWx60z3U47Vn7HACr7/r1ikvMsYfrMsw8SFAf3/r3DuqjZ3ToYOb1e/aYuTW33vO/aY7t94/lZl63xd6GO0ib39nnGCRCpGIXkc0A9gGoB1CnqkXxmBQRxV88ntkvVNWP4nA/RJRA/J2dyBNRi10BvC4iS0WkyYtuiUipiJSLSPlhfB7xcEQUVtSX8SNUtUZEOgNYICIfqOrbjb9AVcsAlAENF5yMeDwiCinSM7uq1sQ+7gQwB0BxPCZFRPEXuthF5GQROeXo5wAuBbA6XhMjoviK8jI+B8AcETl6P79Q1flRJpM/83Yzz5sUfs15q0s3m/muuf3NvNMV69xh28Pm2C5P2teNr/xFoZmf9qwZ46t33erM2sy2j90Zu+w7jyioD28Jup7+dyrta9Zb40+/PNrbVVu+Z18foddD9s9q9vTwP8uXXjXBma2vnOrMQhe7qm4EMDjseCJKLrbeiDzBYifyBIudyBMsdiJPsNiJPJHUJa5Bglpr9RcOdWafTrQv3Zt9+Xoz/6TqVDO3LhadMz/LHBuk4oLnzXzwt+wlssXXrnBmS3PsFtHAG9ea+c9z/9fMEynK8tkgbd60L9dcGbAFeJ/OH5p50KmiT2z+szO78fsTzbGd51Y4M/nc3QbmMzuRJ1jsRJ5gsRN5gsVO5AkWO5EnWOxEnmCxE3kiqVs2t5NsHSYXhR5v9V2Deq5Wjx4A3pg53cyj9HSDSNEgM58/9+dmHmVuc6oXm3mbFtHOIYgi6mN+8ep9zuyNQadEuu8gNfeea+bdH3H32aNYpAuxV3dzy2Yin7HYiTzBYifyBIudyBMsdiJPsNiJPMFiJ/JEWq1nDxKl75rx5rKE3XeQz8bYe2ec9Krd607k3BLdRx/8I/da/BWTnjbH7r9mmJkPmrTSzN8wTl+o+je7D95qtxmj80/tPvmBAYnb6sw636S45KAz4zM7kSdY7ESeYLETeYLFTuQJFjuRJ1jsRJ5gsRN5Iq367BkD+5n5vDdmObNE9qKBaGvpP/6HA2a+Zqr7vpvDOn6ULZMBYNTo68z82bllZt7lcaMfPck+dttf2dtNL2trXxP/0N1NLusGAAy4ZIM5dt2uzmb+2oPLzbykmxlHUrB4vDOrPPCcMwt8ZheR6SKyU0RWN7otW0QWiMiG2McOxzthIkqu5ryMfx7AyC/cdh+AharaF8DC2N+JKI0FFruqvg3giycPjgEwI/b5DABj4zstIoq3sL+z56jqttjn2wHkuL5QREoBlAJAa7QJeTgiiiryu/HacMVK51UrVbVMVYtUtaglWkU9HBGFFLbYd4hIVwCIfdwZvykRUSKELfa5ACbEPp8A4NX4TIeIEiXwuvEi8hKACwB0BLADwHcB/AbALACnA9gCYJyqBqwAjn7d+FTKyHH3Xet3pO8Lm5132Ou2Oz+dmOuXHxWlz5/ocycsGf3yzLx+fWXijm38rAHAxv9yvkWGD+/9b9RW1jR5gkHgG3Sq6urgn5hVS+Qpni5L5AkWO5EnWOxEnmCxE3mCxU7kiaQucZXMDGScmu3MtWcXc/zVL/3Bmc06wx4bVSLba32X2GcWPtXdXuppW26mJU8XmnlG/3wzr19XcZzziZ8WhQPNfN0t7m2Z+95pP6a1ufZCzpbrzTiw5Wi1FTfeYT/mvca526Xb9TNnxmd2Ik+w2Ik8wWIn8gSLncgTLHYiT7DYiTzBYifyROAS13iKusT10IJezizrki2h77c5Htnk7sve29veWjiqqJeDtiR6Gemmlwuc2frzXzDHpnKJa1SVjw0387yJ7ybkuIt0Ifbq7iaXuPKZncgTLHYiT7DYiTzBYifyBIudyBMsdiJPsNiJPHFC9dnTVVAf/LIRY828buPmhB7f0n/a7WY+YYz7GgIA8FbBSaGPHWXNNwBk9uhu5nXVNcc5o/8XOLe1l9t3cFF16GNHwT47EbHYiXzBYifyBIudyBMsdiJPsNiJPMFiJ/JEUvvsHc/oqJfNuMKZ77ispTm+/mP3rtBRe7aJFHVu6bztcdA17197c6gz23DDVHNs1LlvnHyOM+tz318i3XfQv8nwSbeZefuZ4dezf3yL+/v6YPbjOLirKlyfXUSmi8hOEVnd6LaHRaRGRJbH/owONWsiSprmvIx/HsDIJm5/XFULY3/mxXdaRBRvgcWuqm8DcL9+JqITQpQ36O4SkZWxl/nOjbFEpFREykWkvPaT2giHI6Iowhb7VAB5AAoBbAPwmOsLVbVMVYtUtaj1qa1DHo6IogpV7Kq6Q1XrVfUIgGcBFMd3WkQUb6GKXUS6NvrrlQBWu76WiNJD4P7sIvISgAsAdBSRagDfBXCBiBQCUACbAdzanIMd+uAItg7fF3aumFixxpkVLB5vju2KtaGPC9h91aB+cOHkO8w8B+79thNt/zj7+uZtZ9n94KC940u+87k7vMEcGpnVS4967kNQ3h6JuS48AJT/u/v8hOJ3dzmzwGJX1aaqaFqzZkVEaYOnyxJ5gsVO5AkWO5EnWOxEnmCxE3ki8N34eGp1hiDvRfdZdJVn26fTPpZ/pjMLaq3dX7nSzH+Y595aGLCXLAa1WXJ+ktjW2l017i2jP77ZvcQUAE6bFm2pZ+/ff9PMN219LvR9V0yx24KV1z5j5lGWyAa15i78hv19Z71WbuaZuac7s7rNH5pjw+IzO5EnWOxEnmCxE3mCxU7kCRY7kSdY7ESeYLETeSKpl5Ju1bOndv/2Pc48b2LilgVGZfVdvzLFXsLa7cd2nz3KpaIBYMgP3MfPPGj/+2ZPj9ZnD3JkRKEzWzDreXNs1EtJW49rn9dvNsf2vWmpmVe8OMTM8298z8wtUZbfcstmImKxE/mCxU7kCRY7kSdY7ESeYLETeYLFTuSJpPbZ27fK0XO7XOfM66qqE3bsDn/KNvM959nb2e37e/fa6lN+aZ8fELWPHmRUH/fcjtQmdsutRH5vvzto7yD0k/wBCTt2Om8B/lGpe8vmda9E2LKZiP42sNiJPMFiJ/IEi53IEyx2Ik+w2Ik8wWIn8kRS++ztJFuHyUXO/Eeb7X71pFx3P3nbt881x3adkrptkfXcwWb++q9nmPnw5VebefvRFc5sy/fcPVkA6PVQYtezWxLdy7buv+i7t5tjP+vYZKv6mDX/9LSZB24hPjbaFuIukdazi0hPEXlTRN4XkTUicnfs9mwRWSAiG2IfO8R74kQUP815GV8HYKKqDgQwHMCdIjIQwH0AFqpqXwALY38nojQVWOyquk1Vl8U+3wdgLYDuAMYAOPr6cwaAsQmaIxHFwXHt9SYiuQCGAFgEIEdVt8Wi7QByHGNKAZQCQGu0CT1RIoqm2e/Gi0hbAK8AuEdV9zbOtOFdvibf6VPVMlUtUtWilmgVabJEFF6zil1EWqKh0Geq6uzYzTtEpGss7wpgZ2KmSETxEPgyXkQEwDQAa1V1SqNoLoAJACbHPr4adF/tzqzHxbP2OfOCLHtJY2bPHs7s1JJtzgwAMMWOE0n+vCLS+HcLf23mJSh0ZkUX2y2eXQ+FmdGJwWrdlW+dGnosAPRrZ7fuet8fvqUZZdlwcclBZ9ac39nPA3AjgFUicnQWD6ChyGeJyM0AtgAYF3qGRJRwgcWuqu8AcJ1h4D5DhojSCk+XJfIEi53IEyx2Ik+w2Ik8wWIn8kRaLXENktkn15nVbdwc+n4BYGbVn8y8ePZEZ7bxmmfMsSNPLzLz+R+Wm3lQz7f2a8XObOt5GebYrE/tpZw9fhhtaXCUnvHoIZeaedWEfDPv9mj4uafzpaQt3LKZiFjsRL5gsRN5gsVO5AkWO5EnWOxEnmCxE3niuC5LlWpRe+mW63ueZ+Ybt7p76VN29zHHVsz4ipmXdKsz8yCtf7vYmbXpY19iu8uTqbvEdnCv2r4eSrdHE3e9lD6/us3M+8K+7Pn+ce7LngNA93/e4MwOXG9fvq1uS5WZu/CZncgTLHYiT7DYiTzBYifyBIudyBMsdiJPsNiJPHFCrWe3bL/H7id3K1tu5kcOuq+3HdX6srPNvF/pkoQdO8jBK4eZeZs5i5I0ky87UdeUA9HmHmUs17MTEYudyBcsdiJPsNiJPMFiJ/IEi53IEyx2Ik80Z3/2ngBeAJADQAGUqeqTIvIwgFsA7Ip96QOqOs+6r6wBLdBtxinOfOtw997tQbo8Ya/Lri2xr91+JMv+f89aMx4kqI8e1Fc9a6m9G3bHr60/3ikdk8o+evX99rkRJd3s8Xt/n2fm7UZVOrOCZfb18lcOjXb+SULPAWhh7AVQ746ac/GKOgATVXWZiJwCYKmILIhlj6vqj5s/SyJKlebsz74NwLbY5/tEZC2A7omeGBHF13H9zi4iuQCGADj62u8uEVkpItNFpINjTKmIlItIee0ntdFmS0ShNbvYRaQtgFcA3KOqewFMBZAHoBANz/yPNTVOVctUtUhVi1qf2jr6jIkolGYVu4i0REOhz1TV2QCgqjtUtV5VjwB4FoB7d0EiSrnAYhcRATANwFpVndLo9q6NvuxKAKvjPz0iipfAJa4iMgLAHwGsAnAkdvMDAMaj4SW8AtgM4NbYm3lOuYNO0QdfKXTms87oYs6lZvaZzqz719eYY1Pp71Z+ZuZvFZyUpJl82Ym8jDSRoj4uQePPv6PUmZ30G7vNa913cUkVylfUNtlXbM678e8AaGqw2VMnovTCM+iIPMFiJ/IEi53IEyx2Ik+w2Ik8wWIn8kRyLyXdP0eHPXOd+wsuqjbHX/n+Lmc2Z2Anc2xQ3zNIIvvNlTOHmHne9e8l7NhBEtmHP/JV+/tu8Uf7+5az7a2wdckqZ7Z3vL2lcruX7C2Zg6yfap9Q2u/Ope7wiLFONQAvJU1ELHYiX7DYiTzBYifyBIudyBMsdiJPsNiJPJHUPruI7AKwpdFNHQF8lLQJHJ90nVu6zgvg3MKK59x6qWqTJ50ktdi/dHCRclW1L+ieIuk6t3SdF8C5hZWsufFlPJEnWOxEnkh1sZel+PiWdJ1bus4L4NzCSsrcUvo7OxElT6qf2YkoSVjsRJ5ISbGLyEgRWSciFSJyXyrm4CIim0VklYgsF5HyFM9luojsFJHVjW7LFpEFIrIh9rHJPfZSNLeHRaQm9tgtF5HRKZpbTxF5U0TeF5E1InJ37PaUPnbGvJLyuCX9d3YRyQCwHsAlAKoBLAEwXlXfT+pEHERkM4AiVU35CRgicj6A/QBeUNVBsdseBbBbVSfH/qPsoKr3psncHgawP9XbeMd2K+raeJtxAGMB3IQUPnbGvMYhCY9bKp7ZiwFUqOpGVT0E4GUAY1Iwj7Snqm8D2P2Fm8cAmBH7fAYafliSzjG3tKCq21R1WezzfQCObjOe0sfOmFdSpKLYuwOoavT3aqTXfu8K4HURWSoi7j16Uien0TZb2wHkpHIyTQjcxjuZvrDNeNo8dmG2P4+Kb9B92QhVHQpgFIA7Yy9X05I2/A6WTr3TZm3jnSxNbDN+TCofu7Dbn0eVimKvAdCz0d97xG5LC6paE/u4E8AcpN9W1DuO7qAb+7gzxfM5Jp228W5qm3GkwWOXyu3PU1HsSwD0FZHeIpIF4FoAc1Mwjy8RkZNjb5xARE4GcCnSbyvquQAmxD6fAODVFM7lr6TLNt6ubcaR4scu5dufq2rS/wAYjYZ35CsBPJiKOTjm1QfAitifNameG4CX0PCy7jAa3tu4GcBpABYC2ADgDQDZaTS3F9GwtfdKNBRW1xTNbQQaXqKvBLA89md0qh87Y15Jedx4uiyRJ/gGHZEnWOxEnmCxE3mCxU7kCRY7kSdY7ESeYLETeeL/ALWW0hhWE6RyAAAAAElFTkSuQmCC",
                        "text/plain": [
                            "<Figure size 432x288 with 1 Axes>"
                        ]
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": [
                "plt.imshow(df_test.iloc[-1,:].values.reshape(28,28))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 181,
            "metadata": {},
            "outputs": [],
            "source": [
                "pred = y_cap > 0.5"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 190,
            "metadata": {},
            "outputs": [],
            "source": [
                "pd.DataFrame(pred.astype(int)).to_csv('submission.csv', index=True)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**INTERPRETATION:**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*your answer here*"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class='exercise-r'>\n",
                "\n",
                "**2.1.3** \n",
                "\n",
                "**2.1.3.1**\n",
                "    \n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model: \"sequential_3\"\n",
                        "_________________________________________________________________\n",
                        " Layer (type)                Output Shape              Param #   \n",
                        "=================================================================\n",
                        " dense_11 (Dense)            (None, 100)               78500     \n",
                        "                                                                 \n",
                        " dense_12 (Dense)            (None, 100)               10100     \n",
                        "                                                                 \n",
                        " dense_13 (Dense)            (None, 100)               10100     \n",
                        "                                                                 \n",
                        " dense_14 (Dense)            (None, 1)                 101       \n",
                        "                                                                 \n",
                        "=================================================================\n",
                        "Total params: 98,801\n",
                        "Trainable params: 98,801\n",
                        "Non-trainable params: 0\n",
                        "_________________________________________________________________\n"
                    ]
                }
            ],
            "source": [
                "# your code here\n",
                "#  Display your model summary and your training and validation accuracy and loss.\n",
                "NN_model.summary()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "38/38 [==============================] - 1s 8ms/step\n",
                        "NN_model_train_auc: 0.9938521684244479\n",
                        "38/38 [==============================] - 0s 7ms/step\n",
                        "NN_model_test_auc: 0.9938521684244479\n"
                    ]
                }
            ],
            "source": [
                "print(\"NN_model_train_auc:\", roc_auc_score(df.iloc[:,-1], NN_model.predict(df.iloc[:,:-1])))\n",
                "print(\"NN_model_test_auc:\", roc_auc_score(df.iloc[:,-1], NN_model.predict(df.iloc[:,:-1])))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class='exercise-r'>\n",
                "\n",
                "**2.1.3.2**\n",
                "    \n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# your code here"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class='exercise-r'>\n",
                "\n",
                "**2.1.3.3**\n",
                "    \n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# your code here"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class='exercise-r'>\n",
                "\n",
                "**2.1.3.4**\n",
                "    \n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# your code here"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class='exercise-r'>\n",
                "\n",
                "**2.1.3.5**\n",
                "    \n",
                "</div>"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**YOUR KAGGLE LEADERBOARD NAME:**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*your answer here*"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
                "\n",
                "<h1>PART 2.2 [30points]: KMNIST Classification using CNNs</h1>\n",
                "\n",
                "[Return to contents](#contents)\n",
                "\n",
                "In this part of Homework, you will now contruct a CNN-based model in order to best classify the Kannada MNIST dataset.\n",
                "\n",
                "**2.2.1 [5 points]** Examine the dataset and prepare the data by appropriately standardizing, reshaping and type-checking. \n",
                "\n",
                "**2.2.2 [20 points]** Construct a simple CNN model - with not more than 10 layers. Please ensure that you use the following layers/parameters in order to contruct the model -\n",
                "1. Maxpooling\n",
                "2. Dense layers\n",
                "3. Regularization methods such as Adam, Drop out, Batch Normalization etc. \n",
                "\n",
                "**2.2.3 [5 points]** Perform error analysis on the predictions of your model and report classification accuracy. This should also include loss plots that allow for comparision of the model performance across the epochs. Conclusively, provide a detailed inference of why certain misclassifications would have taken place.\n",
                "\n",
                "\n",
                "\n",
                "</div>"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id=\"part2.2solutions\"></a>\n",
                "\n",
                "\n",
                "## PART 2.2 Solutions\n",
                "\n",
                "[Return to contents](#contents)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**2.2.1**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#your code here\n",
                "#  prepare the data by appropriately standardizing, reshaping and type-checking\n",
                "df_train = pd.read_csv('kmnist_train.csv/kmnist_train.csv')\n",
                "df_test = pd.read_csv('kmnist_test.csv/kmnist_test.csv')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**2.2.2**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model: \"sequential_9\"\n",
                        "_________________________________________________________________\n",
                        " Layer (type)                Output Shape              Param #   \n",
                        "=================================================================\n",
                        " conv2d_14 (Conv2D)          (None, 26, 26, 32)        320       \n",
                        "                                                                 \n",
                        " max_pooling2d_14 (MaxPoolin  (None, 13, 13, 32)       0         \n",
                        " g2D)                                                            \n",
                        "                                                                 \n",
                        " conv2d_15 (Conv2D)          (None, 11, 11, 64)        18496     \n",
                        "                                                                 \n",
                        " max_pooling2d_15 (MaxPoolin  (None, 5, 5, 64)         0         \n",
                        " g2D)                                                            \n",
                        "                                                                 \n",
                        " conv2d_16 (Conv2D)          (None, 3, 3, 128)         73856     \n",
                        "                                                                 \n",
                        " max_pooling2d_16 (MaxPoolin  (None, 1, 1, 128)        0         \n",
                        " g2D)                                                            \n",
                        "                                                                 \n",
                        " flatten_4 (Flatten)         (None, 128)               0         \n",
                        "                                                                 \n",
                        " dense_23 (Dense)            (None, 128)               16512     \n",
                        "                                                                 \n",
                        " dense_24 (Dense)            (None, 1)                 129       \n",
                        "                                                                 \n",
                        "=================================================================\n",
                        "Total params: 109,313\n",
                        "Trainable params: 109,313\n",
                        "Non-trainable params: 0\n",
                        "_________________________________________________________________\n"
                    ]
                }
            ],
            "source": [
                "#your code here\n",
                "'''\n",
                "Construct a simple CNN model - with not more than 10 layers. Please ensure that you use the following layers/parameters in order to contruct the model -\n",
                "\n",
                "Maxpooling\n",
                "Dense layers\n",
                "Regularization methods such as Adam, Drop out, Batch Normalization etc.\n",
                "'''\n",
                "CNN_model = tf.keras.models.Sequential()\n",
                "CNN_model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\n",
                "# Add more regulaization layers as well\n",
                "CNN_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
                "CNN_model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
                "CNN_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
                "CNN_model.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
                "CNN_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
                "CNN_model.add(tf.keras.layers.Flatten())\n",
                "CNN_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
                "CNN_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
                "#binary cross entropy\n",
                "CNN_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
                "CNN_model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#prepare the train data for the above neural network\n",
                "X = df_train.iloc[:,:-1].values.reshape(-1,28,28,1)\n",
                "y = df_train.iloc[:,-1].values.astype('float32').reshape((-1,1))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/100\n",
                        "7/7 [==============================] - 4s 184ms/step - loss: 2.7099 - accuracy: 0.4893 - val_loss: 0.8160 - val_accuracy: 0.4944\n",
                        "Epoch 2/100\n",
                        "7/7 [==============================] - 0s 66ms/step - loss: 0.9776 - accuracy: 0.5107 - val_loss: 0.6826 - val_accuracy: 0.5583\n",
                        "Epoch 3/100\n",
                        "7/7 [==============================] - 0s 71ms/step - loss: 0.7211 - accuracy: 0.5214 - val_loss: 0.6873 - val_accuracy: 0.5611\n",
                        "Epoch 4/100\n",
                        "7/7 [==============================] - 0s 66ms/step - loss: 0.6829 - accuracy: 0.5429 - val_loss: 0.6724 - val_accuracy: 0.5556\n",
                        "Epoch 5/100\n",
                        "7/7 [==============================] - 0s 68ms/step - loss: 0.6640 - accuracy: 0.6298 - val_loss: 0.6721 - val_accuracy: 0.5083\n",
                        "Epoch 6/100\n",
                        "7/7 [==============================] - 0s 65ms/step - loss: 0.6508 - accuracy: 0.6012 - val_loss: 0.6554 - val_accuracy: 0.6583\n",
                        "Epoch 7/100\n",
                        "7/7 [==============================] - 0s 70ms/step - loss: 0.6387 - accuracy: 0.6655 - val_loss: 0.6666 - val_accuracy: 0.5028\n",
                        "Epoch 8/100\n",
                        "7/7 [==============================] - 0s 65ms/step - loss: 0.6261 - accuracy: 0.6917 - val_loss: 0.6269 - val_accuracy: 0.6417\n",
                        "Epoch 9/100\n",
                        "7/7 [==============================] - 0s 73ms/step - loss: 0.6034 - accuracy: 0.6869 - val_loss: 0.6072 - val_accuracy: 0.7167\n",
                        "Epoch 10/100\n",
                        "7/7 [==============================] - 1s 76ms/step - loss: 0.5834 - accuracy: 0.7512 - val_loss: 0.5868 - val_accuracy: 0.7000\n",
                        "Epoch 11/100\n",
                        "7/7 [==============================] - 0s 74ms/step - loss: 0.5594 - accuracy: 0.7488 - val_loss: 0.5821 - val_accuracy: 0.7417\n",
                        "Epoch 12/100\n",
                        "7/7 [==============================] - 1s 86ms/step - loss: 0.5574 - accuracy: 0.7262 - val_loss: 0.5578 - val_accuracy: 0.7556\n",
                        "Epoch 13/100\n",
                        "7/7 [==============================] - 1s 93ms/step - loss: 0.5332 - accuracy: 0.7393 - val_loss: 0.5451 - val_accuracy: 0.6972\n",
                        "Epoch 14/100\n",
                        "7/7 [==============================] - 0s 67ms/step - loss: 0.4997 - accuracy: 0.7905 - val_loss: 0.5400 - val_accuracy: 0.7083\n",
                        "Epoch 15/100\n",
                        "7/7 [==============================] - 0s 53ms/step - loss: 0.4717 - accuracy: 0.7893 - val_loss: 0.4963 - val_accuracy: 0.7444\n",
                        "Epoch 16/100\n",
                        "7/7 [==============================] - 0s 48ms/step - loss: 0.4466 - accuracy: 0.8107 - val_loss: 0.5072 - val_accuracy: 0.7667\n",
                        "Epoch 17/100\n",
                        "7/7 [==============================] - 0s 54ms/step - loss: 0.4533 - accuracy: 0.7857 - val_loss: 0.4560 - val_accuracy: 0.7722\n",
                        "Epoch 18/100\n",
                        "7/7 [==============================] - 0s 48ms/step - loss: 0.4115 - accuracy: 0.8167 - val_loss: 0.5075 - val_accuracy: 0.7278\n",
                        "Epoch 19/100\n",
                        "7/7 [==============================] - 0s 44ms/step - loss: 0.3874 - accuracy: 0.8369 - val_loss: 0.4311 - val_accuracy: 0.8056\n",
                        "Epoch 20/100\n",
                        "7/7 [==============================] - 0s 50ms/step - loss: 0.3552 - accuracy: 0.8595 - val_loss: 0.4618 - val_accuracy: 0.8000\n",
                        "Epoch 21/100\n",
                        "7/7 [==============================] - 0s 43ms/step - loss: 0.3622 - accuracy: 0.8381 - val_loss: 0.4144 - val_accuracy: 0.8222\n",
                        "Epoch 22/100\n",
                        "7/7 [==============================] - 0s 52ms/step - loss: 0.3670 - accuracy: 0.8321 - val_loss: 0.4297 - val_accuracy: 0.7861\n",
                        "Epoch 23/100\n",
                        "7/7 [==============================] - 0s 45ms/step - loss: 0.3045 - accuracy: 0.8750 - val_loss: 0.4043 - val_accuracy: 0.8056\n",
                        "Epoch 24/100\n",
                        "7/7 [==============================] - 0s 51ms/step - loss: 0.2911 - accuracy: 0.8929 - val_loss: 0.4174 - val_accuracy: 0.7861\n",
                        "Epoch 25/100\n",
                        "7/7 [==============================] - 0s 44ms/step - loss: 0.2817 - accuracy: 0.8833 - val_loss: 0.4060 - val_accuracy: 0.7944\n",
                        "Epoch 26/100\n",
                        "7/7 [==============================] - 0s 45ms/step - loss: 0.2708 - accuracy: 0.8952 - val_loss: 0.3845 - val_accuracy: 0.8333\n",
                        "Epoch 27/100\n",
                        "7/7 [==============================] - 0s 46ms/step - loss: 0.2390 - accuracy: 0.9190 - val_loss: 0.3768 - val_accuracy: 0.8417\n",
                        "Epoch 28/100\n",
                        "7/7 [==============================] - 0s 45ms/step - loss: 0.2176 - accuracy: 0.9274 - val_loss: 0.3734 - val_accuracy: 0.8444\n",
                        "Epoch 29/100\n",
                        "7/7 [==============================] - 0s 51ms/step - loss: 0.1979 - accuracy: 0.9381 - val_loss: 0.3775 - val_accuracy: 0.8528\n",
                        "Epoch 30/100\n",
                        "7/7 [==============================] - 0s 54ms/step - loss: 0.1836 - accuracy: 0.9500 - val_loss: 0.3693 - val_accuracy: 0.8278\n",
                        "Epoch 31/100\n",
                        "7/7 [==============================] - 0s 51ms/step - loss: 0.1764 - accuracy: 0.9452 - val_loss: 0.3665 - val_accuracy: 0.8472\n",
                        "Epoch 32/100\n",
                        "7/7 [==============================] - 0s 48ms/step - loss: 0.1535 - accuracy: 0.9571 - val_loss: 0.3681 - val_accuracy: 0.8444\n",
                        "Epoch 33/100\n",
                        "7/7 [==============================] - 0s 56ms/step - loss: 0.1802 - accuracy: 0.9310 - val_loss: 0.3674 - val_accuracy: 0.8500\n",
                        "Epoch 34/100\n",
                        "7/7 [==============================] - 0s 44ms/step - loss: 0.1328 - accuracy: 0.9679 - val_loss: 0.3678 - val_accuracy: 0.8556\n",
                        "Epoch 35/100\n",
                        "7/7 [==============================] - 0s 46ms/step - loss: 0.1221 - accuracy: 0.9762 - val_loss: 0.4273 - val_accuracy: 0.8472\n",
                        "Epoch 36/100\n",
                        "7/7 [==============================] - 0s 48ms/step - loss: 0.1345 - accuracy: 0.9512 - val_loss: 0.3691 - val_accuracy: 0.8361\n",
                        "Epoch 37/100\n",
                        "7/7 [==============================] - 0s 50ms/step - loss: 0.1068 - accuracy: 0.9726 - val_loss: 0.4507 - val_accuracy: 0.8472\n",
                        "Epoch 38/100\n",
                        "7/7 [==============================] - 0s 49ms/step - loss: 0.1438 - accuracy: 0.9357 - val_loss: 0.4417 - val_accuracy: 0.7889\n",
                        "Epoch 39/100\n",
                        "7/7 [==============================] - 0s 66ms/step - loss: 0.1387 - accuracy: 0.9488 - val_loss: 0.4408 - val_accuracy: 0.7944\n",
                        "Epoch 40/100\n",
                        "7/7 [==============================] - 0s 63ms/step - loss: 0.1071 - accuracy: 0.9750 - val_loss: 0.3817 - val_accuracy: 0.8444\n",
                        "Epoch 41/100\n",
                        "7/7 [==============================] - 0s 64ms/step - loss: 0.0841 - accuracy: 0.9833 - val_loss: 0.3887 - val_accuracy: 0.8472\n",
                        "Epoch 42/100\n",
                        "7/7 [==============================] - 0s 52ms/step - loss: 0.0699 - accuracy: 0.9929 - val_loss: 0.3918 - val_accuracy: 0.8444\n",
                        "Epoch 43/100\n",
                        "7/7 [==============================] - 0s 68ms/step - loss: 0.0577 - accuracy: 0.9940 - val_loss: 0.3943 - val_accuracy: 0.8417\n",
                        "Epoch 44/100\n",
                        "7/7 [==============================] - 1s 80ms/step - loss: 0.0559 - accuracy: 0.9940 - val_loss: 0.3953 - val_accuracy: 0.8528\n",
                        "Epoch 45/100\n",
                        "7/7 [==============================] - 0s 63ms/step - loss: 0.0491 - accuracy: 0.9964 - val_loss: 0.4145 - val_accuracy: 0.8333\n",
                        "Epoch 46/100\n",
                        "7/7 [==============================] - 0s 62ms/step - loss: 0.0484 - accuracy: 0.9976 - val_loss: 0.4030 - val_accuracy: 0.8444\n",
                        "Epoch 47/100\n",
                        "7/7 [==============================] - 0s 72ms/step - loss: 0.0476 - accuracy: 0.9976 - val_loss: 0.4154 - val_accuracy: 0.8556\n",
                        "Epoch 48/100\n",
                        "7/7 [==============================] - 0s 64ms/step - loss: 0.0427 - accuracy: 0.9976 - val_loss: 0.4196 - val_accuracy: 0.8417\n",
                        "Epoch 49/100\n",
                        "7/7 [==============================] - 0s 65ms/step - loss: 0.0390 - accuracy: 0.9988 - val_loss: 0.4321 - val_accuracy: 0.8222\n",
                        "Epoch 50/100\n",
                        "7/7 [==============================] - 0s 62ms/step - loss: 0.0325 - accuracy: 0.9976 - val_loss: 0.4416 - val_accuracy: 0.8278\n",
                        "Epoch 51/100\n",
                        "7/7 [==============================] - 0s 57ms/step - loss: 0.0243 - accuracy: 0.9988 - val_loss: 0.4432 - val_accuracy: 0.8278\n",
                        "Epoch 52/100\n",
                        "7/7 [==============================] - 0s 61ms/step - loss: 0.0193 - accuracy: 1.0000 - val_loss: 0.4503 - val_accuracy: 0.8417\n",
                        "Epoch 53/100\n",
                        "7/7 [==============================] - 0s 56ms/step - loss: 0.0170 - accuracy: 1.0000 - val_loss: 0.4542 - val_accuracy: 0.8472\n",
                        "Epoch 54/100\n",
                        "7/7 [==============================] - 0s 67ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.4631 - val_accuracy: 0.8444\n",
                        "Epoch 55/100\n",
                        "7/7 [==============================] - 0s 58ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.4856 - val_accuracy: 0.8583\n",
                        "Epoch 56/100\n",
                        "7/7 [==============================] - 0s 60ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.4754 - val_accuracy: 0.8444\n",
                        "Epoch 57/100\n",
                        "7/7 [==============================] - 0s 61ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.4827 - val_accuracy: 0.8306\n",
                        "Epoch 58/100\n",
                        "7/7 [==============================] - 0s 61ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.4884 - val_accuracy: 0.8528\n",
                        "Epoch 59/100\n",
                        "7/7 [==============================] - 1s 90ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.4914 - val_accuracy: 0.8500\n",
                        "Epoch 60/100\n",
                        "7/7 [==============================] - 0s 51ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.4969 - val_accuracy: 0.8417\n",
                        "Epoch 61/100\n",
                        "7/7 [==============================] - 0s 48ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.5003 - val_accuracy: 0.8444\n",
                        "Epoch 62/100\n",
                        "7/7 [==============================] - 0s 45ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.5191 - val_accuracy: 0.8639\n",
                        "Epoch 63/100\n",
                        "7/7 [==============================] - 0s 46ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.5098 - val_accuracy: 0.8472\n",
                        "Epoch 64/100\n",
                        "7/7 [==============================] - 0s 54ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.5169 - val_accuracy: 0.8444\n",
                        "Epoch 65/100\n",
                        "7/7 [==============================] - 0s 49ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.5205 - val_accuracy: 0.8444\n",
                        "Epoch 66/100\n",
                        "7/7 [==============================] - 0s 50ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.5320 - val_accuracy: 0.8444\n",
                        "Epoch 67/100\n",
                        "7/7 [==============================] - 0s 43ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.5366 - val_accuracy: 0.8444\n",
                        "Epoch 68/100\n",
                        "7/7 [==============================] - 0s 54ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.5402 - val_accuracy: 0.8444\n",
                        "Epoch 69/100\n",
                        "7/7 [==============================] - 0s 51ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.5435 - val_accuracy: 0.8472\n",
                        "Epoch 70/100\n",
                        "7/7 [==============================] - 0s 51ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5520 - val_accuracy: 0.8472\n",
                        "Epoch 71/100\n",
                        "7/7 [==============================] - 0s 50ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.5582 - val_accuracy: 0.8472\n",
                        "Epoch 72/100\n",
                        "7/7 [==============================] - 0s 42ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.5585 - val_accuracy: 0.8500\n",
                        "Epoch 73/100\n",
                        "7/7 [==============================] - 0s 48ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.5688 - val_accuracy: 0.8500\n",
                        "Epoch 74/100\n",
                        "7/7 [==============================] - 0s 54ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.5682 - val_accuracy: 0.8472\n",
                        "Epoch 75/100\n",
                        "7/7 [==============================] - 0s 49ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5734 - val_accuracy: 0.8444\n",
                        "Epoch 76/100\n",
                        "7/7 [==============================] - 0s 53ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5856 - val_accuracy: 0.8500\n",
                        "Epoch 77/100\n",
                        "7/7 [==============================] - 0s 46ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5854 - val_accuracy: 0.8472\n",
                        "Epoch 78/100\n",
                        "7/7 [==============================] - 0s 50ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.5911 - val_accuracy: 0.8472\n",
                        "Epoch 79/100\n",
                        "7/7 [==============================] - 0s 64ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6081 - val_accuracy: 0.8556\n",
                        "Epoch 80/100\n",
                        "7/7 [==============================] - 0s 69ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6070 - val_accuracy: 0.8500\n",
                        "Epoch 81/100\n",
                        "7/7 [==============================] - 0s 62ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6145 - val_accuracy: 0.8472\n",
                        "Epoch 82/100\n",
                        "7/7 [==============================] - 0s 51ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6203 - val_accuracy: 0.8472\n",
                        "Epoch 83/100\n",
                        "7/7 [==============================] - 0s 57ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6266 - val_accuracy: 0.8472\n",
                        "Epoch 84/100\n",
                        "7/7 [==============================] - 1s 104ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6291 - val_accuracy: 0.8472\n",
                        "Epoch 85/100\n",
                        "7/7 [==============================] - 0s 51ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6344 - val_accuracy: 0.8472\n",
                        "Epoch 86/100\n",
                        "7/7 [==============================] - 0s 57ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.6399 - val_accuracy: 0.8472\n",
                        "Epoch 87/100\n",
                        "7/7 [==============================] - 0s 48ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.6415 - val_accuracy: 0.8472\n",
                        "Epoch 88/100\n",
                        "7/7 [==============================] - 0s 47ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.6488 - val_accuracy: 0.8444\n",
                        "Epoch 89/100\n",
                        "7/7 [==============================] - 0s 51ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.6489 - val_accuracy: 0.8472\n",
                        "Epoch 90/100\n",
                        "7/7 [==============================] - 0s 67ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.6549 - val_accuracy: 0.8472\n",
                        "Epoch 91/100\n",
                        "7/7 [==============================] - 0s 50ms/step - loss: 9.3856e-04 - accuracy: 1.0000 - val_loss: 0.6573 - val_accuracy: 0.8472\n",
                        "Epoch 92/100\n",
                        "7/7 [==============================] - 0s 45ms/step - loss: 8.9950e-04 - accuracy: 1.0000 - val_loss: 0.6623 - val_accuracy: 0.8444\n",
                        "Epoch 93/100\n",
                        "7/7 [==============================] - 0s 40ms/step - loss: 8.6559e-04 - accuracy: 1.0000 - val_loss: 0.6662 - val_accuracy: 0.8472\n",
                        "Epoch 94/100\n",
                        "7/7 [==============================] - 0s 45ms/step - loss: 8.2879e-04 - accuracy: 1.0000 - val_loss: 0.6683 - val_accuracy: 0.8472\n",
                        "Epoch 95/100\n",
                        "7/7 [==============================] - 0s 51ms/step - loss: 8.0619e-04 - accuracy: 1.0000 - val_loss: 0.6736 - val_accuracy: 0.8444\n",
                        "Epoch 96/100\n",
                        "7/7 [==============================] - 0s 47ms/step - loss: 7.7646e-04 - accuracy: 1.0000 - val_loss: 0.6755 - val_accuracy: 0.8472\n",
                        "Epoch 97/100\n",
                        "7/7 [==============================] - 0s 47ms/step - loss: 7.3885e-04 - accuracy: 1.0000 - val_loss: 0.6788 - val_accuracy: 0.8472\n",
                        "Epoch 98/100\n",
                        "7/7 [==============================] - 0s 56ms/step - loss: 7.1828e-04 - accuracy: 1.0000 - val_loss: 0.6816 - val_accuracy: 0.8472\n",
                        "Epoch 99/100\n",
                        "7/7 [==============================] - 0s 44ms/step - loss: 6.8366e-04 - accuracy: 1.0000 - val_loss: 0.6859 - val_accuracy: 0.8444\n",
                        "Epoch 100/100\n",
                        "7/7 [==============================] - 0s 44ms/step - loss: 6.6004e-04 - accuracy: 1.0000 - val_loss: 0.6887 - val_accuracy: 0.8472\n"
                    ]
                }
            ],
            "source": [
                "history = CNN_model.fit(X, y, epochs=100, batch_size=128, verbose=1, validation_split=0.3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<matplotlib.legend.Legend at 0x7fe6b51cc100>"
                        ]
                    },
                    "execution_count": 166,
                    "metadata": {},
                    "output_type": "execute_result"
                },
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 432x288 with 1 Axes>"
                        ]
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# plot the training accuracy and validation accuracy\n",
                "plt.plot(history.history['accuracy'])\n",
                "plt.plot(history.history['val_accuracy'])\n",
                "plt.legend(['train', 'val'], loc='upper left')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**2.2.3**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#your code here"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Error Analysis Inference:**\n",
                "\n",
                "*your answer here*"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
